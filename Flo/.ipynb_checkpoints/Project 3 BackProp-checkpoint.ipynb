{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"/datasets/sberbank-russian-housing-market/train.csv\")\n",
    "df_macro = pd.read_csv(\"/datasets/sberbank-russian-housing-market/macro.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train[[\"price_doc\"]]\n",
    "X_train = X_train.drop(\"price_doc\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample = X_train[[\"full_sq\", \"life_sq\",\"floor\"]].fillna(0).sample(5000)\n",
    "y_train_sample = y_train.loc[X_train_sample.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sample = y_train_sample/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16826</th>\n",
       "      <td>6.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28971</th>\n",
       "      <td>4.657415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6980</th>\n",
       "      <td>5.851250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5029</th>\n",
       "      <td>6.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>18.327100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29178</th>\n",
       "      <td>3.713450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29789</th>\n",
       "      <td>9.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16673</th>\n",
       "      <td>8.308714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30185</th>\n",
       "      <td>8.972619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23852</th>\n",
       "      <td>7.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       price_doc\n",
       "16826   6.500000\n",
       "28971   4.657415\n",
       "6980    5.851250\n",
       "5029    6.650000\n",
       "1370   18.327100\n",
       "...          ...\n",
       "29178   3.713450\n",
       "29789   9.100000\n",
       "16673   8.308714\n",
       "30185   8.972619\n",
       "23852   7.800000\n",
       "\n",
       "[5000 rows x 1 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def fit(self, X, y, n_hidden, nodes, activations, lr):\n",
    "        self._lr = lr\n",
    "        self._X = X.values\n",
    "        self._y = y.values\n",
    "        self._n_hidden = n_hidden\n",
    "        self._nodes = nodes\n",
    "        self._weights = self._generate_weights()\n",
    "        self._biases = self._generate_bias()\n",
    "        self._activations = activations\n",
    "        self._forward_inputs = []\n",
    "        \n",
    "        self._train()\n",
    "        \n",
    "    def _activation(self, data, activation = \"relu\"):\n",
    "        if activation == \"relu\":\n",
    "            def relu(data):\n",
    "                return np.array([max(0,i) for i in data]).reshape(data.shape)\n",
    "            return np.apply_along_axis(relu, 1, data)\n",
    "        if activation == \"sigmoid\":\n",
    "            def sigmoid(data):\n",
    "                return (1/(1 + np.exp(-data))).reshape(data.shape)\n",
    "            return np.apply_along_axis(sigmoid, 1, data)\n",
    "    \n",
    "    def _der_activation(self, points, activation = \"relu\"):\n",
    "        if activation == \"relu\":\n",
    "            def d_relu(point):\n",
    "                return np.array([0 if y <= 0 else 1 for y in point])\n",
    "            return np.apply_along_axis(d_relu, 1, points)\n",
    "        if activation == \"sigmoid\":\n",
    "            ## todo\n",
    "            return\n",
    "    \n",
    "    def _loss_function(self, ypred, loss = \"l2\"):\n",
    "        if loss == \"mse\":\n",
    "            return ((ypred - self._y) ** 2).mean()\n",
    "        if loss == \"l2\":\n",
    "            return (((ypred - self._y) ** 2)/2)\n",
    "    \n",
    "    def _loss_jacobian(self, ypred, loss = \"l2\"):\n",
    "        if loss == \"l2\":\n",
    "            return (ypred - self._y)/(len(ypred))\n",
    "    \n",
    "    def _generate_weights(self):\n",
    "        hidden_weights = []\n",
    "        nodes = self._nodes\n",
    "        for idx in range(1,len(nodes)):\n",
    "            hidden_weights.append(0.01 * np.random.randn(nodes[idx -1], nodes[idx]))\n",
    "\n",
    "        return hidden_weights\n",
    "    \n",
    "    def _generate_bias(self):\n",
    "        hidden_layers = []\n",
    "        nodes = self._nodes\n",
    "        for i in range(self._n_hidden + 1):\n",
    "            hidden_layers.append(np.zeros((nodes[i + 1], 1)))\n",
    "        return hidden_layers\n",
    "    \n",
    "    def _forward_propagation(self):\n",
    "        \"\"\"\n",
    "        Suppose 2 observations\n",
    "        \n",
    "        Suppose previous layer is 3 nodes\n",
    "        Suppose current layer is 2 nodes\n",
    "        \n",
    "        prev shape (2,3)\n",
    "        prev = ob1 [prev_node_1 val, prev_node_2 val, prev_node_3 val]\n",
    "               ob2 [prev_node_1 val, prev_node_2 val, prev_node_3 val]\n",
    "               \n",
    "        layer shape (3,2)\n",
    "        layer = [weight for current_node_1 for prev_node_1, weight for current_node_2 for prev_node_1]\n",
    "                [weight for current_node_1 for prev_node_2, weight for current_node_2 for prev_node_2]\n",
    "                [weight for current_node_1 for prev_node_3, weight for current_node_2 for prev_node_3]\n",
    "                \n",
    "        output shape (2,2) # since 2 observations and 2 layers\n",
    "        output = ob1 [current_node_1 val, current_node_2 val]\n",
    "                 ob2 [current_node_1 val, current_node_2 val]\n",
    "                 \n",
    "        Then for bias in current layer it is (2,1) since 2 nodes in current layer\n",
    "        \n",
    "        So for each row in output we add the bias row wise and apply the activation function to each row\n",
    "        \n",
    "        prev <- ouput\n",
    "        \n",
    "        Move onto next layer...\n",
    "        \"\"\"\n",
    "        prev = self._X\n",
    "        weights = self._weights\n",
    "        biases = self._biases\n",
    "        activations = self._activations[1:-1]\n",
    "    \n",
    "        for idx, layer in enumerate(weights):\n",
    "            if idx == (len(weights) - 1):\n",
    "                self._forward_inputs.append((prev, None))\n",
    "                prev = (prev @ layer) + biases[idx].T,\n",
    "            else:\n",
    "                weight_output = (prev @ layer) + biases[idx].T\n",
    "                self._forward_inputs.append((prev, weight_output))\n",
    "                prev = self._activation(data = weight_output, activation = activations[idx])\n",
    "\n",
    "        return prev\n",
    "    \n",
    "    def _backward_propagation(self, ypred):\n",
    "        j = self._loss_jacobian(ypred)\n",
    "        #print(\"\\nj\\n\")\n",
    "        #print(j)\n",
    "                \n",
    "        for i in range(len(self._forward_inputs)-1, -1, -1):\n",
    "            if i != (len(self._forward_inputs) - 1):\n",
    "                # activation func on all layers except the last\n",
    "                der_acti = self._der_activation(self._forward_inputs[i][1])\n",
    "                j = np.multiply(j,der_acti)\n",
    "\n",
    "            x = self._forward_inputs[i][0]\n",
    "            #print(\"\\nx:\")\n",
    "            #print(x)\n",
    "            jw = x.T.dot(j)\n",
    "            #print(\"\\nweights before:\")\n",
    "            #print(self._weights[i])\n",
    "            self._weights[i] -= self._lr * jw\n",
    "            #print(\"\\nweights after:\")\n",
    "            #print(self._weights[i])\n",
    "            # todo: update bias\n",
    "            j = j.dot(self._weights[i].T)\n",
    "            \n",
    "        self._forward_inputs = []\n",
    "        \n",
    "    \n",
    "    def _train(self):\n",
    "        for i in range(0, 100):\n",
    "            out = self._forward_propagation()\n",
    "            loss = self._loss_function(out[0])\n",
    "            mse = self._loss_function(out[0], loss = \"mse\")\n",
    "            print(\"\\nloss:\")\n",
    "            print(self._loss_function(out[0]).mean())\n",
    "            print(\"nmse:\")\n",
    "            print(mse)\n",
    "            #print(\"\\npredictions\\n\")\n",
    "            #print(out)\n",
    "            self._backward_propagation(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss:\n",
      "37.68895810752857\n",
      "nmse:\n",
      "75.37791621505714\n",
      "\n",
      "loss:\n",
      "35.92765305090365\n",
      "nmse:\n",
      "71.8553061018073\n",
      "\n",
      "loss:\n",
      "32.64316179440443\n",
      "nmse:\n",
      "65.28632358880886\n",
      "\n",
      "loss:\n",
      "27.39337281629731\n",
      "nmse:\n",
      "54.78674563259462\n",
      "\n",
      "loss:\n",
      "23.286256232158642\n",
      "nmse:\n",
      "46.572512464317285\n",
      "\n",
      "loss:\n",
      "22.013669995923006\n",
      "nmse:\n",
      "44.02733999184601\n",
      "\n",
      "loss:\n",
      "21.21967247536317\n",
      "nmse:\n",
      "42.43934495072634\n",
      "\n",
      "loss:\n",
      "20.431768908347312\n",
      "nmse:\n",
      "40.863537816694624\n",
      "\n",
      "loss:\n",
      "19.64663463553195\n",
      "nmse:\n",
      "39.2932692710639\n",
      "\n",
      "loss:\n",
      "18.870810330410716\n",
      "nmse:\n",
      "37.74162066082143\n",
      "\n",
      "loss:\n",
      "18.11700855348271\n",
      "nmse:\n",
      "36.23401710696542\n",
      "\n",
      "loss:\n",
      "17.395778307104813\n",
      "nmse:\n",
      "34.791556614209625\n",
      "\n",
      "loss:\n",
      "16.728202629564638\n",
      "nmse:\n",
      "33.456405259129276\n",
      "\n",
      "loss:\n",
      "16.132053090240756\n",
      "nmse:\n",
      "32.26410618048151\n",
      "\n",
      "loss:\n",
      "15.63997621709854\n",
      "nmse:\n",
      "31.27995243419708\n",
      "\n",
      "loss:\n",
      "15.238226845308311\n",
      "nmse:\n",
      "30.476453690616623\n",
      "\n",
      "loss:\n",
      "14.912118820783714\n",
      "nmse:\n",
      "29.82423764156743\n",
      "\n",
      "loss:\n",
      "14.658356209590307\n",
      "nmse:\n",
      "29.316712419180615\n",
      "\n",
      "loss:\n",
      "14.477137667626126\n",
      "nmse:\n",
      "28.95427533525225\n",
      "\n",
      "loss:\n",
      "14.350366373369415\n",
      "nmse:\n",
      "28.70073274673883\n",
      "\n",
      "loss:\n",
      "14.256684367959947\n",
      "nmse:\n",
      "28.513368735919894\n",
      "\n",
      "loss:\n",
      "14.176807511870724\n",
      "nmse:\n",
      "28.353615023741447\n",
      "\n",
      "loss:\n",
      "14.111585266225022\n",
      "nmse:\n",
      "28.223170532450045\n",
      "\n",
      "loss:\n",
      "14.062193377565947\n",
      "nmse:\n",
      "28.124386755131894\n",
      "\n",
      "loss:\n",
      "14.016235555780034\n",
      "nmse:\n",
      "28.032471111560067\n",
      "\n",
      "loss:\n",
      "13.976316611421241\n",
      "nmse:\n",
      "27.952633222842483\n",
      "\n",
      "loss:\n",
      "13.938930935633966\n",
      "nmse:\n",
      "27.877861871267932\n",
      "\n",
      "loss:\n",
      "13.902206452343476\n",
      "nmse:\n",
      "27.804412904686952\n",
      "\n",
      "loss:\n",
      "13.868554656355913\n",
      "nmse:\n",
      "27.737109312711826\n",
      "\n",
      "loss:\n",
      "13.834846062052067\n",
      "nmse:\n",
      "27.669692124104134\n",
      "\n",
      "loss:\n",
      "13.80038564764898\n",
      "nmse:\n",
      "27.60077129529796\n",
      "\n",
      "loss:\n",
      "13.767474395848764\n",
      "nmse:\n",
      "27.534948791697527\n",
      "\n",
      "loss:\n",
      "13.735873070997675\n",
      "nmse:\n",
      "27.47174614199535\n",
      "\n",
      "loss:\n",
      "13.702734967482487\n",
      "nmse:\n",
      "27.405469934964973\n",
      "\n",
      "loss:\n",
      "13.669874407715335\n",
      "nmse:\n",
      "27.33974881543067\n",
      "\n",
      "loss:\n",
      "13.639443033267648\n",
      "nmse:\n",
      "27.278886066535296\n",
      "\n",
      "loss:\n",
      "13.60697768608522\n",
      "nmse:\n",
      "27.21395537217044\n",
      "\n",
      "loss:\n",
      "13.574724803322743\n",
      "nmse:\n",
      "27.149449606645486\n",
      "\n",
      "loss:\n",
      "13.544659444251138\n",
      "nmse:\n",
      "27.089318888502277\n",
      "\n",
      "loss:\n",
      "13.512832178676158\n",
      "nmse:\n",
      "27.025664357352316\n",
      "\n",
      "loss:\n",
      "13.481136583505533\n",
      "nmse:\n",
      "26.962273167011066\n",
      "\n",
      "loss:\n",
      "13.451275680283027\n",
      "nmse:\n",
      "26.902551360566054\n",
      "\n",
      "loss:\n",
      "13.420225589028668\n",
      "nmse:\n",
      "26.840451178057336\n",
      "\n",
      "loss:\n",
      "13.389046542201692\n",
      "nmse:\n",
      "26.778093084403384\n",
      "\n",
      "loss:\n",
      "13.359163126419874\n",
      "nmse:\n",
      "26.71832625283975\n",
      "\n",
      "loss:\n",
      "13.329121828178415\n",
      "nmse:\n",
      "26.65824365635683\n",
      "\n",
      "loss:\n",
      "13.298434159723469\n",
      "nmse:\n",
      "26.596868319446937\n",
      "\n",
      "loss:\n",
      "13.26808456623287\n",
      "nmse:\n",
      "26.53616913246574\n",
      "\n",
      "loss:\n",
      "13.23918482668716\n",
      "nmse:\n",
      "26.47836965337432\n",
      "\n",
      "loss:\n",
      "13.208818537203344\n",
      "nmse:\n",
      "26.417637074406688\n",
      "\n",
      "loss:\n",
      "13.178693318628032\n",
      "nmse:\n",
      "26.357386637256063\n",
      "\n",
      "loss:\n",
      "13.14951340795182\n",
      "nmse:\n",
      "26.29902681590364\n",
      "\n",
      "loss:\n",
      "13.120509938485819\n",
      "nmse:\n",
      "26.241019876971638\n",
      "\n",
      "loss:\n",
      "13.090681980280355\n",
      "nmse:\n",
      "26.18136396056071\n",
      "\n",
      "loss:\n",
      "13.060201223155275\n",
      "nmse:\n",
      "26.12040244631055\n",
      "\n",
      "loss:\n",
      "13.0311673603325\n",
      "nmse:\n",
      "26.062334720665\n",
      "\n",
      "loss:\n",
      "13.000706240921243\n",
      "nmse:\n",
      "26.001412481842486\n",
      "\n",
      "loss:\n",
      "12.970836375455576\n",
      "nmse:\n",
      "25.941672750911152\n",
      "\n",
      "loss:\n",
      "12.942893896669636\n",
      "nmse:\n",
      "25.885787793339272\n",
      "\n",
      "loss:\n",
      "12.914419101847315\n",
      "nmse:\n",
      "25.82883820369463\n",
      "\n",
      "loss:\n",
      "12.886030179393076\n",
      "nmse:\n",
      "25.772060358786153\n",
      "\n",
      "loss:\n",
      "12.858611078356658\n",
      "nmse:\n",
      "25.717222156713316\n",
      "\n",
      "loss:\n",
      "12.83111822310457\n",
      "nmse:\n",
      "25.66223644620914\n",
      "\n",
      "loss:\n",
      "12.803351319629321\n",
      "nmse:\n",
      "25.606702639258643\n",
      "\n",
      "loss:\n",
      "12.775945590355258\n",
      "nmse:\n",
      "25.551891180710516\n",
      "\n",
      "loss:\n",
      "12.749679115526792\n",
      "nmse:\n",
      "25.499358231053584\n",
      "\n",
      "loss:\n",
      "12.72252620641235\n",
      "nmse:\n",
      "25.4450524128247\n",
      "\n",
      "loss:\n",
      "12.695619242130176\n",
      "nmse:\n",
      "25.391238484260352\n",
      "\n",
      "loss:\n",
      "12.66948127507736\n",
      "nmse:\n",
      "25.33896255015472\n",
      "\n",
      "loss:\n",
      "12.643592045970943\n",
      "nmse:\n",
      "25.287184091941885\n",
      "\n",
      "loss:\n",
      "12.617367056582257\n",
      "nmse:\n",
      "25.234734113164514\n",
      "\n",
      "loss:\n",
      "12.591427126387082\n",
      "nmse:\n",
      "25.182854252774163\n",
      "\n",
      "loss:\n",
      "12.566749846358405\n",
      "nmse:\n",
      "25.13349969271681\n",
      "\n",
      "loss:\n",
      "12.541335875845137\n",
      "nmse:\n",
      "25.082671751690274\n",
      "\n",
      "loss:\n",
      "12.516300220604226\n",
      "nmse:\n",
      "25.03260044120845\n",
      "\n",
      "loss:\n",
      "12.492444700299803\n",
      "nmse:\n",
      "24.984889400599606\n",
      "\n",
      "loss:\n",
      "12.468288250169996\n",
      "nmse:\n",
      "24.936576500339992\n",
      "\n",
      "loss:\n",
      "12.444862511789484\n",
      "nmse:\n",
      "24.88972502357897\n",
      "\n",
      "loss:\n",
      "12.423211570001861\n",
      "nmse:\n",
      "24.846423140003722\n",
      "\n",
      "loss:\n",
      "12.40286373471117\n",
      "nmse:\n",
      "24.80572746942234\n",
      "\n",
      "loss:\n",
      "12.384955362214193\n",
      "nmse:\n",
      "24.769910724428385\n",
      "\n",
      "loss:\n",
      "12.372040591692567\n",
      "nmse:\n",
      "24.744081183385134\n",
      "\n",
      "loss:\n",
      "12.369122279105122\n",
      "nmse:\n",
      "24.738244558210244\n",
      "\n",
      "loss:\n",
      "12.38174815874928\n",
      "nmse:\n",
      "24.76349631749856\n",
      "\n",
      "loss:\n",
      "12.42866559014032\n",
      "nmse:\n",
      "24.85733118028064\n",
      "\n",
      "loss:\n",
      "12.545522884362109\n",
      "nmse:\n",
      "25.091045768724218\n",
      "\n",
      "loss:\n",
      "12.820963437564753\n",
      "nmse:\n",
      "25.641926875129506\n",
      "\n",
      "loss:\n",
      "13.287524927437929\n",
      "nmse:\n",
      "26.575049854875857\n",
      "\n",
      "loss:\n",
      "13.875840581319167\n",
      "nmse:\n",
      "27.751681162638334\n",
      "\n",
      "loss:\n",
      "14.793916971032047\n",
      "nmse:\n",
      "29.587833942064094\n",
      "\n",
      "loss:\n",
      "14.332073179881187\n",
      "nmse:\n",
      "28.664146359762373\n",
      "\n",
      "loss:\n",
      "15.017207126013743\n",
      "nmse:\n",
      "30.034414252027485\n",
      "\n",
      "loss:\n",
      "13.616195900948282\n",
      "nmse:\n",
      "27.232391801896565\n",
      "\n",
      "loss:\n",
      "14.04944165884708\n",
      "nmse:\n",
      "28.09888331769416\n",
      "\n",
      "loss:\n",
      "13.141766022810398\n",
      "nmse:\n",
      "26.283532045620795\n",
      "\n",
      "loss:\n",
      "12.758289736932959\n",
      "nmse:\n",
      "25.516579473865917\n",
      "\n",
      "loss:\n",
      "12.856713300345847\n",
      "nmse:\n",
      "25.713426600691694\n",
      "\n",
      "loss:\n",
      "13.084280021842206\n",
      "nmse:\n",
      "26.168560043684412\n",
      "\n",
      "loss:\n",
      "13.409750255190739\n",
      "nmse:\n",
      "26.819500510381477\n",
      "\n",
      "loss:\n",
      "13.805955612247503\n",
      "nmse:\n",
      "27.611911224495007\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = X_train_sample.shape[1]\n",
    "OUTPUT_SIZE = 1\n",
    "LEARNING_RATE = 0.001\n",
    "nodes = [INPUT_SIZE,50,OUTPUT_SIZE]\n",
    "activations = [\"relu\" for i in range(len(nodes))]\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "nn.fit(X = X_train_sample,\n",
    "       y = y_train_sample,\n",
    "       n_hidden = len(nodes) - 2,\n",
    "       nodes = nodes,\n",
    "       activations = activations,\n",
    "       lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
