{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"/datasets/sberbank-russian-housing-market/train.csv\", index_col=0)\n",
    "df_macro = pd.read_csv(\"/datasets/sberbank-russian-housing-market/macro.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping na's and processing ONLY for the forward stepwise\n",
    "X_train.dropna(inplace=True)\n",
    "X_train.reset_index(inplace=True)\n",
    "X_train.drop(columns=[\"id\"], inplace=True)\n",
    "X_train = X_train.select_dtypes(include=np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train[[\"price_doc\"]]\n",
    "X_train = X_train.drop(\"price_doc\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample = X_train[[\"full_sq\", \"life_sq\",\"floor\"]].fillna(0).sample(5000)\n",
    "y_train_sample = y_train.loc[X_train_sample.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sample = y_train_sample/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16826</th>\n",
       "      <td>6.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28971</th>\n",
       "      <td>4.657415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6980</th>\n",
       "      <td>5.851250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5029</th>\n",
       "      <td>6.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>18.327100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29178</th>\n",
       "      <td>3.713450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29789</th>\n",
       "      <td>9.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16673</th>\n",
       "      <td>8.308714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30185</th>\n",
       "      <td>8.972619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23852</th>\n",
       "      <td>7.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       price_doc\n",
       "16826   6.500000\n",
       "28971   4.657415\n",
       "6980    5.851250\n",
       "5029    6.650000\n",
       "1370   18.327100\n",
       "...          ...\n",
       "29178   3.713450\n",
       "29789   9.100000\n",
       "16673   8.308714\n",
       "30185   8.972619\n",
       "23852   7.800000\n",
       "\n",
       "[5000 rows x 1 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def fit(self, X, y, n_hidden, nodes, activations, lr):\n",
    "        self._lr = lr\n",
    "        self._X = X.values\n",
    "        self._y = y.values\n",
    "        self._n_hidden = n_hidden\n",
    "        self._nodes = nodes\n",
    "        self._weights = self._generate_weights()\n",
    "        self._biases = self._generate_bias()\n",
    "        self._activations = activations\n",
    "        self._forward_inputs = []\n",
    "        \n",
    "        self._train()\n",
    "        \n",
    "    def _activation(self, data, activation = \"relu\"):\n",
    "        if activation == \"relu\":\n",
    "            def relu(data):\n",
    "                return np.array([max(0,i) for i in data]).reshape(data.shape)\n",
    "            return np.apply_along_axis(relu, 1, data)\n",
    "        if activation == \"sigmoid\":\n",
    "            def sigmoid(data):\n",
    "                return (1/(1 + np.exp(-data))).reshape(data.shape)\n",
    "            return np.apply_along_axis(sigmoid, 1, data)\n",
    "    \n",
    "    def _der_activation(self, points, activation = \"relu\"):\n",
    "        if activation == \"relu\":\n",
    "            def d_relu(point):\n",
    "                return np.array([0 if y <= 0 else 1 for y in point])\n",
    "            return np.apply_along_axis(d_relu, 1, points)\n",
    "        if activation == \"sigmoid\":\n",
    "            ## todo\n",
    "            return\n",
    "    \n",
    "    def _loss_function(self, ypred, loss = \"l2\"):\n",
    "        if loss == \"mse\":\n",
    "            return ((ypred - self._y) ** 2).mean()\n",
    "        if loss == \"l2\":\n",
    "            return (((ypred - self._y) ** 2)/2)\n",
    "    \n",
    "    def _loss_jacobian(self, ypred, loss = \"l2\"):\n",
    "        if loss == \"l2\":\n",
    "            return (ypred - self._y)/(len(ypred))\n",
    "    \n",
    "    def _generate_weights(self):\n",
    "        hidden_weights = []\n",
    "        nodes = self._nodes\n",
    "        for idx in range(1,len(nodes)):\n",
    "            hidden_weights.append(0.01 * np.random.randn(nodes[idx -1], nodes[idx]))\n",
    "\n",
    "        return hidden_weights\n",
    "    \n",
    "    def _generate_bias(self):\n",
    "        hidden_layers = []\n",
    "        nodes = self._nodes\n",
    "        for i in range(self._n_hidden + 1):\n",
    "            hidden_layers.append(np.zeros((nodes[i + 1], 1)))\n",
    "        return hidden_layers\n",
    "    \n",
    "    def _forward_propagation(self):\n",
    "        \"\"\"\n",
    "        Suppose 2 observations\n",
    "        \n",
    "        Suppose previous layer is 3 nodes\n",
    "        Suppose current layer is 2 nodes\n",
    "        \n",
    "        prev shape (2,3)\n",
    "        prev = ob1 [prev_node_1 val, prev_node_2 val, prev_node_3 val]\n",
    "               ob2 [prev_node_1 val, prev_node_2 val, prev_node_3 val]\n",
    "               \n",
    "        layer shape (3,2)\n",
    "        layer = [weight for current_node_1 for prev_node_1, weight for current_node_2 for prev_node_1]\n",
    "                [weight for current_node_1 for prev_node_2, weight for current_node_2 for prev_node_2]\n",
    "                [weight for current_node_1 for prev_node_3, weight for current_node_2 for prev_node_3]\n",
    "                \n",
    "        output shape (2,2) # since 2 observations and 2 layers\n",
    "        output = ob1 [current_node_1 val, current_node_2 val]\n",
    "                 ob2 [current_node_1 val, current_node_2 val]\n",
    "                 \n",
    "        Then for bias in current layer it is (2,1) since 2 nodes in current layer\n",
    "        \n",
    "        So for each row in output we add the bias row wise and apply the activation function to each row\n",
    "        \n",
    "        prev <- ouput\n",
    "        \n",
    "        Move onto next layer...\n",
    "        \"\"\"\n",
    "        prev = self._X\n",
    "        weights = self._weights\n",
    "        biases = self._biases\n",
    "        activations = self._activations[1:-1]\n",
    "    \n",
    "        for idx, layer in enumerate(weights):\n",
    "            if idx == (len(weights) - 1):\n",
    "                self._forward_inputs.append((prev, None))\n",
    "                prev = (prev @ layer) + biases[idx].T,\n",
    "            else:\n",
    "                weight_output = (prev @ layer) + biases[idx].T\n",
    "                self._forward_inputs.append((prev, weight_output))\n",
    "                prev = self._activation(data = weight_output, activation = activations[idx])\n",
    "\n",
    "        return prev\n",
    "    \n",
    "    def _backward_propagation(self, ypred):\n",
    "        j = self._loss_jacobian(ypred)\n",
    "        #print(\"\\nj\\n\")\n",
    "        #print(j)\n",
    "                \n",
    "        for i in range(len(self._forward_inputs)-1, -1, -1):\n",
    "            if i != (len(self._forward_inputs) - 1):\n",
    "                # activation func on all layers except the last\n",
    "                der_acti = self._der_activation(self._forward_inputs[i][1])\n",
    "                j = np.multiply(j,der_acti)\n",
    "\n",
    "            x = self._forward_inputs[i][0]\n",
    "            #print(\"\\nx:\")\n",
    "            #print(x)\n",
    "            jw = x.T.dot(j)\n",
    "            #print(\"\\nweights before:\")\n",
    "            #print(self._weights[i])\n",
    "            self._weights[i] -= self._lr * jw\n",
    "            #print(\"\\nweights after:\")\n",
    "            #print(self._weights[i])\n",
    "            # todo: update bias\n",
    "            j = j.dot(self._weights[i].T)\n",
    "            \n",
    "        self._forward_inputs = []\n",
    "        \n",
    "    \n",
    "    def _train(self):\n",
    "        for i in range(0, 100):\n",
    "            out = self._forward_propagation()\n",
    "            loss = self._loss_function(out[0])\n",
    "            mse = self._loss_function(out[0], loss = \"mse\")\n",
    "            print(\"\\nloss:\")\n",
    "            print(self._loss_function(out[0]).mean())\n",
    "            print(\"nmse:\")\n",
    "            print(mse)\n",
    "            #print(\"\\npredictions\\n\")\n",
    "            #print(out)\n",
    "            self._backward_propagation(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss:\n",
      "37.68895810752857\n",
      "nmse:\n",
      "75.37791621505714\n",
      "\n",
      "loss:\n",
      "35.92765305090365\n",
      "nmse:\n",
      "71.8553061018073\n",
      "\n",
      "loss:\n",
      "32.64316179440443\n",
      "nmse:\n",
      "65.28632358880886\n",
      "\n",
      "loss:\n",
      "27.39337281629731\n",
      "nmse:\n",
      "54.78674563259462\n",
      "\n",
      "loss:\n",
      "23.286256232158642\n",
      "nmse:\n",
      "46.572512464317285\n",
      "\n",
      "loss:\n",
      "22.013669995923006\n",
      "nmse:\n",
      "44.02733999184601\n",
      "\n",
      "loss:\n",
      "21.21967247536317\n",
      "nmse:\n",
      "42.43934495072634\n",
      "\n",
      "loss:\n",
      "20.431768908347312\n",
      "nmse:\n",
      "40.863537816694624\n",
      "\n",
      "loss:\n",
      "19.64663463553195\n",
      "nmse:\n",
      "39.2932692710639\n",
      "\n",
      "loss:\n",
      "18.870810330410716\n",
      "nmse:\n",
      "37.74162066082143\n",
      "\n",
      "loss:\n",
      "18.11700855348271\n",
      "nmse:\n",
      "36.23401710696542\n",
      "\n",
      "loss:\n",
      "17.395778307104813\n",
      "nmse:\n",
      "34.791556614209625\n",
      "\n",
      "loss:\n",
      "16.728202629564638\n",
      "nmse:\n",
      "33.456405259129276\n",
      "\n",
      "loss:\n",
      "16.132053090240756\n",
      "nmse:\n",
      "32.26410618048151\n",
      "\n",
      "loss:\n",
      "15.63997621709854\n",
      "nmse:\n",
      "31.27995243419708\n",
      "\n",
      "loss:\n",
      "15.238226845308311\n",
      "nmse:\n",
      "30.476453690616623\n",
      "\n",
      "loss:\n",
      "14.912118820783714\n",
      "nmse:\n",
      "29.82423764156743\n",
      "\n",
      "loss:\n",
      "14.658356209590307\n",
      "nmse:\n",
      "29.316712419180615\n",
      "\n",
      "loss:\n",
      "14.477137667626126\n",
      "nmse:\n",
      "28.95427533525225\n",
      "\n",
      "loss:\n",
      "14.350366373369415\n",
      "nmse:\n",
      "28.70073274673883\n",
      "\n",
      "loss:\n",
      "14.256684367959947\n",
      "nmse:\n",
      "28.513368735919894\n",
      "\n",
      "loss:\n",
      "14.176807511870724\n",
      "nmse:\n",
      "28.353615023741447\n",
      "\n",
      "loss:\n",
      "14.111585266225022\n",
      "nmse:\n",
      "28.223170532450045\n",
      "\n",
      "loss:\n",
      "14.062193377565947\n",
      "nmse:\n",
      "28.124386755131894\n",
      "\n",
      "loss:\n",
      "14.016235555780034\n",
      "nmse:\n",
      "28.032471111560067\n",
      "\n",
      "loss:\n",
      "13.976316611421241\n",
      "nmse:\n",
      "27.952633222842483\n",
      "\n",
      "loss:\n",
      "13.938930935633966\n",
      "nmse:\n",
      "27.877861871267932\n",
      "\n",
      "loss:\n",
      "13.902206452343476\n",
      "nmse:\n",
      "27.804412904686952\n",
      "\n",
      "loss:\n",
      "13.868554656355913\n",
      "nmse:\n",
      "27.737109312711826\n",
      "\n",
      "loss:\n",
      "13.834846062052067\n",
      "nmse:\n",
      "27.669692124104134\n",
      "\n",
      "loss:\n",
      "13.80038564764898\n",
      "nmse:\n",
      "27.60077129529796\n",
      "\n",
      "loss:\n",
      "13.767474395848764\n",
      "nmse:\n",
      "27.534948791697527\n",
      "\n",
      "loss:\n",
      "13.735873070997675\n",
      "nmse:\n",
      "27.47174614199535\n",
      "\n",
      "loss:\n",
      "13.702734967482487\n",
      "nmse:\n",
      "27.405469934964973\n",
      "\n",
      "loss:\n",
      "13.669874407715335\n",
      "nmse:\n",
      "27.33974881543067\n",
      "\n",
      "loss:\n",
      "13.639443033267648\n",
      "nmse:\n",
      "27.278886066535296\n",
      "\n",
      "loss:\n",
      "13.60697768608522\n",
      "nmse:\n",
      "27.21395537217044\n",
      "\n",
      "loss:\n",
      "13.574724803322743\n",
      "nmse:\n",
      "27.149449606645486\n",
      "\n",
      "loss:\n",
      "13.544659444251138\n",
      "nmse:\n",
      "27.089318888502277\n",
      "\n",
      "loss:\n",
      "13.512832178676158\n",
      "nmse:\n",
      "27.025664357352316\n",
      "\n",
      "loss:\n",
      "13.481136583505533\n",
      "nmse:\n",
      "26.962273167011066\n",
      "\n",
      "loss:\n",
      "13.451275680283027\n",
      "nmse:\n",
      "26.902551360566054\n",
      "\n",
      "loss:\n",
      "13.420225589028668\n",
      "nmse:\n",
      "26.840451178057336\n",
      "\n",
      "loss:\n",
      "13.389046542201692\n",
      "nmse:\n",
      "26.778093084403384\n",
      "\n",
      "loss:\n",
      "13.359163126419874\n",
      "nmse:\n",
      "26.71832625283975\n",
      "\n",
      "loss:\n",
      "13.329121828178415\n",
      "nmse:\n",
      "26.65824365635683\n",
      "\n",
      "loss:\n",
      "13.298434159723469\n",
      "nmse:\n",
      "26.596868319446937\n",
      "\n",
      "loss:\n",
      "13.26808456623287\n",
      "nmse:\n",
      "26.53616913246574\n",
      "\n",
      "loss:\n",
      "13.23918482668716\n",
      "nmse:\n",
      "26.47836965337432\n",
      "\n",
      "loss:\n",
      "13.208818537203344\n",
      "nmse:\n",
      "26.417637074406688\n",
      "\n",
      "loss:\n",
      "13.178693318628032\n",
      "nmse:\n",
      "26.357386637256063\n",
      "\n",
      "loss:\n",
      "13.14951340795182\n",
      "nmse:\n",
      "26.29902681590364\n",
      "\n",
      "loss:\n",
      "13.120509938485819\n",
      "nmse:\n",
      "26.241019876971638\n",
      "\n",
      "loss:\n",
      "13.090681980280355\n",
      "nmse:\n",
      "26.18136396056071\n",
      "\n",
      "loss:\n",
      "13.060201223155275\n",
      "nmse:\n",
      "26.12040244631055\n",
      "\n",
      "loss:\n",
      "13.0311673603325\n",
      "nmse:\n",
      "26.062334720665\n",
      "\n",
      "loss:\n",
      "13.000706240921243\n",
      "nmse:\n",
      "26.001412481842486\n",
      "\n",
      "loss:\n",
      "12.970836375455576\n",
      "nmse:\n",
      "25.941672750911152\n",
      "\n",
      "loss:\n",
      "12.942893896669636\n",
      "nmse:\n",
      "25.885787793339272\n",
      "\n",
      "loss:\n",
      "12.914419101847315\n",
      "nmse:\n",
      "25.82883820369463\n",
      "\n",
      "loss:\n",
      "12.886030179393076\n",
      "nmse:\n",
      "25.772060358786153\n",
      "\n",
      "loss:\n",
      "12.858611078356658\n",
      "nmse:\n",
      "25.717222156713316\n",
      "\n",
      "loss:\n",
      "12.83111822310457\n",
      "nmse:\n",
      "25.66223644620914\n",
      "\n",
      "loss:\n",
      "12.803351319629321\n",
      "nmse:\n",
      "25.606702639258643\n",
      "\n",
      "loss:\n",
      "12.775945590355258\n",
      "nmse:\n",
      "25.551891180710516\n",
      "\n",
      "loss:\n",
      "12.749679115526792\n",
      "nmse:\n",
      "25.499358231053584\n",
      "\n",
      "loss:\n",
      "12.72252620641235\n",
      "nmse:\n",
      "25.4450524128247\n",
      "\n",
      "loss:\n",
      "12.695619242130176\n",
      "nmse:\n",
      "25.391238484260352\n",
      "\n",
      "loss:\n",
      "12.66948127507736\n",
      "nmse:\n",
      "25.33896255015472\n",
      "\n",
      "loss:\n",
      "12.643592045970943\n",
      "nmse:\n",
      "25.287184091941885\n",
      "\n",
      "loss:\n",
      "12.617367056582257\n",
      "nmse:\n",
      "25.234734113164514\n",
      "\n",
      "loss:\n",
      "12.591427126387082\n",
      "nmse:\n",
      "25.182854252774163\n",
      "\n",
      "loss:\n",
      "12.566749846358405\n",
      "nmse:\n",
      "25.13349969271681\n",
      "\n",
      "loss:\n",
      "12.541335875845137\n",
      "nmse:\n",
      "25.082671751690274\n",
      "\n",
      "loss:\n",
      "12.516300220604226\n",
      "nmse:\n",
      "25.03260044120845\n",
      "\n",
      "loss:\n",
      "12.492444700299803\n",
      "nmse:\n",
      "24.984889400599606\n",
      "\n",
      "loss:\n",
      "12.468288250169996\n",
      "nmse:\n",
      "24.936576500339992\n",
      "\n",
      "loss:\n",
      "12.444862511789484\n",
      "nmse:\n",
      "24.88972502357897\n",
      "\n",
      "loss:\n",
      "12.423211570001861\n",
      "nmse:\n",
      "24.846423140003722\n",
      "\n",
      "loss:\n",
      "12.40286373471117\n",
      "nmse:\n",
      "24.80572746942234\n",
      "\n",
      "loss:\n",
      "12.384955362214193\n",
      "nmse:\n",
      "24.769910724428385\n",
      "\n",
      "loss:\n",
      "12.372040591692567\n",
      "nmse:\n",
      "24.744081183385134\n",
      "\n",
      "loss:\n",
      "12.369122279105122\n",
      "nmse:\n",
      "24.738244558210244\n",
      "\n",
      "loss:\n",
      "12.38174815874928\n",
      "nmse:\n",
      "24.76349631749856\n",
      "\n",
      "loss:\n",
      "12.42866559014032\n",
      "nmse:\n",
      "24.85733118028064\n",
      "\n",
      "loss:\n",
      "12.545522884362109\n",
      "nmse:\n",
      "25.091045768724218\n",
      "\n",
      "loss:\n",
      "12.820963437564753\n",
      "nmse:\n",
      "25.641926875129506\n",
      "\n",
      "loss:\n",
      "13.287524927437929\n",
      "nmse:\n",
      "26.575049854875857\n",
      "\n",
      "loss:\n",
      "13.875840581319167\n",
      "nmse:\n",
      "27.751681162638334\n",
      "\n",
      "loss:\n",
      "14.793916971032047\n",
      "nmse:\n",
      "29.587833942064094\n",
      "\n",
      "loss:\n",
      "14.332073179881187\n",
      "nmse:\n",
      "28.664146359762373\n",
      "\n",
      "loss:\n",
      "15.017207126013743\n",
      "nmse:\n",
      "30.034414252027485\n",
      "\n",
      "loss:\n",
      "13.616195900948282\n",
      "nmse:\n",
      "27.232391801896565\n",
      "\n",
      "loss:\n",
      "14.04944165884708\n",
      "nmse:\n",
      "28.09888331769416\n",
      "\n",
      "loss:\n",
      "13.141766022810398\n",
      "nmse:\n",
      "26.283532045620795\n",
      "\n",
      "loss:\n",
      "12.758289736932959\n",
      "nmse:\n",
      "25.516579473865917\n",
      "\n",
      "loss:\n",
      "12.856713300345847\n",
      "nmse:\n",
      "25.713426600691694\n",
      "\n",
      "loss:\n",
      "13.084280021842206\n",
      "nmse:\n",
      "26.168560043684412\n",
      "\n",
      "loss:\n",
      "13.409750255190739\n",
      "nmse:\n",
      "26.819500510381477\n",
      "\n",
      "loss:\n",
      "13.805955612247503\n",
      "nmse:\n",
      "27.611911224495007\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = X_train_sample.shape[1]\n",
    "OUTPUT_SIZE = 1\n",
    "LEARNING_RATE = 0.001\n",
    "nodes = [INPUT_SIZE,50,OUTPUT_SIZE]\n",
    "activations = [\"relu\" for i in range(len(nodes))]\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "nn.fit(X = X_train_sample,\n",
    "       y = y_train_sample,\n",
    "       n_hidden = len(nodes) - 2,\n",
    "       nodes = nodes,\n",
    "       activations = activations,\n",
    "       lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_for_B(x, y, n):\n",
    "    # takes a data frame, and a numpy array\n",
    "    x = np.concatenate((np.ones((n,1)), x.to_numpy()), axis=1)\n",
    "    x_transpose = np.transpose(x)\n",
    "    return np.linalg.solve(np.matmul(x_transpose, x), x_transpose @ y)\n",
    "\n",
    "def get_predicted_values(beta, design_matrix, n):\n",
    "    return np.matmul(np.concatenate((np.ones((n,1)), design_matrix), axis = 1), beta)\n",
    "\n",
    "def BIC(predictions, actuals, d, n):\n",
    "    # numpy array - predictions, numpy array - actual \n",
    "    return (np.square(actuals - predictions).sum()) + (d * np.log(n))\n",
    "\n",
    "def RSquaredAdj(predictions, actuals, d, n):\n",
    "    x1 = np.square(actuals - actuals.mean()).sum()\n",
    "    x2 = np.square(predictions - actuals.mean()).sum()\n",
    "    r2 = x2/x1\n",
    "    return (1 - ((1 - r2) * (n - 1) / (n - d - 1)))\n",
    "\n",
    "def RMSE(predictions, actuals, n):\n",
    "    return np.sqrt((np.square(actuals - predictions).sum()) / n) \n",
    "\n",
    "def train_model(design_matrix, dependent_variable_series):\n",
    "    n = design_matrix.shape[0]\n",
    "    beta = solve_for_B(design_matrix, dependent_variable_series, n)\n",
    "    predicted_values = get_predicted_values(beta, design_matrix, n)\n",
    "    calculated_BIC = BIC(predicted_values, dependent_variable_series, d = design_matrix.shape[1], n = n)\n",
    "    calculated_RMSE = RMSE(predicted_values, dependent_variable_series, n = n)\n",
    "    residuals = predicted_values - dependent_variable_series\n",
    "    return beta, calculated_BIC, calculated_RMSE, residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_crossv_model(df, y): \n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(.25 * m)\n",
    "    train_1 = df.iloc[perm[:train_end]]\n",
    "    test_1 = y.iloc[perm[:train_end]]\n",
    "\n",
    "    train_2 = df.iloc[perm[train_end:train_end*2]]\n",
    "    test_2 = y.iloc[perm[train_end:train_end*2]]\n",
    "\n",
    "    train_3 = df.iloc[perm[train_end*2:train_end*3]]\n",
    "    test_3 = y.iloc[perm[train_end*2:train_end*3]]\n",
    "\n",
    "    train_4 = df.iloc[perm[train_end*3:]]\n",
    "    test_4 = y.iloc[perm[train_end*3:]]\n",
    "    return train_1, test_1, train_2, test_2, train_3, test_3, train_4, test_4\n",
    "\n",
    "\n",
    "def get_average_RMSE_for_one_model(df, y):\n",
    "    train_1, test_1, train_2, test_2, train_3, test_3, train_4, test_4 = train_crossv_model(df, y)\n",
    "\n",
    "    beta4, bic4, RMSE4, _ = train_model(train_1.append([train_2, train_3]), test_1.append([test_2, test_3]))\n",
    "\n",
    "    beta2, bic3,RMSE2, _ = train_model(train_1.append([train_4, train_3]), test_1.append([test_4, test_3]))\n",
    "\n",
    "    beta3, bic2, RMSE3, _ = train_model(train_1.append([train_2, train_4]), test_1.append([test_2, test_4]))\n",
    "\n",
    "    beta1, bic1, RMSE1, _ = train_model(train_4.append([train_2, train_3]), test_4.append([test_2, test_3]))\n",
    "\n",
    "    return sum([RMSE1, RMSE2, RMSE3, RMSE4]) / 4, sum([bic1, bic2, bic3, bic4]) / 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_sq\n",
      "bic: 9.626303946636282e+16\n",
      "rmse: 4606838.482945066\n",
      "\n",
      "office_sqm_5000\n",
      "bic: 9.236088392607434e+16\n",
      "rmse: 4509890.259458901\n",
      "\n",
      "church_count_500\n",
      "bic: 8.745980531475309e+16\n",
      "rmse: 4390793.45502309\n",
      "\n",
      "prom_part_2000\n",
      "bic: 8.511633546360696e+16\n",
      "rmse: 4333342.953272082\n",
      "\n",
      "kitch_sq\n",
      "bic: 8.310655887323078e+16\n",
      "rmse: 4281289.175857935\n",
      "\n",
      "life_sq\n",
      "bic: 8.215130287457203e+16\n",
      "rmse: 4254329.778610688\n",
      "\n",
      "leisure_count_1000\n",
      "bic: 8.161106327089005e+16\n",
      "rmse: 4236389.35523205\n",
      "\n",
      "big_church_count_1500\n",
      "bic: 8.058786114158811e+16\n",
      "rmse: 4215470.631885044\n",
      "\n",
      "mosque_km\n",
      "bic: 7.944348729565469e+16\n",
      "rmse: 4185852.331020752\n",
      "\n",
      "cafe_count_1500_price_1500\n",
      "bic: 7.896509252549352e+16\n",
      "rmse: 4169259.28286633\n",
      "\n",
      "office_sqm_1500\n",
      "bic: 7.847669871204554e+16\n",
      "rmse: 4160174.121158242\n",
      "\n",
      "cafe_count_5000\n",
      "bic: 7.824552931615512e+16\n",
      "rmse: 4152199.015860779\n",
      "\n",
      "office_count_3000\n",
      "bic: 7.778728666210142e+16\n",
      "rmse: 4142233.1379193245\n",
      "\n",
      "cafe_count_500_price_4000\n",
      "bic: 7.692101040364656e+16\n",
      "rmse: 4116071.3189909803\n",
      "\n",
      "green_part_2000\n",
      "bic: 7.674373145853638e+16\n",
      "rmse: 4113657.250657238\n",
      "\n",
      "build_count_mix\n",
      "bic: 7.680311786238806e+16\n",
      "rmse: 4113563.837153145\n",
      "\n",
      "floor\n",
      "bic: 7.65857609108639e+16\n",
      "rmse: 4106396.516268879\n",
      "\n",
      "male_f\n",
      "bic: 7.633186254849541e+16\n",
      "rmse: 4101568.63152971\n",
      "\n",
      "build_count_1971-1995\n",
      "bic: 7.620298167931363e+16\n",
      "rmse: 4096131.4150387836\n",
      "\n",
      "num_room\n",
      "bic: 7.55289604229202e+16\n",
      "rmse: 4080960.3420264814\n",
      "\n",
      "leisure_count_1500\n",
      "bic: 7.5228673102557e+16\n",
      "rmse: 4070437.3257457744\n",
      "\n",
      "state\n",
      "bic: 7.511986503782722e+16\n",
      "rmse: 4062326.489717514\n",
      "\n",
      "fitness_km\n",
      "bic: 7.448653987398805e+16\n",
      "rmse: 4052349.3174064844\n",
      "\n",
      "hospital_beds_raion\n",
      "bic: 7.44006842257685e+16\n",
      "rmse: 4050129.361337675\n",
      "\n",
      "leisure_count_2000\n",
      "bic: 7.40338991329617e+16\n",
      "rmse: 4036121.3174029146\n",
      "\n",
      "cafe_count_1000_price_500\n",
      "bic: 7.36416006952318e+16\n",
      "rmse: 4027147.8220237615\n",
      "\n",
      "cafe_count_2000_na_price\n",
      "bic: 7.340314366723547e+16\n",
      "rmse: 4021216.153837182\n",
      "\n",
      "cafe_count_3000_price_4000\n",
      "bic: 7.339829964297309e+16\n",
      "rmse: 4023368.990371015\n",
      "\n",
      "public_transport_station_min_walk\n",
      "bic: 7.335725917973778e+16\n",
      "rmse: 4017454.945213433\n",
      "\n",
      "ttk_km\n",
      "bic: 7.325136442805432e+16\n",
      "rmse: 4016147.6017246516\n",
      "\n",
      "office_sqm_3000\n",
      "bic: 7.321340653615309e+16\n",
      "rmse: 4016395.618801268\n",
      "\n",
      "big_church_km\n",
      "bic: 7.3188685089022e+16\n",
      "rmse: 4017113.320693792\n",
      "\n",
      "office_sqm_2000\n",
      "bic: 7.294042370722938e+16\n",
      "rmse: 4009411.493887178\n",
      "\n",
      "workplaces_km\n",
      "bic: 7.281308376002829e+16\n",
      "rmse: 4006567.4108336167\n",
      "\n",
      "green_part_1500\n",
      "bic: 7.283128265839187e+16\n",
      "rmse: 4006954.939855951\n",
      "\n",
      "water_km\n",
      "bic: 7.281810275649462e+16\n",
      "rmse: 4004843.3385597537\n",
      "\n",
      "preschool_quota\n",
      "bic: 7.287185139960459e+16\n",
      "rmse: 4007230.1812344743\n",
      "\n",
      "catering_km\n",
      "bic: 7.272089616127267e+16\n",
      "rmse: 4000037.0520485314\n",
      "\n",
      "prom_part_1000\n",
      "bic: 7.271318797496293e+16\n",
      "rmse: 4002231.2540524206\n",
      "\n",
      "market_count_500\n",
      "bic: 7.273189166865533e+16\n",
      "rmse: 4003182.6914754473\n",
      "\n",
      "school_quota\n",
      "bic: 7.278280364131069e+16\n",
      "rmse: 4004422.5878490945\n",
      "\n",
      "office_sqm_1000\n",
      "bic: 7.262241911605189e+16\n",
      "rmse: 4000388.503377341\n",
      "\n",
      "cafe_count_5000_price_2500\n",
      "bic: 7.243400948616074e+16\n",
      "rmse: 3995650.816055692\n",
      "\n",
      "cafe_avg_price_500\n",
      "bic: 7.248550807309618e+16\n",
      "rmse: 3995025.400789014\n",
      "\n",
      "cafe_count_5000_price_1500\n",
      "bic: 7.259119978465805e+16\n",
      "rmse: 3996775.3238605307\n",
      "\n",
      "radiation_km\n",
      "bic: 7.246412109926864e+16\n",
      "rmse: 3993969.4230149486\n",
      "\n",
      "trc_sqm_1000\n",
      "bic: 7.219533704230445e+16\n",
      "rmse: 3989007.373247315\n",
      "\n",
      "church_count_2000\n",
      "bic: 7.235494204335008e+16\n",
      "rmse: 3994145.1400378947\n",
      "\n",
      "young_female\n",
      "bic: 7.205725760616661e+16\n",
      "rmse: 3984362.623183224\n",
      "\n",
      "build_count_foam\n",
      "bic: 7.22759579138958e+16\n",
      "rmse: 3991356.7085112464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# forward cross validated stepwise\n",
    "\n",
    "train_data = X_train.copy()\n",
    "df_with_col = pd.DataFrame()\n",
    "\n",
    "#min_RMSE, min_bic = get_average_RMSE_for_one_model(train_data, y_train_sample)\n",
    "\n",
    "col_added = []\n",
    "rmses = []\n",
    "\n",
    "for i in range(0,50):\n",
    "\n",
    "    min_col = None\n",
    "    min_RMSE = 100000000000000000000\n",
    "    min_bic = 10000000000000000000\n",
    "\n",
    "    for col in train_data.columns:\n",
    "\n",
    "        df_with_col[col] = train_data[col]\n",
    "        \n",
    "        new_RMSE, new_bic = get_average_RMSE_for_one_model(df_with_col, y_train)\n",
    "                \n",
    "        if new_RMSE[0] <= min_RMSE:\n",
    "            min_bic = new_bic[0]\n",
    "            min_RMSE = new_RMSE[0]\n",
    "            min_col = col\n",
    "            \n",
    "        df_with_col.drop(columns=[col], inplace=True)\n",
    "\n",
    "    if min_col is not None:\n",
    "        print(min_col)\n",
    "        df_with_col[min_col] = train_data[min_col]\n",
    "        train_data.drop(columns=[min_col], inplace=True)\n",
    "        rmses.append(min_RMSE)\n",
    "        print(\"bic: \" + str(min_bic))\n",
    "        print(\"rmse: \" + str(min_RMSE))\n",
    "        col_added.append(min_col)\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(\"Failed #2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd76bd9d630>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RMSEs\n",
    "plt.plot(range(0,50), rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['full_sq',\n",
       " 'office_sqm_5000',\n",
       " 'church_count_500',\n",
       " 'prom_part_2000',\n",
       " 'kitch_sq',\n",
       " 'life_sq',\n",
       " 'leisure_count_1000',\n",
       " 'big_church_count_1500',\n",
       " 'mosque_km',\n",
       " 'cafe_count_1500_price_1500',\n",
       " 'office_sqm_1500',\n",
       " 'cafe_count_5000',\n",
       " 'office_count_3000',\n",
       " 'cafe_count_500_price_4000',\n",
       " 'green_part_2000',\n",
       " 'build_count_mix',\n",
       " 'floor',\n",
       " 'male_f',\n",
       " 'build_count_1971-1995',\n",
       " 'num_room',\n",
       " 'leisure_count_1500',\n",
       " 'state',\n",
       " 'fitness_km',\n",
       " 'hospital_beds_raion',\n",
       " 'leisure_count_2000',\n",
       " 'cafe_count_1000_price_500',\n",
       " 'cafe_count_2000_na_price',\n",
       " 'cafe_count_3000_price_4000',\n",
       " 'public_transport_station_min_walk',\n",
       " 'ttk_km',\n",
       " 'office_sqm_3000',\n",
       " 'big_church_km',\n",
       " 'office_sqm_2000',\n",
       " 'workplaces_km',\n",
       " 'green_part_1500',\n",
       " 'water_km',\n",
       " 'preschool_quota',\n",
       " 'catering_km',\n",
       " 'prom_part_1000',\n",
       " 'market_count_500',\n",
       " 'school_quota',\n",
       " 'office_sqm_1000',\n",
       " 'cafe_count_5000_price_2500',\n",
       " 'cafe_avg_price_500',\n",
       " 'cafe_count_5000_price_1500',\n",
       " 'radiation_km',\n",
       " 'trc_sqm_1000',\n",
       " 'church_count_2000',\n",
       " 'young_female',\n",
       " 'build_count_foam']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
