{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full unprocessed dataset\n",
    "X_train = pd.read_csv(\"/datasets/sberbank-russian-housing-market/train.csv\", index_col=0)\n",
    "df_macro = pd.read_csv(\"/datasets/sberbank-russian-housing-market/macro.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping na's and processing ONLY for the forward stepwise\n",
    "X_train.dropna(inplace=True)\n",
    "X_train.reset_index(inplace=True)\n",
    "X_train.drop(columns=[\"id\"], inplace=True)\n",
    "X_train = X_train.select_dtypes(include=np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train[[\"price_doc\"]]\n",
    "X_train = X_train.drop(\"price_doc\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample = X_train[[\"full_sq\", \"life_sq\",\"floor\"]].fillna(0).sample(5000)\n",
    "y_train_sample = y_train.loc[X_train_sample.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entire normalized and cleaned dataset\n",
    "X_train = pd.read_csv(\"../Data/df_train_vvfinal\").drop(\"Unnamed: 0\", axis = 1)\n",
    "y_train = pd.read_csv(\"../Data/y_train_vfinal\").drop(\"Unnamed: 0\", axis = 1)\n",
    "X_train_normalize = (X_train - X_train.mean())/X_train.std()\n",
    "y_train_normalize = (y_train - y_train.mean())/y_train.std()\n",
    "\n",
    "X_val = pd.read_csv(\"../Data/df_val_vvfinal\").drop(\"Unnamed: 0\", axis = 1)\n",
    "X_val.at[591, \"build_year\"] = 2009\n",
    "y_val = pd.read_csv(\"../Data/y_val_vfinal\").drop(\"Unnamed: 0\", axis = 1)\n",
    "X_val_normalize = (X_val - X_train.mean())/X_train.std()\n",
    "y_val_normalize = (y_val - y_train.mean())/y_train.std()\n",
    "\n",
    "X_test = pd.read_csv(\"../Data/df_test_vvfinal\").drop(\"Unnamed: 0\", axis = 1)\n",
    "y_test = pd.read_csv(\"../Data/y_test_vfinal\").drop(\"Unnamed: 0\", axis = 1)\n",
    "X_test_normalize = (X_test - X_train.mean())/X_train.std()\n",
    "y_test_normalize = (y_test - y_train.mean())/y_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(): \n",
    "    def fit(self, X, y, n_hidden, nodes, activations, lr, validation_X, validation_y, batch_size = 0):\n",
    "        self._lr = lr\n",
    "        self._X = X.values\n",
    "        self._y = y.values\n",
    "        self._n_hidden = n_hidden\n",
    "        self._nodes = nodes\n",
    "        self._weights = self._generate_weights()\n",
    "        self._biases = self._generate_bias()\n",
    "        self._activations = activations\n",
    "        self._forward_inputs = []\n",
    "        self._val_X = validation_X.values\n",
    "        self._val_y = validation_y.values\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "\n",
    "        return self._train()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pred = X\n",
    "        weights = self._weights\n",
    "        biases = self._biases\n",
    "        activations = self._activations[1:-1]\n",
    "    \n",
    "        for idx, layer in enumerate(weights):\n",
    "            if idx == (len(weights) - 1):\n",
    "                pred = (pred @ layer) + biases[idx].T,\n",
    "            else:\n",
    "                weight_output = (pred @ layer) + biases[idx].T\n",
    "                pred = self._activation(data = weight_output, activation = activations[idx])\n",
    "\n",
    "        return pred[0]\n",
    "        \n",
    "    def _activation(self, data, activation = \"relu\"):\n",
    "        if activation == \"relu\":\n",
    "            def relu(data):\n",
    "                return np.array([max(0,i) for i in data]).reshape(data.shape)\n",
    "            return np.apply_along_axis(relu, 1, data)\n",
    "        if activation == \"sigmoid\":\n",
    "            def sigmoid(data):\n",
    "                return (1/(1 + np.exp(-data))).reshape(data.shape)\n",
    "            return np.apply_along_axis(sigmoid, 1, data)\n",
    "    \n",
    "    def _der_activation(self, points, activation = \"relu\"):\n",
    "        if activation == \"relu\":\n",
    "            def d_relu(point):\n",
    "                return np.array([0 if y <= 0 else 1 for y in point])\n",
    "            return np.apply_along_axis(d_relu, 1, points)\n",
    "        if activation == \"sigmoid\":\n",
    "            ## todo\n",
    "            return\n",
    "    \n",
    "    def _loss_function(self, ypred, loss = \"l2\"):\n",
    "        y = self._val_y\n",
    "        if loss == \"mse\":\n",
    "            return ((ypred - y) ** 2).mean()\n",
    "        if loss == \"l2\":\n",
    "            return (((ypred - y) ** 2)/2).mean()\n",
    "    \n",
    "    def _loss_jacobian(self, ypred, loss = \"l2\"):\n",
    "        if self._batch_size > 0:\n",
    "            y = self._batchy\n",
    "        else:\n",
    "            y = self._y\n",
    "        if loss == \"l2\":\n",
    "            return (ypred - y)/(len(ypred))\n",
    "    \n",
    "    def _generate_weights(self):\n",
    "        hidden_weights = []\n",
    "        nodes = self._nodes\n",
    "        for idx in range(1,len(nodes)):\n",
    "            hidden_weights.append(0.1 * np.random.randn(nodes[idx -1], nodes[idx]))\n",
    "            #hidden_weights.append(0.01 *np.random.randn(nodes[idx -1], nodes[idx]))\n",
    "\n",
    "        return hidden_weights\n",
    "    \n",
    "    def _generate_bias(self):\n",
    "        hidden_layers = []\n",
    "        nodes = self._nodes\n",
    "        for i in range(self._n_hidden + 1):\n",
    "            hidden_layers.append(np.zeros((nodes[i + 1], 1)))\n",
    "        return hidden_layers\n",
    "    \n",
    "    \n",
    "    def _forward_propagation(self):\n",
    "        \"\"\"\n",
    "        Suppose 2 observations\n",
    "        \n",
    "        Suppose previous layer is 3 nodes\n",
    "        Suppose current layer is 2 nodes\n",
    "        \n",
    "        prev shape (2,3)\n",
    "        prev = ob1 [prev_node_1 val, prev_node_2 val, prev_node_3 val]\n",
    "               ob2 [prev_node_1 val, prev_node_2 val, prev_node_3 val]\n",
    "               \n",
    "        layer shape (3,2)\n",
    "        layer = [weight for current_node_1 for prev_node_1, weight for current_node_2 for prev_node_1]\n",
    "                [weight for current_node_1 for prev_node_2, weight for current_node_2 for prev_node_2]\n",
    "                [weight for current_node_1 for prev_node_3, weight for current_node_2 for prev_node_3]\n",
    "                \n",
    "        output shape (2,2) # since 2 observations and 2 layers\n",
    "        output = ob1 [current_node_1 val, current_node_2 val]\n",
    "                 ob2 [current_node_1 val, current_node_2 val]\n",
    "                 \n",
    "        Then for bias in current layer it is (2,1) since 2 nodes in current layer\n",
    "        \n",
    "        So for each row in output we add the bias row wise and apply the activation function to each row\n",
    "        \n",
    "        prev <- ouput\n",
    "        \n",
    "        Move onto next layer...\n",
    "        \"\"\"\n",
    "        if self._batch_size > 0:\n",
    "            prev = self._batchX\n",
    "        else:\n",
    "            prev = self._X\n",
    "        weights = self._weights\n",
    "        biases = self._biases\n",
    "        activations = self._activations[1:-1]\n",
    "    \n",
    "        for idx, layer in enumerate(weights):\n",
    "            if idx == (len(weights) - 1):\n",
    "                self._forward_inputs.append((prev, None))\n",
    "                prev = (prev @ layer) + biases[idx].T,\n",
    "            else:\n",
    "                weight_output = (prev @ layer) + biases[idx].T\n",
    "                self._forward_inputs.append((prev, weight_output))\n",
    "                prev = self._activation(data = weight_output, activation = activations[idx])\n",
    "\n",
    "        return prev\n",
    "    \n",
    "    def _backward_propagation(self, ypred):\n",
    "            \n",
    "        j = self._loss_jacobian(ypred)\n",
    "                \n",
    "        for i in range(len(self._forward_inputs)-1, -1, -1):\n",
    "            if i != (len(self._forward_inputs) - 1):\n",
    "                # activation func on all layers except the last\n",
    "                der_acti = self._der_activation(self._forward_inputs[i][1])\n",
    "                j = np.multiply(j,der_acti)\n",
    "\n",
    "            x = self._forward_inputs[i][0]\n",
    "\n",
    "            jw = x.T.dot(j)\n",
    "\n",
    "            b = np.ones((j.shape[0],1))\n",
    "            jb = j.T.dot(b)\n",
    "            \n",
    "            j = j.dot(self._weights[i].T)\n",
    "            \n",
    "            self._weights[i] -= self._lr * jw\n",
    "            self._biases[i] -= self._lr * jb\n",
    "            \n",
    "        self._forward_inputs = []        \n",
    "    \n",
    "    def _train(self):\n",
    "        min_loss = old_loss = np.inf\n",
    "        losses = []\n",
    "        mses = []\n",
    "        tol = 0.00001\n",
    "        terminate_count = anneal_count = step_count = 0\n",
    "        while True:\n",
    "            if self._batch_size > 0:\n",
    "                X_index = np.arange(self._X.shape[0])\n",
    "                np.random.shuffle(X_index)\n",
    "                batch_index = X_index[:self._batch_size]\n",
    "                self._batchX = self._X[batch_index,:]\n",
    "                self._batchy = self._y[batch_index,:]\n",
    "            \n",
    "            batched_out = self._forward_propagation()\n",
    "            validation_out = self.predict(self._val_X)\n",
    "                \n",
    "            loss = self._loss_function(validation_out)\n",
    "            mse = self._loss_function(validation_out, loss = \"mse\")\n",
    "            print(\"\\nloss:\")\n",
    "            print(loss)\n",
    "            print(\"mse:\")\n",
    "            print(mse)\n",
    "            if loss <= min_loss:\n",
    "                min_loss = loss\n",
    "                terminate_count = anneal_count = 0\n",
    "            if loss <= old_loss:\n",
    "                anneal_count = 0\n",
    "            else:\n",
    "                terminate_count += 1\n",
    "                anneal_count += 1\n",
    "                print(\"INCREASE IN LOSS\")\n",
    "                if anneal_count >= 2:\n",
    "                    anneal_count = 0\n",
    "                    self._lr = self._lr / 2\n",
    "                    print(\"Decreasing learning rate. New rate is \" + str(self._lr))\n",
    "                if terminate_count > 20:\n",
    "                    break\n",
    "            if step_count > 25:\n",
    "                self._lr = self._lr * 0.9\n",
    "                print(\"Annealing learning rate. New rate is \" + str(self._lr))\n",
    "                step_count = 0\n",
    "            if self._lr < tol:\n",
    "                break\n",
    "            \n",
    "            losses.append(loss)\n",
    "            mses.append(mse)\n",
    "            self._backward_propagation(batched_out[0])\n",
    "            old_loss = loss\n",
    "            step_count += 1\n",
    "            \n",
    "        return losses, mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = X_train_normalize.shape[1]\n",
    "OUTPUT_SIZE = 1\n",
    "LEARNING_RATE = 0.5\n",
    "nodes = [INPUT_SIZE,50,OUTPUT_SIZE]\n",
    "activations = [\"relu\" for i in range(len(nodes))]\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "losses, mses = nn.fit(X = X_train_normalize,\n",
    "       y = y_train_normalize,\n",
    "       n_hidden = len(nodes) - 2,\n",
    "       nodes = nodes,\n",
    "       activations = activations,\n",
    "       lr = LEARNING_RATE,\n",
    "       validation_X = X_val_normalize,\n",
    "       validation_y = y_val_normalize,\n",
    "       batch_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and MSE Plot\n",
    "plt.plot(range(0,len(mses)), mses, color='red', linewidth=2, label=\"mse\")\n",
    "plt.plot(range(0,len(mses)), losses, color='blue', linewidth=2, linestyle='dashed', label=\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRID SEARCH\n",
    "INPUT_SIZE = X_train_normalize.shape[1]\n",
    "OUTPUT_SIZE = 1\n",
    "LEARNING_RATE = 0.5\n",
    "\n",
    "\n",
    "batch_size_param = [64, 128, 512, 1024, 2048, 5000, 10000]\n",
    "width_param = [34, 50, 75, 100]\n",
    "depth_param = [1, 5, 7, 8, 10]\n",
    "\n",
    "for batch in batch_size_param:\n",
    "    for width in width_param:\n",
    "        for depth in depth_param:\n",
    "            \n",
    "            min_losses = []\n",
    "            min_mses = []\n",
    "            \n",
    "            for i in range(0,5):\n",
    "                nodes = [INPUT_SIZE] + [width for i in range(depth)] + [OUTPUT_SIZE]\n",
    "                activations = [\"relu\" for i in range(len(nodes))]\n",
    "\n",
    "                nn = NeuralNetwork()\n",
    "\n",
    "                losses, mses = nn.fit(X = X_train_normalize,\n",
    "                       y = y_train_normalize,\n",
    "                       n_hidden = len(nodes) - 2,\n",
    "                       nodes = nodes,\n",
    "                       activations = activations,\n",
    "                       lr = LEARNING_RATE,\n",
    "                       validation_X = X_val_normalize,\n",
    "                       validation_y = y_val_normalize,\n",
    "                       batch_size = batch)\n",
    "\n",
    "                print((batch, width, depth))\n",
    "                print(min(losses), min(mses))\n",
    "                min_losses.append(min(losses))\n",
    "                min_mses.append(min(mses))\n",
    "                \n",
    "                plt.figure()\n",
    "                plt.plot(range(0,len(mses)), mses, color='red', linewidth=2, label=\"mse\")\n",
    "                plt.plot(range(0,len(mses)), losses, color='blue', linewidth=2, linestyle='dashed', label=\"loss\")\n",
    "                plt.legend()\n",
    "            \n",
    "            print(mean(min_losses), mean(mses)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1x34 (batch = 0)\n",
    "batch = 0\n",
    "width = 34\n",
    "depth = 1\n",
    "\n",
    "INPUT_SIZE = X_train_normalize.shape[1]\n",
    "OUTPUT_SIZE = 1\n",
    "LEARNING_RATE = 0.5\n",
    "\n",
    "nodes = [INPUT_SIZE] + [width for i in range(depth)] + [OUTPUT_SIZE]\n",
    "activations = [\"relu\" for i in range(len(nodes))]\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "losses, mses = nn.fit(X = X_train_normalize,\n",
    "                      y = y_train_normalize,\n",
    "                      n_hidden = len(nodes) - 2,\n",
    "                      nodes = nodes,\n",
    "                      activations = activations,\n",
    "                      lr = LEARNING_RATE,\n",
    "                      validation_X = X_val_normalize,\n",
    "                      validation_y = y_val_normalize,\n",
    "                      batch_size = batch)\n",
    "\n",
    "# RESULTS\n",
    "## loss = 0.186\n",
    "## mse = 0.373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss:\n",
      "0.44545745594455444\n",
      "mse:\n",
      "0.8909149118891089\n",
      "\n",
      "loss:\n",
      "0.44476697627248707\n",
      "mse:\n",
      "0.8895339525449741\n",
      "\n",
      "loss:\n",
      "0.4441709209086317\n",
      "mse:\n",
      "0.8883418418172634\n",
      "\n",
      "loss:\n",
      "0.4436311719464357\n",
      "mse:\n",
      "0.8872623438928714\n",
      "\n",
      "loss:\n",
      "0.44310508053157993\n",
      "mse:\n",
      "0.8862101610631599\n",
      "\n",
      "loss:\n",
      "0.44253799022408846\n",
      "mse:\n",
      "0.8850759804481769\n",
      "\n",
      "loss:\n",
      "0.4418881615280752\n",
      "mse:\n",
      "0.8837763230561504\n",
      "\n",
      "loss:\n",
      "0.44112366846375145\n",
      "mse:\n",
      "0.8822473369275029\n",
      "\n",
      "loss:\n",
      "0.4402101149591218\n",
      "mse:\n",
      "0.8804202299182436\n",
      "\n",
      "loss:\n",
      "0.4390900336408204\n",
      "mse:\n",
      "0.8781800672816408\n",
      "\n",
      "loss:\n",
      "0.43769689786521915\n",
      "mse:\n",
      "0.8753937957304383\n",
      "\n",
      "loss:\n",
      "0.4360285389182484\n",
      "mse:\n",
      "0.8720570778364968\n",
      "\n",
      "loss:\n",
      "0.43397542786908494\n",
      "mse:\n",
      "0.8679508557381699\n",
      "\n",
      "loss:\n",
      "0.43139078577247086\n",
      "mse:\n",
      "0.8627815715449417\n",
      "\n",
      "loss:\n",
      "0.428067997034054\n",
      "mse:\n",
      "0.856135994068108\n",
      "\n",
      "loss:\n",
      "0.4237332772143261\n",
      "mse:\n",
      "0.8474665544286522\n",
      "\n",
      "loss:\n",
      "0.41800016378300536\n",
      "mse:\n",
      "0.8360003275660107\n",
      "\n",
      "loss:\n",
      "0.41028517708408135\n",
      "mse:\n",
      "0.8205703541681627\n",
      "\n",
      "loss:\n",
      "0.39982574573785934\n",
      "mse:\n",
      "0.7996514914757187\n",
      "\n",
      "loss:\n",
      "0.38560256610850463\n",
      "mse:\n",
      "0.7712051322170093\n",
      "\n",
      "loss:\n",
      "0.3663282126618401\n",
      "mse:\n",
      "0.7326564253236802\n",
      "\n",
      "loss:\n",
      "0.340920143379737\n",
      "mse:\n",
      "0.681840286759474\n",
      "\n",
      "loss:\n",
      "0.3098455887205723\n",
      "mse:\n",
      "0.6196911774411445\n",
      "\n",
      "loss:\n",
      "0.2779497726756089\n",
      "mse:\n",
      "0.5558995453512178\n",
      "\n",
      "loss:\n",
      "0.25863512705871683\n",
      "mse:\n",
      "0.5172702541174337\n",
      "\n",
      "loss:\n",
      "0.25356307187781546\n",
      "mse:\n",
      "0.5071261437556309\n",
      "\n",
      "loss:\n",
      "0.3062977714678168\n",
      "mse:\n",
      "0.6125955429356336\n",
      "INCREASE IN LOSS\n",
      "Annealing learning rate. New rate is 0.675\n",
      "\n",
      "loss:\n",
      "0.7537217425437216\n",
      "mse:\n",
      "1.5074434850874432\n",
      "INCREASE IN LOSS\n",
      "Decreasing learning rate. New rate is 0.3375\n",
      "\n",
      "loss:\n",
      "0.3942687258670202\n",
      "mse:\n",
      "0.7885374517340404\n",
      "\n",
      "loss:\n",
      "0.3368687729831983\n",
      "mse:\n",
      "0.6737375459663966\n",
      "\n",
      "loss:\n",
      "0.28982954888767404\n",
      "mse:\n",
      "0.5796590977753481\n",
      "\n",
      "loss:\n",
      "0.2682575180523766\n",
      "mse:\n",
      "0.5365150361047532\n",
      "\n",
      "loss:\n",
      "0.2578722319033356\n",
      "mse:\n",
      "0.5157444638066712\n",
      "\n",
      "loss:\n",
      "0.2534640421292683\n",
      "mse:\n",
      "0.5069280842585366\n",
      "\n",
      "loss:\n",
      "0.25108203712138205\n",
      "mse:\n",
      "0.5021640742427641\n",
      "\n",
      "loss:\n",
      "0.24972133070627203\n",
      "mse:\n",
      "0.49944266141254406\n",
      "\n",
      "loss:\n",
      "0.24846808398612938\n",
      "mse:\n",
      "0.49693616797225876\n",
      "\n",
      "loss:\n",
      "0.24746999775151116\n",
      "mse:\n",
      "0.4949399955030223\n",
      "\n",
      "loss:\n",
      "0.24631676434423158\n",
      "mse:\n",
      "0.49263352868846316\n",
      "\n",
      "loss:\n",
      "0.24535926374903572\n",
      "mse:\n",
      "0.49071852749807143\n",
      "\n",
      "loss:\n",
      "0.24428439647665764\n",
      "mse:\n",
      "0.4885687929533153\n",
      "\n",
      "loss:\n",
      "0.2434519614953203\n",
      "mse:\n",
      "0.4869039229906406\n",
      "\n",
      "loss:\n",
      "0.24244334529783115\n",
      "mse:\n",
      "0.4848866905956623\n",
      "\n",
      "loss:\n",
      "0.24172500784869966\n",
      "mse:\n",
      "0.4834500156973993\n",
      "\n",
      "loss:\n",
      "0.24070495771828757\n",
      "mse:\n",
      "0.48140991543657513\n",
      "\n",
      "loss:\n",
      "0.24017807772525998\n",
      "mse:\n",
      "0.48035615545051996\n",
      "\n",
      "loss:\n",
      "0.2390408555341887\n",
      "mse:\n",
      "0.4780817110683774\n",
      "\n",
      "loss:\n",
      "0.23888887961211902\n",
      "mse:\n",
      "0.47777775922423804\n",
      "\n",
      "loss:\n",
      "0.23742905670019449\n",
      "mse:\n",
      "0.47485811340038897\n",
      "\n",
      "loss:\n",
      "0.2382348472431282\n",
      "mse:\n",
      "0.4764696944862564\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.2362554935675178\n",
      "mse:\n",
      "0.4725109871350356\n",
      "\n",
      "loss:\n",
      "0.2404589961695434\n",
      "mse:\n",
      "0.4809179923390868\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.2403968479561284\n",
      "mse:\n",
      "0.4807936959122568\n",
      "Annealing learning rate. New rate is 0.30375\n",
      "\n",
      "loss:\n",
      "0.25151206544011395\n",
      "mse:\n",
      "0.5030241308802279\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.2582391054326541\n",
      "mse:\n",
      "0.5164782108653082\n",
      "INCREASE IN LOSS\n",
      "Decreasing learning rate. New rate is 0.151875\n",
      "\n",
      "loss:\n",
      "0.23308557263215898\n",
      "mse:\n",
      "0.46617114526431797\n",
      "\n",
      "loss:\n",
      "0.23301381822391848\n",
      "mse:\n",
      "0.46602763644783696\n",
      "\n",
      "loss:\n",
      "0.23268973414319355\n",
      "mse:\n",
      "0.4653794682863871\n",
      "\n",
      "loss:\n",
      "0.23241041590974\n",
      "mse:\n",
      "0.46482083181948\n",
      "\n",
      "loss:\n",
      "0.23210839965768526\n",
      "mse:\n",
      "0.4642167993153705\n",
      "\n",
      "loss:\n",
      "0.23180382157708831\n",
      "mse:\n",
      "0.46360764315417663\n",
      "\n",
      "loss:\n",
      "0.23148042627537385\n",
      "mse:\n",
      "0.4629608525507477\n",
      "\n",
      "loss:\n",
      "0.23115185086948295\n",
      "mse:\n",
      "0.4623037017389659\n",
      "\n",
      "loss:\n",
      "0.23081845999963493\n",
      "mse:\n",
      "0.46163691999926987\n",
      "\n",
      "loss:\n",
      "0.23047951108099526\n",
      "mse:\n",
      "0.46095902216199053\n",
      "\n",
      "loss:\n",
      "0.23012937540109463\n",
      "mse:\n",
      "0.46025875080218925\n",
      "\n",
      "loss:\n",
      "0.22976831355805485\n",
      "mse:\n",
      "0.4595366271161097\n",
      "\n",
      "loss:\n",
      "0.22940343647421604\n",
      "mse:\n",
      "0.4588068729484321\n",
      "\n",
      "loss:\n",
      "0.22903974663953044\n",
      "mse:\n",
      "0.4580794932790609\n",
      "\n",
      "loss:\n",
      "0.22868213493705034\n",
      "mse:\n",
      "0.45736426987410067\n",
      "\n",
      "loss:\n",
      "0.2283338111484377\n",
      "mse:\n",
      "0.4566676222968754\n",
      "\n",
      "loss:\n",
      "0.22798943084774442\n",
      "mse:\n",
      "0.45597886169548885\n",
      "\n",
      "loss:\n",
      "0.22765491858643983\n",
      "mse:\n",
      "0.45530983717287965\n",
      "\n",
      "loss:\n",
      "0.22732548315111797\n",
      "mse:\n",
      "0.45465096630223595\n",
      "\n",
      "loss:\n",
      "0.22700379109745578\n",
      "mse:\n",
      "0.45400758219491155\n",
      "\n",
      "loss:\n",
      "0.22668522589499687\n",
      "mse:\n",
      "0.45337045178999374\n",
      "\n",
      "loss:\n",
      "0.2263723828396737\n",
      "mse:\n",
      "0.4527447656793474\n",
      "\n",
      "loss:\n",
      "0.22606592529879613\n",
      "mse:\n",
      "0.45213185059759226\n",
      "\n",
      "loss:\n",
      "0.22576258790574247\n",
      "mse:\n",
      "0.45152517581148494\n",
      "Annealing learning rate. New rate is 0.13668750000000002\n",
      "\n",
      "loss:\n",
      "0.22549301229551322\n",
      "mse:\n",
      "0.45098602459102644\n",
      "\n",
      "loss:\n",
      "0.22522517512383528\n",
      "mse:\n",
      "0.45045035024767055\n",
      "\n",
      "loss:\n",
      "0.2249636160179449\n",
      "mse:\n",
      "0.4499272320358898\n",
      "\n",
      "loss:\n",
      "0.22470631118911816\n",
      "mse:\n",
      "0.4494126223782363\n",
      "\n",
      "loss:\n",
      "0.22445406354975367\n",
      "mse:\n",
      "0.44890812709950734\n",
      "\n",
      "loss:\n",
      "0.22420756418660032\n",
      "mse:\n",
      "0.44841512837320063\n",
      "\n",
      "loss:\n",
      "0.22396185934399146\n",
      "mse:\n",
      "0.4479237186879829\n",
      "\n",
      "loss:\n",
      "0.22372001913215506\n",
      "mse:\n",
      "0.4474400382643101\n",
      "\n",
      "loss:\n",
      "0.22348197769734518\n",
      "mse:\n",
      "0.44696395539469036\n",
      "\n",
      "loss:\n",
      "0.2232440737626966\n",
      "mse:\n",
      "0.4464881475253932\n",
      "\n",
      "loss:\n",
      "0.22300914537429248\n",
      "mse:\n",
      "0.44601829074858496\n",
      "\n",
      "loss:\n",
      "0.222775866883047\n",
      "mse:\n",
      "0.445551733766094\n",
      "\n",
      "loss:\n",
      "0.22254797844503948\n",
      "mse:\n",
      "0.44509595689007897\n",
      "\n",
      "loss:\n",
      "0.22232183043816428\n",
      "mse:\n",
      "0.44464366087632856\n",
      "\n",
      "loss:\n",
      "0.22210018689360422\n",
      "mse:\n",
      "0.44420037378720845\n",
      "\n",
      "loss:\n",
      "0.22188157817300752\n",
      "mse:\n",
      "0.44376315634601504\n",
      "\n",
      "loss:\n",
      "0.2216599024412911\n",
      "mse:\n",
      "0.4433198048825822\n",
      "\n",
      "loss:\n",
      "0.22143842804973657\n",
      "mse:\n",
      "0.44287685609947314\n",
      "\n",
      "loss:\n",
      "0.22121830349701788\n",
      "mse:\n",
      "0.44243660699403575\n",
      "\n",
      "loss:\n",
      "0.22100006367265174\n",
      "mse:\n",
      "0.4420001273453035\n",
      "\n",
      "loss:\n",
      "0.22078190500437778\n",
      "mse:\n",
      "0.44156381000875555\n",
      "\n",
      "loss:\n",
      "0.22056426593232387\n",
      "mse:\n",
      "0.44112853186464773\n",
      "\n",
      "loss:\n",
      "0.22035040230508965\n",
      "mse:\n",
      "0.4407008046101793\n",
      "\n",
      "loss:\n",
      "0.22013312451881012\n",
      "mse:\n",
      "0.44026624903762024\n",
      "\n",
      "loss:\n",
      "0.21991699618733984\n",
      "mse:\n",
      "0.4398339923746797\n",
      "\n",
      "loss:\n",
      "0.21970478177838432\n",
      "mse:\n",
      "0.43940956355676863\n",
      "Annealing learning rate. New rate is 0.12301875000000002\n",
      "\n",
      "loss:\n",
      "0.21951059705368314\n",
      "mse:\n",
      "0.4390211941073663\n",
      "\n",
      "loss:\n",
      "0.21931853032173526\n",
      "mse:\n",
      "0.4386370606434705\n",
      "\n",
      "loss:\n",
      "0.2191235953328457\n",
      "mse:\n",
      "0.4382471906656914\n",
      "\n",
      "loss:\n",
      "0.2189348374460786\n",
      "mse:\n",
      "0.4378696748921572\n",
      "\n",
      "loss:\n",
      "0.21874398439994\n",
      "mse:\n",
      "0.43748796879988\n",
      "\n",
      "loss:\n",
      "0.2185530670670332\n",
      "mse:\n",
      "0.4371061341340664\n",
      "\n",
      "loss:\n",
      "0.21836470931718488\n",
      "mse:\n",
      "0.43672941863436976\n",
      "\n",
      "loss:\n",
      "0.21817445000747354\n",
      "mse:\n",
      "0.4363489000149471\n",
      "\n",
      "loss:\n",
      "0.21798889099116292\n",
      "mse:\n",
      "0.43597778198232584\n",
      "\n",
      "loss:\n",
      "0.21780467504224907\n",
      "mse:\n",
      "0.43560935008449814\n",
      "\n",
      "loss:\n",
      "0.21761714844382896\n",
      "mse:\n",
      "0.4352342968876579\n",
      "\n",
      "loss:\n",
      "0.21743608404555803\n",
      "mse:\n",
      "0.43487216809111606\n",
      "\n",
      "loss:\n",
      "0.21725231905310646\n",
      "mse:\n",
      "0.4345046381062129\n",
      "\n",
      "loss:\n",
      "0.2170697825902557\n",
      "mse:\n",
      "0.4341395651805114\n",
      "\n",
      "loss:\n",
      "0.21688614590192967\n",
      "mse:\n",
      "0.43377229180385934\n",
      "\n",
      "loss:\n",
      "0.2167090771329272\n",
      "mse:\n",
      "0.4334181542658544\n",
      "\n",
      "loss:\n",
      "0.21652157658737872\n",
      "mse:\n",
      "0.43304315317475744\n",
      "\n",
      "loss:\n",
      "0.2163391026817852\n",
      "mse:\n",
      "0.4326782053635704\n",
      "\n",
      "loss:\n",
      "0.2161493805103197\n",
      "mse:\n",
      "0.4322987610206394\n",
      "\n",
      "loss:\n",
      "0.21596546704219902\n",
      "mse:\n",
      "0.43193093408439803\n",
      "\n",
      "loss:\n",
      "0.21577443131916135\n",
      "mse:\n",
      "0.4315488626383227\n",
      "\n",
      "loss:\n",
      "0.21559457512625155\n",
      "mse:\n",
      "0.4311891502525031\n",
      "\n",
      "loss:\n",
      "0.2154038293300445\n",
      "mse:\n",
      "0.430807658660089\n",
      "\n",
      "loss:\n",
      "0.21521974542619512\n",
      "mse:\n",
      "0.43043949085239025\n",
      "\n",
      "loss:\n",
      "0.21503329129005813\n",
      "mse:\n",
      "0.43006658258011626\n",
      "\n",
      "loss:\n",
      "0.2148497930627552\n",
      "mse:\n",
      "0.4296995861255104\n",
      "Annealing learning rate. New rate is 0.11071687500000002\n",
      "\n",
      "loss:\n",
      "0.21468639202617484\n",
      "mse:\n",
      "0.4293727840523497\n",
      "\n",
      "loss:\n",
      "0.21452200658729662\n",
      "mse:\n",
      "0.42904401317459323\n",
      "\n",
      "loss:\n",
      "0.21435555499578735\n",
      "mse:\n",
      "0.4287111099915747\n",
      "\n",
      "loss:\n",
      "0.21419102746436464\n",
      "mse:\n",
      "0.4283820549287293\n",
      "\n",
      "loss:\n",
      "0.21402596321158598\n",
      "mse:\n",
      "0.42805192642317197\n",
      "\n",
      "loss:\n",
      "0.21386488724412966\n",
      "mse:\n",
      "0.42772977448825933\n",
      "\n",
      "loss:\n",
      "0.21370122976994477\n",
      "mse:\n",
      "0.42740245953988953\n",
      "\n",
      "loss:\n",
      "0.21353666609060284\n",
      "mse:\n",
      "0.4270733321812057\n",
      "\n",
      "loss:\n",
      "0.2133736032360172\n",
      "mse:\n",
      "0.4267472064720344\n",
      "\n",
      "loss:\n",
      "0.21321250615318685\n",
      "mse:\n",
      "0.4264250123063737\n",
      "\n",
      "loss:\n",
      "0.21305387475233445\n",
      "mse:\n",
      "0.4261077495046689\n",
      "\n",
      "loss:\n",
      "0.21289362289856528\n",
      "mse:\n",
      "0.42578724579713056\n",
      "\n",
      "loss:\n",
      "0.21272971237430152\n",
      "mse:\n",
      "0.42545942474860304\n",
      "\n",
      "loss:\n",
      "0.21256538438698763\n",
      "mse:\n",
      "0.42513076877397526\n",
      "\n",
      "loss:\n",
      "0.21240404779810854\n",
      "mse:\n",
      "0.4248080955962171\n",
      "\n",
      "loss:\n",
      "0.21224128066044148\n",
      "mse:\n",
      "0.42448256132088297\n",
      "\n",
      "loss:\n",
      "0.21207893927334256\n",
      "mse:\n",
      "0.42415787854668513\n",
      "\n",
      "loss:\n",
      "0.21191782516229682\n",
      "mse:\n",
      "0.42383565032459364\n",
      "\n",
      "loss:\n",
      "0.21175644613070885\n",
      "mse:\n",
      "0.4235128922614177\n",
      "\n",
      "loss:\n",
      "0.21159452037592377\n",
      "mse:\n",
      "0.42318904075184754\n",
      "\n",
      "loss:\n",
      "0.21143436493995524\n",
      "mse:\n",
      "0.4228687298799105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss:\n",
      "0.21127712942794866\n",
      "mse:\n",
      "0.4225542588558973\n",
      "\n",
      "loss:\n",
      "0.21111748714587722\n",
      "mse:\n",
      "0.42223497429175444\n",
      "\n",
      "loss:\n",
      "0.21095610483555127\n",
      "mse:\n",
      "0.42191220967110254\n",
      "\n",
      "loss:\n",
      "0.21079823323984953\n",
      "mse:\n",
      "0.42159646647969906\n",
      "\n",
      "loss:\n",
      "0.2106376892661368\n",
      "mse:\n",
      "0.4212753785322736\n",
      "Annealing learning rate. New rate is 0.09964518750000002\n",
      "\n",
      "loss:\n",
      "0.2104966832531492\n",
      "mse:\n",
      "0.4209933665062984\n",
      "\n",
      "loss:\n",
      "0.2103570164500275\n",
      "mse:\n",
      "0.420714032900055\n",
      "\n",
      "loss:\n",
      "0.21021546807368546\n",
      "mse:\n",
      "0.4204309361473709\n",
      "\n",
      "loss:\n",
      "0.21007416361754522\n",
      "mse:\n",
      "0.42014832723509044\n",
      "\n",
      "loss:\n",
      "0.20993077814283642\n",
      "mse:\n",
      "0.41986155628567284\n",
      "\n",
      "loss:\n",
      "0.2097895173742767\n",
      "mse:\n",
      "0.4195790347485534\n",
      "\n",
      "loss:\n",
      "0.20964520463094705\n",
      "mse:\n",
      "0.4192904092618941\n",
      "\n",
      "loss:\n",
      "0.20950473903986333\n",
      "mse:\n",
      "0.41900947807972666\n",
      "\n",
      "loss:\n",
      "0.2093634246442959\n",
      "mse:\n",
      "0.4187268492885918\n",
      "\n",
      "loss:\n",
      "0.2092212210436784\n",
      "mse:\n",
      "0.4184424420873568\n",
      "\n",
      "loss:\n",
      "0.20907664164891154\n",
      "mse:\n",
      "0.4181532832978231\n",
      "\n",
      "loss:\n",
      "0.2089359337503472\n",
      "mse:\n",
      "0.4178718675006944\n",
      "\n",
      "loss:\n",
      "0.20879335884606337\n",
      "mse:\n",
      "0.41758671769212674\n",
      "\n",
      "loss:\n",
      "0.20865416400258477\n",
      "mse:\n",
      "0.41730832800516954\n",
      "\n",
      "loss:\n",
      "0.20851103615141717\n",
      "mse:\n",
      "0.41702207230283433\n",
      "\n",
      "loss:\n",
      "0.20837141624654487\n",
      "mse:\n",
      "0.41674283249308974\n",
      "\n",
      "loss:\n",
      "0.20823151894221237\n",
      "mse:\n",
      "0.41646303788442474\n",
      "\n",
      "loss:\n",
      "0.20809133638035032\n",
      "mse:\n",
      "0.41618267276070064\n",
      "\n",
      "loss:\n",
      "0.20795160216208589\n",
      "mse:\n",
      "0.41590320432417177\n",
      "\n",
      "loss:\n",
      "0.2078140346169838\n",
      "mse:\n",
      "0.4156280692339676\n",
      "\n",
      "loss:\n",
      "0.20767030765450364\n",
      "mse:\n",
      "0.4153406153090073\n",
      "\n",
      "loss:\n",
      "0.20753028378520463\n",
      "mse:\n",
      "0.41506056757040927\n",
      "\n",
      "loss:\n",
      "0.20738613511266654\n",
      "mse:\n",
      "0.4147722702253331\n",
      "\n",
      "loss:\n",
      "0.20724261366959998\n",
      "mse:\n",
      "0.41448522733919996\n",
      "\n",
      "loss:\n",
      "0.20709696391069737\n",
      "mse:\n",
      "0.41419392782139475\n",
      "\n",
      "loss:\n",
      "0.20695361308027105\n",
      "mse:\n",
      "0.4139072261605421\n",
      "Annealing learning rate. New rate is 0.08968066875000003\n",
      "\n",
      "loss:\n",
      "0.20682098291171208\n",
      "mse:\n",
      "0.41364196582342416\n",
      "\n",
      "loss:\n",
      "0.20669375790082253\n",
      "mse:\n",
      "0.41338751580164507\n",
      "\n",
      "loss:\n",
      "0.20656254711767513\n",
      "mse:\n",
      "0.41312509423535027\n",
      "\n",
      "loss:\n",
      "0.20643223753525228\n",
      "mse:\n",
      "0.41286447507050456\n",
      "\n",
      "loss:\n",
      "0.20630330794712762\n",
      "mse:\n",
      "0.41260661589425524\n",
      "\n",
      "loss:\n",
      "0.2061732492951638\n",
      "mse:\n",
      "0.4123464985903276\n",
      "\n",
      "loss:\n",
      "0.2060462539221841\n",
      "mse:\n",
      "0.4120925078443682\n",
      "\n",
      "loss:\n",
      "0.2059175516590323\n",
      "mse:\n",
      "0.4118351033180646\n",
      "\n",
      "loss:\n",
      "0.205791536748399\n",
      "mse:\n",
      "0.411583073496798\n",
      "\n",
      "loss:\n",
      "0.20566226279821484\n",
      "mse:\n",
      "0.4113245255964297\n",
      "\n",
      "loss:\n",
      "0.2055370510761519\n",
      "mse:\n",
      "0.4110741021523038\n",
      "\n",
      "loss:\n",
      "0.2054084624003503\n",
      "mse:\n",
      "0.4108169248007006\n",
      "\n",
      "loss:\n",
      "0.2052810562307745\n",
      "mse:\n",
      "0.410562112461549\n",
      "\n",
      "loss:\n",
      "0.20515491694613916\n",
      "mse:\n",
      "0.4103098338922783\n",
      "\n",
      "loss:\n",
      "0.20502782207308817\n",
      "mse:\n",
      "0.41005564414617635\n",
      "\n",
      "loss:\n",
      "0.20489891242914143\n",
      "mse:\n",
      "0.40979782485828287\n",
      "\n",
      "loss:\n",
      "0.20477370279508744\n",
      "mse:\n",
      "0.4095474055901749\n",
      "\n",
      "loss:\n",
      "0.2046474980178674\n",
      "mse:\n",
      "0.4092949960357348\n",
      "\n",
      "loss:\n",
      "0.20452180908030582\n",
      "mse:\n",
      "0.40904361816061163\n",
      "\n",
      "loss:\n",
      "0.20439565385803224\n",
      "mse:\n",
      "0.4087913077160645\n",
      "\n",
      "loss:\n",
      "0.20426971828047893\n",
      "mse:\n",
      "0.40853943656095787\n",
      "\n",
      "loss:\n",
      "0.20414462951917295\n",
      "mse:\n",
      "0.4082892590383459\n",
      "\n",
      "loss:\n",
      "0.20401671406247185\n",
      "mse:\n",
      "0.4080334281249437\n",
      "\n",
      "loss:\n",
      "0.20388964094535977\n",
      "mse:\n",
      "0.40777928189071955\n",
      "\n",
      "loss:\n",
      "0.20376192805107923\n",
      "mse:\n",
      "0.40752385610215847\n",
      "\n",
      "loss:\n",
      "0.2036365467906824\n",
      "mse:\n",
      "0.4072730935813648\n",
      "Annealing learning rate. New rate is 0.08071260187500003\n",
      "\n",
      "loss:\n",
      "0.2035243275336197\n",
      "mse:\n",
      "0.4070486550672394\n",
      "\n",
      "loss:\n",
      "0.20340879067097295\n",
      "mse:\n",
      "0.4068175813419459\n",
      "\n",
      "loss:\n",
      "0.20329409287901218\n",
      "mse:\n",
      "0.40658818575802436\n",
      "\n",
      "loss:\n",
      "0.2031812118107958\n",
      "mse:\n",
      "0.4063624236215916\n",
      "\n",
      "loss:\n",
      "0.20306887281205901\n",
      "mse:\n",
      "0.40613774562411803\n",
      "\n",
      "loss:\n",
      "0.20295770505023286\n",
      "mse:\n",
      "0.4059154101004657\n",
      "\n",
      "loss:\n",
      "0.20284632721685814\n",
      "mse:\n",
      "0.40569265443371627\n",
      "\n",
      "loss:\n",
      "0.2027300550352407\n",
      "mse:\n",
      "0.4054601100704814\n",
      "\n",
      "loss:\n",
      "0.20261178900896057\n",
      "mse:\n",
      "0.40522357801792114\n",
      "\n",
      "loss:\n",
      "0.20249110924862765\n",
      "mse:\n",
      "0.4049822184972553\n",
      "\n",
      "loss:\n",
      "0.20236872641879444\n",
      "mse:\n",
      "0.4047374528375889\n",
      "\n",
      "loss:\n",
      "0.20224798881617706\n",
      "mse:\n",
      "0.4044959776323541\n",
      "\n",
      "loss:\n",
      "0.20212910364430214\n",
      "mse:\n",
      "0.4042582072886043\n",
      "\n",
      "loss:\n",
      "0.20201113495924228\n",
      "mse:\n",
      "0.40402226991848456\n",
      "\n",
      "loss:\n",
      "0.20189432866490967\n",
      "mse:\n",
      "0.40378865732981933\n",
      "\n",
      "loss:\n",
      "0.2017753419056602\n",
      "mse:\n",
      "0.4035506838113204\n",
      "\n",
      "loss:\n",
      "0.20165674477721235\n",
      "mse:\n",
      "0.4033134895544247\n",
      "\n",
      "loss:\n",
      "0.20153766685668908\n",
      "mse:\n",
      "0.40307533371337817\n",
      "\n",
      "loss:\n",
      "0.20141984403020208\n",
      "mse:\n",
      "0.40283968806040416\n",
      "\n",
      "loss:\n",
      "0.20130197335010752\n",
      "mse:\n",
      "0.40260394670021504\n",
      "\n",
      "loss:\n",
      "0.20118304011233004\n",
      "mse:\n",
      "0.4023660802246601\n",
      "\n",
      "loss:\n",
      "0.20106496042505465\n",
      "mse:\n",
      "0.4021299208501093\n",
      "\n",
      "loss:\n",
      "0.20094649019002872\n",
      "mse:\n",
      "0.40189298038005744\n",
      "\n",
      "loss:\n",
      "0.20083001540486856\n",
      "mse:\n",
      "0.4016600308097371\n",
      "\n",
      "loss:\n",
      "0.20071143264756536\n",
      "mse:\n",
      "0.4014228652951307\n",
      "\n",
      "loss:\n",
      "0.20059352830172167\n",
      "mse:\n",
      "0.40118705660344334\n",
      "Annealing learning rate. New rate is 0.07264134168750003\n",
      "\n",
      "loss:\n",
      "0.2004862864001191\n",
      "mse:\n",
      "0.4009725728002382\n",
      "\n",
      "loss:\n",
      "0.20038024772376742\n",
      "mse:\n",
      "0.40076049544753484\n",
      "\n",
      "loss:\n",
      "0.20027473824069442\n",
      "mse:\n",
      "0.40054947648138883\n",
      "\n",
      "loss:\n",
      "0.20017062672167266\n",
      "mse:\n",
      "0.4003412534433453\n",
      "\n",
      "loss:\n",
      "0.2000665894458625\n",
      "mse:\n",
      "0.400133178891725\n",
      "\n",
      "loss:\n",
      "0.19996322474834008\n",
      "mse:\n",
      "0.39992644949668016\n",
      "\n",
      "loss:\n",
      "0.19985909533058305\n",
      "mse:\n",
      "0.3997181906611661\n",
      "\n",
      "loss:\n",
      "0.19975696026758002\n",
      "mse:\n",
      "0.39951392053516005\n",
      "\n",
      "loss:\n",
      "0.19965493732996553\n",
      "mse:\n",
      "0.39930987465993106\n",
      "\n",
      "loss:\n",
      "0.1995541037690973\n",
      "mse:\n",
      "0.3991082075381946\n",
      "\n",
      "loss:\n",
      "0.19945355908484055\n",
      "mse:\n",
      "0.3989071181696811\n",
      "\n",
      "loss:\n",
      "0.19935057907184078\n",
      "mse:\n",
      "0.39870115814368157\n",
      "\n",
      "loss:\n",
      "0.19924949262603164\n",
      "mse:\n",
      "0.3984989852520633\n",
      "\n",
      "loss:\n",
      "0.1991482213932819\n",
      "mse:\n",
      "0.3982964427865638\n",
      "\n",
      "loss:\n",
      "0.199047560093212\n",
      "mse:\n",
      "0.398095120186424\n",
      "\n",
      "loss:\n",
      "0.19894730589706228\n",
      "mse:\n",
      "0.39789461179412455\n",
      "\n",
      "loss:\n",
      "0.19884504291827088\n",
      "mse:\n",
      "0.39769008583654175\n",
      "\n",
      "loss:\n",
      "0.19874379134574494\n",
      "mse:\n",
      "0.39748758269148987\n",
      "\n",
      "loss:\n",
      "0.1986362340303583\n",
      "mse:\n",
      "0.3972724680607166\n",
      "\n",
      "loss:\n",
      "0.19852800984666397\n",
      "mse:\n",
      "0.39705601969332793\n",
      "\n",
      "loss:\n",
      "0.19842220243801653\n",
      "mse:\n",
      "0.39684440487603306\n",
      "\n",
      "loss:\n",
      "0.19831708977518472\n",
      "mse:\n",
      "0.39663417955036945\n",
      "\n",
      "loss:\n",
      "0.198214776354602\n",
      "mse:\n",
      "0.396429552709204\n",
      "\n",
      "loss:\n",
      "0.19811297347172993\n",
      "mse:\n",
      "0.39622594694345986\n",
      "\n",
      "loss:\n",
      "0.1980120458124556\n",
      "mse:\n",
      "0.3960240916249112\n",
      "\n",
      "loss:\n",
      "0.19791046847663482\n",
      "mse:\n",
      "0.39582093695326964\n",
      "Annealing learning rate. New rate is 0.06537720751875002\n",
      "\n",
      "loss:\n",
      "0.19782076488903919\n",
      "mse:\n",
      "0.39564152977807837\n",
      "\n",
      "loss:\n",
      "0.19773208838448786\n",
      "mse:\n",
      "0.3954641767689757\n",
      "\n",
      "loss:\n",
      "0.19764443113507701\n",
      "mse:\n",
      "0.39528886227015403\n",
      "\n",
      "loss:\n",
      "0.19755808924453286\n",
      "mse:\n",
      "0.3951161784890657\n",
      "\n",
      "loss:\n",
      "0.19747042007269114\n",
      "mse:\n",
      "0.3949408401453823\n",
      "\n",
      "loss:\n",
      "0.1973825642515271\n",
      "mse:\n",
      "0.3947651285030542\n",
      "\n",
      "loss:\n",
      "0.19729621879676343\n",
      "mse:\n",
      "0.39459243759352686\n",
      "\n",
      "loss:\n",
      "0.19720908133237092\n",
      "mse:\n",
      "0.39441816266474183\n",
      "\n",
      "loss:\n",
      "0.19712276428676706\n",
      "mse:\n",
      "0.3942455285735341\n",
      "\n",
      "loss:\n",
      "0.19703659714602365\n",
      "mse:\n",
      "0.3940731942920473\n",
      "\n",
      "loss:\n",
      "0.19695064585847666\n",
      "mse:\n",
      "0.3939012917169533\n",
      "\n",
      "loss:\n",
      "0.19686544395892153\n",
      "mse:\n",
      "0.39373088791784305\n",
      "\n",
      "loss:\n",
      "0.19677871565700625\n",
      "mse:\n",
      "0.3935574313140125\n",
      "\n",
      "loss:\n",
      "0.19669440284117318\n",
      "mse:\n",
      "0.39338880568234635\n",
      "\n",
      "loss:\n",
      "0.19660897607740516\n",
      "mse:\n",
      "0.3932179521548103\n",
      "\n",
      "loss:\n",
      "0.19652350312619124\n",
      "mse:\n",
      "0.3930470062523825\n",
      "\n",
      "loss:\n",
      "0.19643834442700595\n",
      "mse:\n",
      "0.3928766888540119\n",
      "\n",
      "loss:\n",
      "0.1963545511387143\n",
      "mse:\n",
      "0.3927091022774286\n",
      "\n",
      "loss:\n",
      "0.19627088071917456\n",
      "mse:\n",
      "0.3925417614383491\n",
      "\n",
      "loss:\n",
      "0.19618623038860883\n",
      "mse:\n",
      "0.39237246077721766\n",
      "\n",
      "loss:\n",
      "0.1961020941816045\n",
      "mse:\n",
      "0.392204188363209\n",
      "\n",
      "loss:\n",
      "0.19601781636660728\n",
      "mse:\n",
      "0.39203563273321457\n",
      "\n",
      "loss:\n",
      "0.19593378517127458\n",
      "mse:\n",
      "0.39186757034254915\n",
      "\n",
      "loss:\n",
      "0.19584945219216524\n",
      "mse:\n",
      "0.39169890438433047\n",
      "\n",
      "loss:\n",
      "0.19576472679799103\n",
      "mse:\n",
      "0.39152945359598207\n",
      "\n",
      "loss:\n",
      "0.19568077324526312\n",
      "mse:\n",
      "0.39136154649052624\n",
      "Annealing learning rate. New rate is 0.05883948676687502\n",
      "\n",
      "loss:\n",
      "0.19560448209795026\n",
      "mse:\n",
      "0.3912089641959005\n",
      "\n",
      "loss:\n",
      "0.19552747437839527\n",
      "mse:\n",
      "0.39105494875679053\n",
      "\n",
      "loss:\n",
      "0.19545013844247602\n",
      "mse:\n",
      "0.39090027688495205\n",
      "\n",
      "loss:\n",
      "0.1953739162116702\n",
      "mse:\n",
      "0.3907478324233404\n",
      "\n",
      "loss:\n",
      "0.195297968060914\n",
      "mse:\n",
      "0.390595936121828\n",
      "\n",
      "loss:\n",
      "0.19522107821546972\n",
      "mse:\n",
      "0.39044215643093944\n",
      "\n",
      "loss:\n",
      "0.19514590435685852\n",
      "mse:\n",
      "0.39029180871371705\n",
      "\n",
      "loss:\n",
      "0.1950705930454258\n",
      "mse:\n",
      "0.3901411860908516\n",
      "\n",
      "loss:\n",
      "0.19499499867725362\n",
      "mse:\n",
      "0.38998999735450723\n",
      "\n",
      "loss:\n",
      "0.19492145912687361\n",
      "mse:\n",
      "0.38984291825374723\n",
      "\n",
      "loss:\n",
      "0.19484739624065997\n",
      "mse:\n",
      "0.38969479248131994\n",
      "\n",
      "loss:\n",
      "0.19477237647697837\n",
      "mse:\n",
      "0.38954475295395674\n",
      "\n",
      "loss:\n",
      "0.1946968930089796\n",
      "mse:\n",
      "0.3893937860179592\n",
      "\n",
      "loss:\n",
      "0.19462176605936612\n",
      "mse:\n",
      "0.38924353211873225\n",
      "\n",
      "loss:\n",
      "0.1945456556295323\n",
      "mse:\n",
      "0.3890913112590646\n",
      "\n",
      "loss:\n",
      "0.1944693735498957\n",
      "mse:\n",
      "0.3889387470997914\n",
      "\n",
      "loss:\n",
      "0.1943926403068048\n",
      "mse:\n",
      "0.3887852806136096\n",
      "\n",
      "loss:\n",
      "0.19431618909362774\n",
      "mse:\n",
      "0.3886323781872555\n",
      "\n",
      "loss:\n",
      "0.19424026404591033\n",
      "mse:\n",
      "0.38848052809182065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss:\n",
      "0.1941634974894417\n",
      "mse:\n",
      "0.3883269949788834\n",
      "\n",
      "loss:\n",
      "0.19408627850635507\n",
      "mse:\n",
      "0.38817255701271014\n",
      "\n",
      "loss:\n",
      "0.1940108156003381\n",
      "mse:\n",
      "0.3880216312006762\n",
      "\n",
      "loss:\n",
      "0.19393574928641352\n",
      "mse:\n",
      "0.38787149857282704\n",
      "\n",
      "loss:\n",
      "0.1938605748362419\n",
      "mse:\n",
      "0.3877211496724838\n",
      "\n",
      "loss:\n",
      "0.19378431696804874\n",
      "mse:\n",
      "0.3875686339360975\n",
      "\n",
      "loss:\n",
      "0.19370869971338542\n",
      "mse:\n",
      "0.38741739942677084\n",
      "Annealing learning rate. New rate is 0.05295553809018752\n",
      "\n",
      "loss:\n",
      "0.19364125393228473\n",
      "mse:\n",
      "0.38728250786456947\n",
      "\n",
      "loss:\n",
      "0.19357340233840287\n",
      "mse:\n",
      "0.38714680467680573\n",
      "\n",
      "loss:\n",
      "0.1935060561034699\n",
      "mse:\n",
      "0.3870121122069398\n",
      "\n",
      "loss:\n",
      "0.1934384329200525\n",
      "mse:\n",
      "0.386876865840105\n",
      "\n",
      "loss:\n",
      "0.1933694031705529\n",
      "mse:\n",
      "0.3867388063411058\n",
      "\n",
      "loss:\n",
      "0.19330128958276144\n",
      "mse:\n",
      "0.3866025791655229\n",
      "\n",
      "loss:\n",
      "0.19323335399305447\n",
      "mse:\n",
      "0.38646670798610894\n",
      "\n",
      "loss:\n",
      "0.19316516872311715\n",
      "mse:\n",
      "0.3863303374462343\n",
      "\n",
      "loss:\n",
      "0.19309724734873984\n",
      "mse:\n",
      "0.3861944946974797\n",
      "\n",
      "loss:\n",
      "0.1930296864429985\n",
      "mse:\n",
      "0.386059372885997\n",
      "\n",
      "loss:\n",
      "0.19296284676047978\n",
      "mse:\n",
      "0.38592569352095957\n",
      "\n",
      "loss:\n",
      "0.19289579404517035\n",
      "mse:\n",
      "0.3857915880903407\n",
      "\n",
      "loss:\n",
      "0.19282869805481218\n",
      "mse:\n",
      "0.38565739610962435\n",
      "\n",
      "loss:\n",
      "0.19276225243348735\n",
      "mse:\n",
      "0.3855245048669747\n",
      "\n",
      "loss:\n",
      "0.19269524435360438\n",
      "mse:\n",
      "0.38539048870720877\n",
      "\n",
      "loss:\n",
      "0.19262809520320517\n",
      "mse:\n",
      "0.38525619040641035\n",
      "\n",
      "loss:\n",
      "0.19256053106943377\n",
      "mse:\n",
      "0.38512106213886754\n",
      "\n",
      "loss:\n",
      "0.19249360800882295\n",
      "mse:\n",
      "0.3849872160176459\n",
      "\n",
      "loss:\n",
      "0.19242662018207896\n",
      "mse:\n",
      "0.3848532403641579\n",
      "\n",
      "loss:\n",
      "0.1923597454994658\n",
      "mse:\n",
      "0.3847194909989316\n",
      "\n",
      "loss:\n",
      "0.19229312935084247\n",
      "mse:\n",
      "0.38458625870168495\n",
      "\n",
      "loss:\n",
      "0.19222724106856603\n",
      "mse:\n",
      "0.38445448213713207\n",
      "\n",
      "loss:\n",
      "0.19216213245718589\n",
      "mse:\n",
      "0.38432426491437177\n",
      "\n",
      "loss:\n",
      "0.19209703704096942\n",
      "mse:\n",
      "0.38419407408193884\n",
      "\n",
      "loss:\n",
      "0.19203188354356868\n",
      "mse:\n",
      "0.38406376708713735\n",
      "\n",
      "loss:\n",
      "0.1919673564611854\n",
      "mse:\n",
      "0.3839347129223708\n",
      "Annealing learning rate. New rate is 0.04765998428116877\n",
      "\n",
      "loss:\n",
      "0.1919082916314337\n",
      "mse:\n",
      "0.3838165832628674\n",
      "\n",
      "loss:\n",
      "0.19184998801723815\n",
      "mse:\n",
      "0.3836999760344763\n",
      "\n",
      "loss:\n",
      "0.19179191564443382\n",
      "mse:\n",
      "0.38358383128886764\n",
      "\n",
      "loss:\n",
      "0.19173429217439428\n",
      "mse:\n",
      "0.38346858434878855\n",
      "\n",
      "loss:\n",
      "0.19167706144240135\n",
      "mse:\n",
      "0.3833541228848027\n",
      "\n",
      "loss:\n",
      "0.19161945706811553\n",
      "mse:\n",
      "0.38323891413623107\n",
      "\n",
      "loss:\n",
      "0.19156255421513269\n",
      "mse:\n",
      "0.38312510843026537\n",
      "\n",
      "loss:\n",
      "0.1915041918152323\n",
      "mse:\n",
      "0.3830083836304646\n",
      "\n",
      "loss:\n",
      "0.19144702349591183\n",
      "mse:\n",
      "0.38289404699182367\n",
      "\n",
      "loss:\n",
      "0.19138972033590593\n",
      "mse:\n",
      "0.38277944067181185\n",
      "\n",
      "loss:\n",
      "0.19133258004918885\n",
      "mse:\n",
      "0.3826651600983777\n",
      "\n",
      "loss:\n",
      "0.19127513296597087\n",
      "mse:\n",
      "0.38255026593194175\n",
      "\n",
      "loss:\n",
      "0.1912177445258076\n",
      "mse:\n",
      "0.3824354890516152\n",
      "\n",
      "loss:\n",
      "0.19116010408512765\n",
      "mse:\n",
      "0.3823202081702553\n",
      "\n",
      "loss:\n",
      "0.19110132815002503\n",
      "mse:\n",
      "0.38220265630005007\n",
      "\n",
      "loss:\n",
      "0.19104394342555547\n",
      "mse:\n",
      "0.38208788685111095\n",
      "\n",
      "loss:\n",
      "0.19098631301363883\n",
      "mse:\n",
      "0.38197262602727766\n",
      "\n",
      "loss:\n",
      "0.19092861208709727\n",
      "mse:\n",
      "0.38185722417419454\n",
      "\n",
      "loss:\n",
      "0.19087143029028197\n",
      "mse:\n",
      "0.38174286058056395\n",
      "\n",
      "loss:\n",
      "0.19081352588620168\n",
      "mse:\n",
      "0.38162705177240336\n",
      "\n",
      "loss:\n",
      "0.19075720631693688\n",
      "mse:\n",
      "0.38151441263387376\n",
      "\n",
      "loss:\n",
      "0.19069945569689545\n",
      "mse:\n",
      "0.3813989113937909\n",
      "\n",
      "loss:\n",
      "0.1906424424479742\n",
      "mse:\n",
      "0.3812848848959484\n",
      "\n",
      "loss:\n",
      "0.19058400853202392\n",
      "mse:\n",
      "0.38116801706404785\n",
      "\n",
      "loss:\n",
      "0.19052666454194975\n",
      "mse:\n",
      "0.3810533290838995\n",
      "\n",
      "loss:\n",
      "0.19046919067892562\n",
      "mse:\n",
      "0.38093838135785124\n",
      "Annealing learning rate. New rate is 0.042893985853051896\n",
      "\n",
      "loss:\n",
      "0.19041627400679045\n",
      "mse:\n",
      "0.3808325480135809\n",
      "\n",
      "loss:\n",
      "0.19036421508529885\n",
      "mse:\n",
      "0.3807284301705977\n",
      "\n",
      "loss:\n",
      "0.19031252630300627\n",
      "mse:\n",
      "0.38062505260601254\n",
      "\n",
      "loss:\n",
      "0.19026108807991074\n",
      "mse:\n",
      "0.3805221761598215\n",
      "\n",
      "loss:\n",
      "0.19020886159816736\n",
      "mse:\n",
      "0.3804177231963347\n",
      "\n",
      "loss:\n",
      "0.19015768362868482\n",
      "mse:\n",
      "0.38031536725736964\n",
      "\n",
      "loss:\n",
      "0.19010643564471702\n",
      "mse:\n",
      "0.38021287128943404\n",
      "\n",
      "loss:\n",
      "0.19005529072018207\n",
      "mse:\n",
      "0.38011058144036414\n",
      "\n",
      "loss:\n",
      "0.19000298326405332\n",
      "mse:\n",
      "0.38000596652810664\n",
      "\n",
      "loss:\n",
      "0.18995181766090072\n",
      "mse:\n",
      "0.37990363532180144\n",
      "\n",
      "loss:\n",
      "0.1899000928790294\n",
      "mse:\n",
      "0.3798001857580588\n",
      "\n",
      "loss:\n",
      "0.1898475481420999\n",
      "mse:\n",
      "0.3796950962841998\n",
      "\n",
      "loss:\n",
      "0.18979620692966323\n",
      "mse:\n",
      "0.37959241385932646\n",
      "\n",
      "loss:\n",
      "0.1897444631300201\n",
      "mse:\n",
      "0.3794889262600402\n",
      "\n",
      "loss:\n",
      "0.18969300719139756\n",
      "mse:\n",
      "0.3793860143827951\n",
      "\n",
      "loss:\n",
      "0.18964081313717931\n",
      "mse:\n",
      "0.37928162627435863\n",
      "\n",
      "loss:\n",
      "0.18958978334551413\n",
      "mse:\n",
      "0.37917956669102826\n",
      "\n",
      "loss:\n",
      "0.18953874017187333\n",
      "mse:\n",
      "0.37907748034374666\n",
      "\n",
      "loss:\n",
      "0.18948797361370875\n",
      "mse:\n",
      "0.3789759472274175\n",
      "\n",
      "loss:\n",
      "0.18943682054360803\n",
      "mse:\n",
      "0.37887364108721605\n",
      "\n",
      "loss:\n",
      "0.18938539576779334\n",
      "mse:\n",
      "0.3787707915355867\n",
      "\n",
      "loss:\n",
      "0.1893355003999749\n",
      "mse:\n",
      "0.3786710007999498\n",
      "\n",
      "loss:\n",
      "0.18928532181495145\n",
      "mse:\n",
      "0.3785706436299029\n",
      "\n",
      "loss:\n",
      "0.18923348436321893\n",
      "mse:\n",
      "0.37846696872643787\n",
      "\n",
      "loss:\n",
      "0.18918320562006768\n",
      "mse:\n",
      "0.37836641124013537\n",
      "\n",
      "loss:\n",
      "0.18913310587665766\n",
      "mse:\n",
      "0.3782662117533153\n",
      "Annealing learning rate. New rate is 0.03860458726774671\n",
      "\n",
      "loss:\n",
      "0.18908685171111586\n",
      "mse:\n",
      "0.3781737034222317\n",
      "\n",
      "loss:\n",
      "0.18904216598542975\n",
      "mse:\n",
      "0.3780843319708595\n",
      "\n",
      "loss:\n",
      "0.1889974563127694\n",
      "mse:\n",
      "0.3779949126255388\n",
      "\n",
      "loss:\n",
      "0.18895146542728264\n",
      "mse:\n",
      "0.3779029308545653\n",
      "\n",
      "loss:\n",
      "0.1889068100972357\n",
      "mse:\n",
      "0.3778136201944714\n",
      "\n",
      "loss:\n",
      "0.18886181841053337\n",
      "mse:\n",
      "0.37772363682106674\n",
      "\n",
      "loss:\n",
      "0.18881705443547508\n",
      "mse:\n",
      "0.37763410887095017\n",
      "\n",
      "loss:\n",
      "0.18877171878123022\n",
      "mse:\n",
      "0.37754343756246045\n",
      "\n",
      "loss:\n",
      "0.1887272852110651\n",
      "mse:\n",
      "0.3774545704221302\n",
      "\n",
      "loss:\n",
      "0.1886827091008732\n",
      "mse:\n",
      "0.3773654182017464\n",
      "\n",
      "loss:\n",
      "0.18863710825389854\n",
      "mse:\n",
      "0.3772742165077971\n",
      "\n",
      "loss:\n",
      "0.1885929053092628\n",
      "mse:\n",
      "0.3771858106185256\n",
      "\n",
      "loss:\n",
      "0.18854863601916108\n",
      "mse:\n",
      "0.37709727203832216\n",
      "\n",
      "loss:\n",
      "0.18850367588781536\n",
      "mse:\n",
      "0.3770073517756307\n",
      "\n",
      "loss:\n",
      "0.188459536654642\n",
      "mse:\n",
      "0.376919073309284\n",
      "\n",
      "loss:\n",
      "0.1884153666972423\n",
      "mse:\n",
      "0.3768307333944846\n",
      "\n",
      "loss:\n",
      "0.1883715178488896\n",
      "mse:\n",
      "0.3767430356977792\n",
      "\n",
      "loss:\n",
      "0.18832645515522003\n",
      "mse:\n",
      "0.37665291031044007\n",
      "\n",
      "loss:\n",
      "0.18828285675131118\n",
      "mse:\n",
      "0.37656571350262236\n",
      "\n",
      "loss:\n",
      "0.18823902709205648\n",
      "mse:\n",
      "0.37647805418411295\n",
      "\n",
      "loss:\n",
      "0.18819585381242707\n",
      "mse:\n",
      "0.37639170762485413\n",
      "\n",
      "loss:\n",
      "0.18815091491719677\n",
      "mse:\n",
      "0.37630182983439353\n",
      "\n",
      "loss:\n",
      "0.1881068840507884\n",
      "mse:\n",
      "0.3762137681015768\n",
      "\n",
      "loss:\n",
      "0.18806290351753574\n",
      "mse:\n",
      "0.3761258070350715\n",
      "\n",
      "loss:\n",
      "0.18801905760713694\n",
      "mse:\n",
      "0.3760381152142739\n",
      "\n",
      "loss:\n",
      "0.18797494544089718\n",
      "mse:\n",
      "0.37594989088179437\n",
      "Annealing learning rate. New rate is 0.03474412854097204\n",
      "\n",
      "loss:\n",
      "0.187934530111583\n",
      "mse:\n",
      "0.375869060223166\n",
      "\n",
      "loss:\n",
      "0.18789547406169552\n",
      "mse:\n",
      "0.37579094812339103\n",
      "\n",
      "loss:\n",
      "0.18785574977725916\n",
      "mse:\n",
      "0.3757114995545183\n",
      "\n",
      "loss:\n",
      "0.18781680929211386\n",
      "mse:\n",
      "0.3756336185842277\n",
      "\n",
      "loss:\n",
      "0.1877770826910735\n",
      "mse:\n",
      "0.375554165382147\n",
      "\n",
      "loss:\n",
      "0.18773786376176693\n",
      "mse:\n",
      "0.37547572752353386\n",
      "\n",
      "loss:\n",
      "0.18769952933366532\n",
      "mse:\n",
      "0.37539905866733064\n",
      "\n",
      "loss:\n",
      "0.1876606681483389\n",
      "mse:\n",
      "0.3753213362966778\n",
      "\n",
      "loss:\n",
      "0.18762202969283254\n",
      "mse:\n",
      "0.3752440593856651\n",
      "\n",
      "loss:\n",
      "0.18758286539042668\n",
      "mse:\n",
      "0.37516573078085336\n",
      "\n",
      "loss:\n",
      "0.187545188840877\n",
      "mse:\n",
      "0.375090377681754\n",
      "\n",
      "loss:\n",
      "0.1875067098404396\n",
      "mse:\n",
      "0.3750134196808792\n",
      "\n",
      "loss:\n",
      "0.18746904520937868\n",
      "mse:\n",
      "0.37493809041875736\n",
      "\n",
      "loss:\n",
      "0.18743085101830528\n",
      "mse:\n",
      "0.37486170203661057\n",
      "\n",
      "loss:\n",
      "0.18739247037121973\n",
      "mse:\n",
      "0.37478494074243945\n",
      "\n",
      "loss:\n",
      "0.1873522749833802\n",
      "mse:\n",
      "0.3747045499667604\n",
      "\n",
      "loss:\n",
      "0.18731275430684596\n",
      "mse:\n",
      "0.3746255086136919\n",
      "\n",
      "loss:\n",
      "0.1872735365227709\n",
      "mse:\n",
      "0.3745470730455418\n",
      "\n",
      "loss:\n",
      "0.18723432878671362\n",
      "mse:\n",
      "0.37446865757342723\n",
      "\n",
      "loss:\n",
      "0.1871944809758398\n",
      "mse:\n",
      "0.3743889619516796\n",
      "\n",
      "loss:\n",
      "0.18715506893432793\n",
      "mse:\n",
      "0.37431013786865586\n",
      "\n",
      "loss:\n",
      "0.18711555079947445\n",
      "mse:\n",
      "0.3742311015989489\n",
      "\n",
      "loss:\n",
      "0.1870760194538586\n",
      "mse:\n",
      "0.3741520389077172\n",
      "\n",
      "loss:\n",
      "0.187037158776549\n",
      "mse:\n",
      "0.374074317553098\n",
      "\n",
      "loss:\n",
      "0.18699793692516484\n",
      "mse:\n",
      "0.3739958738503297\n",
      "\n",
      "loss:\n",
      "0.18695799572671398\n",
      "mse:\n",
      "0.37391599145342796\n",
      "Annealing learning rate. New rate is 0.03126971568687483\n",
      "\n",
      "loss:\n",
      "0.18692251642543337\n",
      "mse:\n",
      "0.37384503285086673\n",
      "\n",
      "loss:\n",
      "0.18688692162639164\n",
      "mse:\n",
      "0.3737738432527833\n",
      "\n",
      "loss:\n",
      "0.18685203124331218\n",
      "mse:\n",
      "0.37370406248662436\n",
      "\n",
      "loss:\n",
      "0.1868164963892341\n",
      "mse:\n",
      "0.3736329927784682\n",
      "\n",
      "loss:\n",
      "0.18678114005666532\n",
      "mse:\n",
      "0.37356228011333065\n",
      "\n",
      "loss:\n",
      "0.18674610533916827\n",
      "mse:\n",
      "0.37349221067833654\n",
      "\n",
      "loss:\n",
      "0.18671048810671195\n",
      "mse:\n",
      "0.3734209762134239\n",
      "\n",
      "loss:\n",
      "0.186675798390967\n",
      "mse:\n",
      "0.373351596781934\n",
      "\n",
      "loss:\n",
      "0.1866408090699703\n",
      "mse:\n",
      "0.3732816181399406\n",
      "\n",
      "loss:\n",
      "0.18660571033327955\n",
      "mse:\n",
      "0.3732114206665591\n",
      "\n",
      "loss:\n",
      "0.1865707270512596\n",
      "mse:\n",
      "0.3731414541025192\n",
      "\n",
      "loss:\n",
      "0.18653613590586296\n",
      "mse:\n",
      "0.3730722718117259\n",
      "\n",
      "loss:\n",
      "0.18650134696538853\n",
      "mse:\n",
      "0.37300269393077706\n",
      "\n",
      "loss:\n",
      "0.18646745946750742\n",
      "mse:\n",
      "0.37293491893501485\n",
      "\n",
      "loss:\n",
      "0.18643322174151217\n",
      "mse:\n",
      "0.37286644348302433\n",
      "\n",
      "loss:\n",
      "0.18639853311069665\n",
      "mse:\n",
      "0.3727970662213933\n",
      "\n",
      "loss:\n",
      "0.18636383075023616\n",
      "mse:\n",
      "0.3727276615004723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss:\n",
      "0.1863290937970435\n",
      "mse:\n",
      "0.372658187594087\n",
      "\n",
      "loss:\n",
      "0.1862934270343991\n",
      "mse:\n",
      "0.3725868540687982\n",
      "\n",
      "loss:\n",
      "0.1862586536015478\n",
      "mse:\n",
      "0.3725173072030956\n",
      "\n",
      "loss:\n",
      "0.18622404328397366\n",
      "mse:\n",
      "0.3724480865679473\n",
      "\n",
      "loss:\n",
      "0.1861886989267535\n",
      "mse:\n",
      "0.372377397853507\n",
      "\n",
      "loss:\n",
      "0.1861545029097941\n",
      "mse:\n",
      "0.3723090058195882\n",
      "\n",
      "loss:\n",
      "0.18611942286759356\n",
      "mse:\n",
      "0.3722388457351871\n",
      "\n",
      "loss:\n",
      "0.1860855561612046\n",
      "mse:\n",
      "0.3721711123224092\n",
      "\n",
      "loss:\n",
      "0.18605059480491062\n",
      "mse:\n",
      "0.37210118960982125\n",
      "Annealing learning rate. New rate is 0.02814274411818735\n",
      "\n",
      "loss:\n",
      "0.18601971735241485\n",
      "mse:\n",
      "0.3720394347048297\n",
      "\n",
      "loss:\n",
      "0.1859896489214941\n",
      "mse:\n",
      "0.3719792978429882\n",
      "\n",
      "loss:\n",
      "0.18595913376473247\n",
      "mse:\n",
      "0.37191826752946494\n",
      "\n",
      "loss:\n",
      "0.1859296009899523\n",
      "mse:\n",
      "0.3718592019799046\n",
      "\n",
      "loss:\n",
      "0.18589956492820786\n",
      "mse:\n",
      "0.3717991298564157\n",
      "\n",
      "loss:\n",
      "0.1858702581559567\n",
      "mse:\n",
      "0.3717405163119134\n",
      "\n",
      "loss:\n",
      "0.1858402106652051\n",
      "mse:\n",
      "0.3716804213304102\n",
      "\n",
      "loss:\n",
      "0.1858114621990456\n",
      "mse:\n",
      "0.3716229243980912\n",
      "\n",
      "loss:\n",
      "0.1857824726353406\n",
      "mse:\n",
      "0.3715649452706812\n",
      "\n",
      "loss:\n",
      "0.18575349922134457\n",
      "mse:\n",
      "0.37150699844268914\n",
      "\n",
      "loss:\n",
      "0.1857243140358474\n",
      "mse:\n",
      "0.3714486280716948\n",
      "\n",
      "loss:\n",
      "0.18569470238058697\n",
      "mse:\n",
      "0.37138940476117394\n",
      "\n",
      "loss:\n",
      "0.18566526703944472\n",
      "mse:\n",
      "0.37133053407888944\n",
      "\n",
      "loss:\n",
      "0.18563487685941107\n",
      "mse:\n",
      "0.37126975371882215\n",
      "\n",
      "loss:\n",
      "0.18560501512927943\n",
      "mse:\n",
      "0.37121003025855887\n",
      "\n",
      "loss:\n",
      "0.18557478346192816\n",
      "mse:\n",
      "0.3711495669238563\n",
      "\n",
      "loss:\n",
      "0.18554502190182345\n",
      "mse:\n",
      "0.3710900438036469\n",
      "\n",
      "loss:\n",
      "0.18551488984897588\n",
      "mse:\n",
      "0.37102977969795176\n",
      "\n",
      "loss:\n",
      "0.18548514041095696\n",
      "mse:\n",
      "0.3709702808219139\n",
      "\n",
      "loss:\n",
      "0.1854549705182509\n",
      "mse:\n",
      "0.3709099410365018\n",
      "\n",
      "loss:\n",
      "0.18542509464741\n",
      "mse:\n",
      "0.37085018929482\n",
      "\n",
      "loss:\n",
      "0.18539509984279015\n",
      "mse:\n",
      "0.3707901996855803\n",
      "\n",
      "loss:\n",
      "0.1853647105954777\n",
      "mse:\n",
      "0.3707294211909554\n",
      "\n",
      "loss:\n",
      "0.18533526677990186\n",
      "mse:\n",
      "0.3706705335598037\n",
      "\n",
      "loss:\n",
      "0.18530446325193745\n",
      "mse:\n",
      "0.3706089265038749\n",
      "\n",
      "loss:\n",
      "0.1852741778269112\n",
      "mse:\n",
      "0.3705483556538224\n",
      "Annealing learning rate. New rate is 0.025328469706368616\n",
      "\n",
      "loss:\n",
      "0.18524648465256197\n",
      "mse:\n",
      "0.37049296930512393\n",
      "\n",
      "loss:\n",
      "0.18521930086934513\n",
      "mse:\n",
      "0.37043860173869025\n",
      "\n",
      "loss:\n",
      "0.18519101232543958\n",
      "mse:\n",
      "0.37038202465087916\n",
      "\n",
      "loss:\n",
      "0.18516334366427586\n",
      "mse:\n",
      "0.37032668732855173\n",
      "\n",
      "loss:\n",
      "0.18513500912947167\n",
      "mse:\n",
      "0.37027001825894335\n",
      "\n",
      "loss:\n",
      "0.185107652386252\n",
      "mse:\n",
      "0.370215304772504\n",
      "\n",
      "loss:\n",
      "0.18508009437423892\n",
      "mse:\n",
      "0.37016018874847784\n",
      "\n",
      "loss:\n",
      "0.18505182898817965\n",
      "mse:\n",
      "0.3701036579763593\n",
      "\n",
      "loss:\n",
      "0.18502439334704154\n",
      "mse:\n",
      "0.3700487866940831\n",
      "\n",
      "loss:\n",
      "0.18499639381911576\n",
      "mse:\n",
      "0.3699927876382315\n",
      "\n",
      "loss:\n",
      "0.18496905406203926\n",
      "mse:\n",
      "0.3699381081240785\n",
      "\n",
      "loss:\n",
      "0.1849405047067165\n",
      "mse:\n",
      "0.369881009413433\n",
      "\n",
      "loss:\n",
      "0.18491289865625649\n",
      "mse:\n",
      "0.36982579731251297\n",
      "\n",
      "loss:\n",
      "0.18488429351703958\n",
      "mse:\n",
      "0.36976858703407917\n",
      "\n",
      "loss:\n",
      "0.18485619991129207\n",
      "mse:\n",
      "0.36971239982258414\n",
      "\n",
      "loss:\n",
      "0.18482805746296582\n",
      "mse:\n",
      "0.36965611492593164\n",
      "\n",
      "loss:\n",
      "0.18479907757146083\n",
      "mse:\n",
      "0.36959815514292166\n",
      "\n",
      "loss:\n",
      "0.18477067056083255\n",
      "mse:\n",
      "0.3695413411216651\n",
      "\n",
      "loss:\n",
      "0.1847416353997542\n",
      "mse:\n",
      "0.3694832707995084\n",
      "\n",
      "loss:\n",
      "0.18471238979457935\n",
      "mse:\n",
      "0.3694247795891587\n",
      "\n",
      "loss:\n",
      "0.18468378443432146\n",
      "mse:\n",
      "0.3693675688686429\n",
      "\n",
      "loss:\n",
      "0.18465428491531055\n",
      "mse:\n",
      "0.3693085698306211\n",
      "\n",
      "loss:\n",
      "0.1846257612385951\n",
      "mse:\n",
      "0.3692515224771902\n",
      "\n",
      "loss:\n",
      "0.18459688750302455\n",
      "mse:\n",
      "0.3691937750060491\n",
      "\n",
      "loss:\n",
      "0.18456838616910298\n",
      "mse:\n",
      "0.36913677233820597\n",
      "\n",
      "loss:\n",
      "0.18453957984607738\n",
      "mse:\n",
      "0.36907915969215477\n",
      "Annealing learning rate. New rate is 0.022795622735731755\n",
      "\n",
      "loss:\n",
      "0.18451371201248998\n",
      "mse:\n",
      "0.36902742402497996\n",
      "\n",
      "loss:\n",
      "0.1844871621697616\n",
      "mse:\n",
      "0.3689743243395232\n",
      "\n",
      "loss:\n",
      "0.1844611410800758\n",
      "mse:\n",
      "0.3689222821601516\n",
      "\n",
      "loss:\n",
      "0.18443453428104076\n",
      "mse:\n",
      "0.3688690685620815\n",
      "\n",
      "loss:\n",
      "0.18440923428011122\n",
      "mse:\n",
      "0.36881846856022243\n",
      "\n",
      "loss:\n",
      "0.1843857802504043\n",
      "mse:\n",
      "0.3687715605008086\n",
      "\n",
      "loss:\n",
      "0.18436043176640587\n",
      "mse:\n",
      "0.36872086353281175\n",
      "\n",
      "loss:\n",
      "0.1843349532064075\n",
      "mse:\n",
      "0.368669906412815\n",
      "\n",
      "loss:\n",
      "0.18431132240088266\n",
      "mse:\n",
      "0.3686226448017653\n",
      "\n",
      "loss:\n",
      "0.18428593692742504\n",
      "mse:\n",
      "0.3685718738548501\n",
      "\n",
      "loss:\n",
      "0.18425988426756676\n",
      "mse:\n",
      "0.3685197685351335\n",
      "\n",
      "loss:\n",
      "0.184237319443872\n",
      "mse:\n",
      "0.368474638887744\n",
      "\n",
      "loss:\n",
      "0.1842123141325248\n",
      "mse:\n",
      "0.3684246282650496\n",
      "\n",
      "loss:\n",
      "0.18418998809075754\n",
      "mse:\n",
      "0.3683799761815151\n",
      "\n",
      "loss:\n",
      "0.18416447116563506\n",
      "mse:\n",
      "0.3683289423312701\n",
      "\n",
      "loss:\n",
      "0.184139623630427\n",
      "mse:\n",
      "0.368279247260854\n",
      "\n",
      "loss:\n",
      "0.18411725187739564\n",
      "mse:\n",
      "0.3682345037547913\n",
      "\n",
      "loss:\n",
      "0.18409125406693028\n",
      "mse:\n",
      "0.36818250813386055\n",
      "\n",
      "loss:\n",
      "0.18406607561564917\n",
      "mse:\n",
      "0.36813215123129833\n",
      "\n",
      "loss:\n",
      "0.1840437106523169\n",
      "mse:\n",
      "0.3680874213046338\n",
      "\n",
      "loss:\n",
      "0.18401804688768586\n",
      "mse:\n",
      "0.3680360937753717\n",
      "\n",
      "loss:\n",
      "0.18399575291962333\n",
      "mse:\n",
      "0.36799150583924667\n",
      "\n",
      "loss:\n",
      "0.18397052274645376\n",
      "mse:\n",
      "0.3679410454929075\n",
      "\n",
      "loss:\n",
      "0.1839454400423643\n",
      "mse:\n",
      "0.3678908800847286\n",
      "\n",
      "loss:\n",
      "0.1839223502237287\n",
      "mse:\n",
      "0.3678447004474574\n",
      "\n",
      "loss:\n",
      "0.1838972295598499\n",
      "mse:\n",
      "0.3677944591196998\n",
      "Annealing learning rate. New rate is 0.02051606046215858\n",
      "\n",
      "loss:\n",
      "0.18387473470512244\n",
      "mse:\n",
      "0.3677494694102449\n",
      "\n",
      "loss:\n",
      "0.1838546538512581\n",
      "mse:\n",
      "0.3677093077025162\n",
      "\n",
      "loss:\n",
      "0.18383154975376326\n",
      "mse:\n",
      "0.3676630995075265\n",
      "\n",
      "loss:\n",
      "0.1838113401340987\n",
      "mse:\n",
      "0.3676226802681974\n",
      "\n",
      "loss:\n",
      "0.18378837232000272\n",
      "mse:\n",
      "0.36757674464000545\n",
      "\n",
      "loss:\n",
      "0.18376480631632805\n",
      "mse:\n",
      "0.3675296126326561\n",
      "\n",
      "loss:\n",
      "0.18374201106361954\n",
      "mse:\n",
      "0.3674840221272391\n",
      "\n",
      "loss:\n",
      "0.18371757784108642\n",
      "mse:\n",
      "0.36743515568217283\n",
      "\n",
      "loss:\n",
      "0.18369587710244842\n",
      "mse:\n",
      "0.36739175420489684\n",
      "\n",
      "loss:\n",
      "0.1836717578862354\n",
      "mse:\n",
      "0.3673435157724708\n",
      "\n",
      "loss:\n",
      "0.183650427704299\n",
      "mse:\n",
      "0.367300855408598\n",
      "\n",
      "loss:\n",
      "0.18362643636887763\n",
      "mse:\n",
      "0.36725287273775525\n",
      "\n",
      "loss:\n",
      "0.18360298722299548\n",
      "mse:\n",
      "0.36720597444599096\n",
      "\n",
      "loss:\n",
      "0.18358194116930404\n",
      "mse:\n",
      "0.3671638823386081\n",
      "\n",
      "loss:\n",
      "0.18355787093250822\n",
      "mse:\n",
      "0.36711574186501644\n",
      "\n",
      "loss:\n",
      "0.18353720489799963\n",
      "mse:\n",
      "0.36707440979599926\n",
      "\n",
      "loss:\n",
      "0.18351376877452627\n",
      "mse:\n",
      "0.36702753754905254\n",
      "\n",
      "loss:\n",
      "0.18349310115607556\n",
      "mse:\n",
      "0.3669862023121511\n",
      "\n",
      "loss:\n",
      "0.1834698138589809\n",
      "mse:\n",
      "0.3669396277179618\n",
      "\n",
      "loss:\n",
      "0.18344682356244513\n",
      "mse:\n",
      "0.36689364712489025\n",
      "\n",
      "loss:\n",
      "0.18342594020709616\n",
      "mse:\n",
      "0.3668518804141923\n",
      "\n",
      "loss:\n",
      "0.18340274439102788\n",
      "mse:\n",
      "0.36680548878205577\n",
      "\n",
      "loss:\n",
      "0.18338271198387854\n",
      "mse:\n",
      "0.3667654239677571\n",
      "\n",
      "loss:\n",
      "0.1833597814934944\n",
      "mse:\n",
      "0.3667195629869888\n",
      "\n",
      "loss:\n",
      "0.18333935736362827\n",
      "mse:\n",
      "0.36667871472725655\n",
      "\n",
      "loss:\n",
      "0.1833168504195311\n",
      "mse:\n",
      "0.3666337008390622\n",
      "Annealing learning rate. New rate is 0.018464454415942723\n",
      "\n",
      "loss:\n",
      "0.18329587162871466\n",
      "mse:\n",
      "0.3665917432574293\n",
      "\n",
      "loss:\n",
      "0.1832776721160254\n",
      "mse:\n",
      "0.3665553442320508\n",
      "\n",
      "loss:\n",
      "0.18325708797125062\n",
      "mse:\n",
      "0.36651417594250124\n",
      "\n",
      "loss:\n",
      "0.18323877837301591\n",
      "mse:\n",
      "0.36647755674603183\n",
      "\n",
      "loss:\n",
      "0.183218446376074\n",
      "mse:\n",
      "0.366436892752148\n",
      "\n",
      "loss:\n",
      "0.1831996225958789\n",
      "mse:\n",
      "0.3663992451917578\n",
      "\n",
      "loss:\n",
      "0.18317917196866706\n",
      "mse:\n",
      "0.36635834393733413\n",
      "\n",
      "loss:\n",
      "0.18315883806561234\n",
      "mse:\n",
      "0.3663176761312247\n",
      "\n",
      "loss:\n",
      "0.18314067982861573\n",
      "mse:\n",
      "0.36628135965723146\n",
      "\n",
      "loss:\n",
      "0.1831203596747988\n",
      "mse:\n",
      "0.3662407193495976\n",
      "\n",
      "loss:\n",
      "0.18310240692541202\n",
      "mse:\n",
      "0.36620481385082404\n",
      "\n",
      "loss:\n",
      "0.18308124547948654\n",
      "mse:\n",
      "0.3661624909589731\n",
      "\n",
      "loss:\n",
      "0.18306296091181945\n",
      "mse:\n",
      "0.3661259218236389\n",
      "\n",
      "loss:\n",
      "0.18304256282469464\n",
      "mse:\n",
      "0.3660851256493893\n",
      "\n",
      "loss:\n",
      "0.18302165567925063\n",
      "mse:\n",
      "0.36604331135850127\n",
      "\n",
      "loss:\n",
      "0.18300353356168747\n",
      "mse:\n",
      "0.36600706712337494\n",
      "\n",
      "loss:\n",
      "0.18298329166629262\n",
      "mse:\n",
      "0.36596658333258525\n",
      "\n",
      "loss:\n",
      "0.18296534799259875\n",
      "mse:\n",
      "0.3659306959851975\n",
      "\n",
      "loss:\n",
      "0.18294444980235075\n",
      "mse:\n",
      "0.3658888996047015\n",
      "\n",
      "loss:\n",
      "0.18292405292673186\n",
      "mse:\n",
      "0.3658481058534637\n",
      "\n",
      "loss:\n",
      "0.18290579095267864\n",
      "mse:\n",
      "0.3658115819053573\n",
      "\n",
      "loss:\n",
      "0.18288576848626803\n",
      "mse:\n",
      "0.36577153697253606\n",
      "\n",
      "loss:\n",
      "0.1828680453025035\n",
      "mse:\n",
      "0.365736090605007\n",
      "\n",
      "loss:\n",
      "0.1828477583943297\n",
      "mse:\n",
      "0.3656955167886594\n",
      "\n",
      "loss:\n",
      "0.1828302946943751\n",
      "mse:\n",
      "0.3656605893887502\n",
      "\n",
      "loss:\n",
      "0.18281032057694924\n",
      "mse:\n",
      "0.3656206411538985\n",
      "Annealing learning rate. New rate is 0.016618008974348453\n",
      "\n",
      "loss:\n",
      "0.1827922046734686\n",
      "mse:\n",
      "0.3655844093469372\n",
      "\n",
      "loss:\n",
      "0.18277648623422785\n",
      "mse:\n",
      "0.3655529724684557\n",
      "\n",
      "loss:\n",
      "0.18275873223220887\n",
      "mse:\n",
      "0.36551746446441774\n",
      "\n",
      "loss:\n",
      "0.18274253723806078\n",
      "mse:\n",
      "0.36548507447612155\n",
      "\n",
      "loss:\n",
      "0.18272476043380073\n",
      "mse:\n",
      "0.36544952086760146\n",
      "\n",
      "loss:\n",
      "0.1827088993936576\n",
      "mse:\n",
      "0.3654177987873152\n",
      "\n",
      "loss:\n",
      "0.18269141230418984\n",
      "mse:\n",
      "0.3653828246083797\n",
      "\n",
      "loss:\n",
      "0.18267603009915329\n",
      "mse:\n",
      "0.36535206019830657\n",
      "\n",
      "loss:\n",
      "0.18265826917924158\n",
      "mse:\n",
      "0.36531653835848316\n",
      "\n",
      "loss:\n",
      "0.1826408191484872\n",
      "mse:\n",
      "0.3652816382969744\n",
      "\n",
      "loss:\n",
      "0.1826253637674581\n",
      "mse:\n",
      "0.3652507275349162\n",
      "\n",
      "loss:\n",
      "0.18260779537968383\n",
      "mse:\n",
      "0.36521559075936766\n",
      "\n",
      "loss:\n",
      "0.18259241066042495\n",
      "mse:\n",
      "0.3651848213208499\n",
      "\n",
      "loss:\n",
      "0.18257493151984416\n",
      "mse:\n",
      "0.3651498630396883\n",
      "\n",
      "loss:\n",
      "0.18255898485053978\n",
      "mse:\n",
      "0.36511796970107957\n",
      "\n",
      "loss:\n",
      "0.18254138997097577\n",
      "mse:\n",
      "0.36508277994195154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss:\n",
      "0.18252576163008064\n",
      "mse:\n",
      "0.36505152326016127\n",
      "\n",
      "loss:\n",
      "0.18250812571833308\n",
      "mse:\n",
      "0.36501625143666616\n",
      "\n",
      "loss:\n",
      "0.18249276049562826\n",
      "mse:\n",
      "0.36498552099125653\n",
      "\n",
      "loss:\n",
      "0.18247491569261767\n",
      "mse:\n",
      "0.36494983138523535\n",
      "\n",
      "loss:\n",
      "0.18245750913849113\n",
      "mse:\n",
      "0.36491501827698225\n",
      "\n",
      "loss:\n",
      "0.18244160467061105\n",
      "mse:\n",
      "0.3648832093412221\n",
      "\n",
      "loss:\n",
      "0.18242425065772833\n",
      "mse:\n",
      "0.36484850131545665\n",
      "\n",
      "loss:\n",
      "0.182408225639888\n",
      "mse:\n",
      "0.364816451279776\n",
      "\n",
      "loss:\n",
      "0.18239063609047415\n",
      "mse:\n",
      "0.3647812721809483\n",
      "\n",
      "loss:\n",
      "0.18237501193262523\n",
      "mse:\n",
      "0.36475002386525046\n",
      "Annealing learning rate. New rate is 0.014956208076913608\n",
      "\n",
      "loss:\n",
      "0.18235906922881745\n",
      "mse:\n",
      "0.3647181384576349\n",
      "\n",
      "loss:\n",
      "0.18234492840764652\n",
      "mse:\n",
      "0.36468985681529303\n",
      "\n",
      "loss:\n",
      "0.18232872261622843\n",
      "mse:\n",
      "0.36465744523245686\n",
      "\n",
      "loss:\n",
      "0.18231465986730644\n",
      "mse:\n",
      "0.3646293197346129\n",
      "\n",
      "loss:\n",
      "0.18229866487020496\n",
      "mse:\n",
      "0.3645973297404099\n",
      "\n",
      "loss:\n",
      "0.1822828263926682\n",
      "mse:\n",
      "0.3645656527853364\n",
      "\n",
      "loss:\n",
      "0.18226875471370899\n",
      "mse:\n",
      "0.36453750942741797\n",
      "\n",
      "loss:\n",
      "0.18225266003901294\n",
      "mse:\n",
      "0.3645053200780259\n",
      "\n",
      "loss:\n",
      "0.18223830081930478\n",
      "mse:\n",
      "0.36447660163860957\n",
      "\n",
      "loss:\n",
      "0.182221914178951\n",
      "mse:\n",
      "0.364443828357902\n",
      "\n",
      "loss:\n",
      "0.18220781786585574\n",
      "mse:\n",
      "0.3644156357317115\n",
      "\n",
      "loss:\n",
      "0.18219179745147668\n",
      "mse:\n",
      "0.36438359490295336\n",
      "\n",
      "loss:\n",
      "0.18217534840250593\n",
      "mse:\n",
      "0.36435069680501186\n",
      "\n",
      "loss:\n",
      "0.18216120666096722\n",
      "mse:\n",
      "0.36432241332193444\n",
      "\n",
      "loss:\n",
      "0.18214511249409376\n",
      "mse:\n",
      "0.3642902249881875\n",
      "\n",
      "loss:\n",
      "0.18213090209235358\n",
      "mse:\n",
      "0.36426180418470716\n",
      "\n",
      "loss:\n",
      "0.18211488426672612\n",
      "mse:\n",
      "0.36422976853345224\n",
      "\n",
      "loss:\n",
      "0.18210031863022313\n",
      "mse:\n",
      "0.36420063726044627\n",
      "\n",
      "loss:\n",
      "0.18208433187701117\n",
      "mse:\n",
      "0.36416866375402235\n",
      "\n",
      "loss:\n",
      "0.18206986610722817\n",
      "mse:\n",
      "0.36413973221445634\n",
      "\n",
      "loss:\n",
      "0.18205402708903407\n",
      "mse:\n",
      "0.36410805417806813\n",
      "\n",
      "loss:\n",
      "0.18203780722513005\n",
      "mse:\n",
      "0.3640756144502601\n",
      "\n",
      "loss:\n",
      "0.18202383041518616\n",
      "mse:\n",
      "0.3640476608303723\n",
      "\n",
      "loss:\n",
      "0.1820078796237331\n",
      "mse:\n",
      "0.3640157592474662\n",
      "\n",
      "loss:\n",
      "0.18199410079321193\n",
      "mse:\n",
      "0.36398820158642387\n",
      "\n",
      "loss:\n",
      "0.1819781642426497\n",
      "mse:\n",
      "0.3639563284852994\n",
      "Annealing learning rate. New rate is 0.013460587269222246\n",
      "\n",
      "loss:\n",
      "0.18196566944192308\n",
      "mse:\n",
      "0.36393133888384616\n",
      "\n",
      "loss:\n",
      "0.18195146185956704\n",
      "mse:\n",
      "0.3639029237191341\n",
      "\n",
      "loss:\n",
      "0.1819387564106443\n",
      "mse:\n",
      "0.3638775128212886\n",
      "\n",
      "loss:\n",
      "0.18192459332854224\n",
      "mse:\n",
      "0.3638491866570845\n",
      "\n",
      "loss:\n",
      "0.18191005252823406\n",
      "mse:\n",
      "0.3638201050564681\n",
      "\n",
      "loss:\n",
      "0.18189774744990359\n",
      "mse:\n",
      "0.36379549489980717\n",
      "\n",
      "loss:\n",
      "0.18188322314552668\n",
      "mse:\n",
      "0.36376644629105337\n",
      "\n",
      "loss:\n",
      "0.18187088277519853\n",
      "mse:\n",
      "0.36374176555039706\n",
      "\n",
      "loss:\n",
      "0.18185642061013008\n",
      "mse:\n",
      "0.36371284122026015\n",
      "\n",
      "loss:\n",
      "0.18184401395007135\n",
      "mse:\n",
      "0.3636880279001427\n",
      "\n",
      "loss:\n",
      "0.1818293980844678\n",
      "mse:\n",
      "0.3636587961689356\n",
      "\n",
      "loss:\n",
      "0.18181522353288046\n",
      "mse:\n",
      "0.3636304470657609\n",
      "\n",
      "loss:\n",
      "0.18180255321275696\n",
      "mse:\n",
      "0.3636051064255139\n",
      "\n",
      "loss:\n",
      "0.1817884069132688\n",
      "mse:\n",
      "0.3635768138265376\n",
      "\n",
      "loss:\n",
      "0.18177611748924352\n",
      "mse:\n",
      "0.36355223497848704\n",
      "\n",
      "loss:\n",
      "0.181761742821418\n",
      "mse:\n",
      "0.363523485642836\n",
      "\n",
      "loss:\n",
      "0.18174769858770123\n",
      "mse:\n",
      "0.36349539717540247\n",
      "\n",
      "loss:\n",
      "0.18173502362598454\n",
      "mse:\n",
      "0.3634700472519691\n",
      "\n",
      "loss:\n",
      "0.18172111107472305\n",
      "mse:\n",
      "0.3634422221494461\n",
      "\n",
      "loss:\n",
      "0.18170851317580125\n",
      "mse:\n",
      "0.3634170263516025\n",
      "\n",
      "loss:\n",
      "0.18169465104286406\n",
      "mse:\n",
      "0.3633893020857281\n",
      "\n",
      "loss:\n",
      "0.18168223988215942\n",
      "mse:\n",
      "0.36336447976431885\n",
      "\n",
      "loss:\n",
      "0.18166828604237065\n",
      "mse:\n",
      "0.3633365720847413\n",
      "\n",
      "loss:\n",
      "0.18165407849244614\n",
      "mse:\n",
      "0.36330815698489227\n",
      "\n",
      "loss:\n",
      "0.18164189943247902\n",
      "mse:\n",
      "0.36328379886495804\n",
      "\n",
      "loss:\n",
      "0.18162795209793517\n",
      "mse:\n",
      "0.36325590419587034\n",
      "Annealing learning rate. New rate is 0.012114528542300022\n",
      "\n",
      "loss:\n",
      "0.18161666960004635\n",
      "mse:\n",
      "0.3632333392000927\n",
      "\n",
      "loss:\n",
      "0.18160407887708016\n",
      "mse:\n",
      "0.3632081577541603\n",
      "\n",
      "loss:\n",
      "0.1815911978344024\n",
      "mse:\n",
      "0.3631823956688048\n",
      "\n",
      "loss:\n",
      "0.18158008449679278\n",
      "mse:\n",
      "0.36316016899358555\n",
      "\n",
      "loss:\n",
      "0.1815672928173442\n",
      "mse:\n",
      "0.3631345856346884\n",
      "\n",
      "loss:\n",
      "0.1815565933481787\n",
      "mse:\n",
      "0.3631131866963574\n",
      "\n",
      "loss:\n",
      "0.1815440964048526\n",
      "mse:\n",
      "0.3630881928097052\n",
      "\n",
      "loss:\n",
      "0.18153335324902103\n",
      "mse:\n",
      "0.36306670649804207\n",
      "\n",
      "loss:\n",
      "0.18152074769474447\n",
      "mse:\n",
      "0.36304149538948893\n",
      "\n",
      "loss:\n",
      "0.1815085997012786\n",
      "mse:\n",
      "0.3630171994025572\n",
      "\n",
      "loss:\n",
      "0.18149762350389678\n",
      "mse:\n",
      "0.36299524700779356\n",
      "\n",
      "loss:\n",
      "0.18148488116329833\n",
      "mse:\n",
      "0.36296976232659667\n",
      "\n",
      "loss:\n",
      "0.18147390070265584\n",
      "mse:\n",
      "0.3629478014053117\n",
      "\n",
      "loss:\n",
      "0.18146103988075404\n",
      "mse:\n",
      "0.3629220797615081\n",
      "\n",
      "loss:\n",
      "0.18145036835661138\n",
      "mse:\n",
      "0.36290073671322276\n",
      "\n",
      "loss:\n",
      "0.18143759620449332\n",
      "mse:\n",
      "0.36287519240898664\n",
      "\n",
      "loss:\n",
      "0.18142533082953413\n",
      "mse:\n",
      "0.36285066165906826\n",
      "\n",
      "loss:\n",
      "0.18141437172251632\n",
      "mse:\n",
      "0.36282874344503263\n",
      "\n",
      "loss:\n",
      "0.18140217693173064\n",
      "mse:\n",
      "0.36280435386346127\n",
      "\n",
      "loss:\n",
      "0.1813913351863594\n",
      "mse:\n",
      "0.3627826703727188\n",
      "\n",
      "loss:\n",
      "0.18137904766546628\n",
      "mse:\n",
      "0.36275809533093256\n",
      "\n",
      "loss:\n",
      "0.18136810977330306\n",
      "mse:\n",
      "0.3627362195466061\n",
      "\n",
      "loss:\n",
      "0.18135590389506437\n",
      "mse:\n",
      "0.36271180779012874\n",
      "\n",
      "loss:\n",
      "0.1813448360798033\n",
      "mse:\n",
      "0.3626896721596066\n",
      "\n",
      "loss:\n",
      "0.18133257266316707\n",
      "mse:\n",
      "0.36266514532633415\n",
      "\n",
      "loss:\n",
      "0.18132187720121326\n",
      "mse:\n",
      "0.3626437544024265\n",
      "Annealing learning rate. New rate is 0.01090307568807002\n",
      "\n",
      "loss:\n",
      "0.18131050087810788\n",
      "mse:\n",
      "0.36262100175621576\n",
      "\n",
      "loss:\n",
      "0.1812995353312839\n",
      "mse:\n",
      "0.3625990706625678\n",
      "\n",
      "loss:\n",
      "0.18128958110712382\n",
      "mse:\n",
      "0.36257916221424763\n",
      "\n",
      "loss:\n",
      "0.18127898335652948\n",
      "mse:\n",
      "0.36255796671305895\n",
      "\n",
      "loss:\n",
      "0.1812692753855036\n",
      "mse:\n",
      "0.3625385507710072\n",
      "\n",
      "loss:\n",
      "0.1812583200659493\n",
      "mse:\n",
      "0.3625166401318986\n",
      "\n",
      "loss:\n",
      "0.18124876104245813\n",
      "mse:\n",
      "0.36249752208491626\n",
      "\n",
      "loss:\n",
      "0.18123787698311028\n",
      "mse:\n",
      "0.36247575396622056\n",
      "\n",
      "loss:\n",
      "0.18122836357077846\n",
      "mse:\n",
      "0.3624567271415569\n",
      "\n",
      "loss:\n",
      "0.1812174639482302\n",
      "mse:\n",
      "0.3624349278964604\n",
      "\n",
      "loss:\n",
      "0.1812079277780169\n",
      "mse:\n",
      "0.3624158555560338\n",
      "\n",
      "loss:\n",
      "0.1811967850271892\n",
      "mse:\n",
      "0.3623935700543784\n",
      "\n",
      "loss:\n",
      "0.18118612772236298\n",
      "mse:\n",
      "0.36237225544472595\n",
      "\n",
      "loss:\n",
      "0.1811764839385676\n",
      "mse:\n",
      "0.3623529678771352\n",
      "\n",
      "loss:\n",
      "0.1811655518371168\n",
      "mse:\n",
      "0.3623311036742336\n",
      "\n",
      "loss:\n",
      "0.1811561431293481\n",
      "mse:\n",
      "0.3623122862586962\n",
      "\n",
      "loss:\n",
      "0.1811449703395262\n",
      "mse:\n",
      "0.3622899406790524\n",
      "\n",
      "loss:\n",
      "0.18113569183062037\n",
      "mse:\n",
      "0.36227138366124073\n",
      "\n",
      "loss:\n",
      "0.18112460771256841\n",
      "mse:\n",
      "0.36224921542513683\n",
      "\n",
      "loss:\n",
      "0.18111525252732272\n",
      "mse:\n",
      "0.36223050505464544\n",
      "\n",
      "loss:\n",
      "0.18110450105856774\n",
      "mse:\n",
      "0.36220900211713547\n",
      "\n",
      "loss:\n",
      "0.18109470971494898\n",
      "mse:\n",
      "0.36218941942989796\n",
      "\n",
      "loss:\n",
      "0.18108376243348331\n",
      "mse:\n",
      "0.36216752486696663\n",
      "\n",
      "loss:\n",
      "0.1810725263826563\n",
      "mse:\n",
      "0.3621450527653126\n",
      "\n",
      "loss:\n",
      "0.181062995083834\n",
      "mse:\n",
      "0.362125990167668\n",
      "\n",
      "loss:\n",
      "0.18105185145680058\n",
      "mse:\n",
      "0.36210370291360117\n",
      "Annealing learning rate. New rate is 0.009812768119263017\n",
      "\n",
      "loss:\n",
      "0.18104325382578113\n",
      "mse:\n",
      "0.36208650765156225\n",
      "\n",
      "loss:\n",
      "0.18103337131028643\n",
      "mse:\n",
      "0.36206674262057287\n",
      "\n",
      "loss:\n",
      "0.18102460523777997\n",
      "mse:\n",
      "0.36204921047555993\n",
      "\n",
      "loss:\n",
      "0.18101472792288278\n",
      "mse:\n",
      "0.36202945584576557\n",
      "\n",
      "loss:\n",
      "0.18100603941374124\n",
      "mse:\n",
      "0.3620120788274825\n",
      "\n",
      "loss:\n",
      "0.1809962941762073\n",
      "mse:\n",
      "0.3619925883524146\n",
      "\n",
      "loss:\n",
      "0.1809877303228049\n",
      "mse:\n",
      "0.3619754606456098\n",
      "\n",
      "loss:\n",
      "0.18097758321898583\n",
      "mse:\n",
      "0.36195516643797165\n",
      "\n",
      "loss:\n",
      "0.18096883626184893\n",
      "mse:\n",
      "0.36193767252369785\n",
      "\n",
      "loss:\n",
      "0.18095863997338205\n",
      "mse:\n",
      "0.3619172799467641\n",
      "\n",
      "loss:\n",
      "0.18095001146406733\n",
      "mse:\n",
      "0.36190002292813467\n",
      "\n",
      "loss:\n",
      "0.1809397680981235\n",
      "mse:\n",
      "0.361879536196247\n",
      "\n",
      "loss:\n",
      "0.18093123385096035\n",
      "mse:\n",
      "0.3618624677019207\n",
      "\n",
      "loss:\n",
      "0.1809211049431574\n",
      "mse:\n",
      "0.3618422098863148\n",
      "\n",
      "loss:\n",
      "0.1809109292947215\n",
      "mse:\n",
      "0.361821858589443\n",
      "\n",
      "loss:\n",
      "0.18090210628720174\n",
      "mse:\n",
      "0.3618042125744035\n",
      "\n",
      "loss:\n",
      "0.18089201156700588\n",
      "mse:\n",
      "0.36178402313401176\n",
      "\n",
      "loss:\n",
      "0.18088293081351955\n",
      "mse:\n",
      "0.3617658616270391\n",
      "\n",
      "loss:\n",
      "0.18087271080186962\n",
      "mse:\n",
      "0.36174542160373924\n",
      "\n",
      "loss:\n",
      "0.18086336508420078\n",
      "mse:\n",
      "0.36172673016840157\n",
      "\n",
      "loss:\n",
      "0.18085247908162963\n",
      "mse:\n",
      "0.36170495816325926\n",
      "\n",
      "loss:\n",
      "0.18084370410805714\n",
      "mse:\n",
      "0.3616874082161143\n",
      "\n",
      "loss:\n",
      "0.1808344121510882\n",
      "mse:\n",
      "0.3616688243021764\n",
      "\n",
      "loss:\n",
      "0.18082359773211867\n",
      "mse:\n",
      "0.36164719546423735\n",
      "\n",
      "loss:\n",
      "0.18081478299992101\n",
      "mse:\n",
      "0.36162956599984203\n",
      "\n",
      "loss:\n",
      "0.1808039629405033\n",
      "mse:\n",
      "0.3616079258810066\n",
      "Annealing learning rate. New rate is 0.008831491307336715\n",
      "\n",
      "loss:\n",
      "0.18079556779591666\n",
      "mse:\n",
      "0.3615911355918333\n",
      "\n",
      "loss:\n",
      "0.18078595788688864\n",
      "mse:\n",
      "0.3615719157737773\n",
      "\n",
      "loss:\n",
      "0.18077800427335966\n",
      "mse:\n",
      "0.3615560085467193\n",
      "\n",
      "loss:\n",
      "0.18076844386702806\n",
      "mse:\n",
      "0.3615368877340561\n",
      "\n",
      "loss:\n",
      "0.18076009575069954\n",
      "mse:\n",
      "0.3615201915013991\n",
      "\n",
      "loss:\n",
      "0.1807510002924619\n",
      "mse:\n",
      "0.3615020005849238\n",
      "\n",
      "loss:\n",
      "0.18074273250924341\n",
      "mse:\n",
      "0.36148546501848683\n",
      "\n",
      "loss:\n",
      "0.1807331546742686\n",
      "mse:\n",
      "0.3614663093485372\n",
      "\n",
      "loss:\n",
      "0.18072508067014773\n",
      "mse:\n",
      "0.36145016134029545\n",
      "\n",
      "loss:\n",
      "0.1807173599442264\n",
      "mse:\n",
      "0.3614347198884528\n",
      "\n",
      "loss:\n",
      "0.1807079734398049\n",
      "mse:\n",
      "0.3614159468796098\n",
      "\n",
      "loss:\n",
      "0.18069978188172883\n",
      "mse:\n",
      "0.36139956376345767\n",
      "\n",
      "loss:\n",
      "0.18069049553350952\n",
      "mse:\n",
      "0.36138099106701904\n",
      "\n",
      "loss:\n",
      "0.18068247609083682\n",
      "mse:\n",
      "0.36136495218167364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss:\n",
      "0.1806727958049441\n",
      "mse:\n",
      "0.3613455916098882\n",
      "\n",
      "loss:\n",
      "0.18066460090649966\n",
      "mse:\n",
      "0.3613292018129993\n",
      "\n",
      "loss:\n",
      "0.18065663020219602\n",
      "mse:\n",
      "0.36131326040439204\n",
      "\n",
      "loss:\n",
      "0.18064709091791725\n",
      "mse:\n",
      "0.3612941818358345\n",
      "\n",
      "loss:\n",
      "0.18063924900799608\n",
      "mse:\n",
      "0.36127849801599216\n",
      "\n",
      "loss:\n",
      "0.1806300083338487\n",
      "mse:\n",
      "0.3612600166676974\n",
      "\n",
      "loss:\n",
      "0.18062194272538842\n",
      "mse:\n",
      "0.36124388545077685\n",
      "\n",
      "loss:\n",
      "0.18061248238500752\n",
      "mse:\n",
      "0.36122496477001503\n",
      "\n",
      "loss:\n",
      "0.18060434249160182\n",
      "mse:\n",
      "0.36120868498320363\n",
      "\n",
      "loss:\n",
      "0.1805965093710542\n",
      "mse:\n",
      "0.3611930187421084\n",
      "\n",
      "loss:\n",
      "0.18058699434651176\n",
      "mse:\n",
      "0.36117398869302353\n",
      "\n",
      "loss:\n",
      "0.1805791109690015\n",
      "mse:\n",
      "0.361158221938003\n",
      "Annealing learning rate. New rate is 0.007948342176603044\n",
      "\n",
      "loss:\n",
      "0.180570570454229\n",
      "mse:\n",
      "0.361141140908458\n",
      "\n",
      "loss:\n",
      "0.18056339464316803\n",
      "mse:\n",
      "0.36112678928633607\n",
      "\n",
      "loss:\n",
      "0.1805549768053751\n",
      "mse:\n",
      "0.3611099536107502\n",
      "\n",
      "loss:\n",
      "0.18054762386676732\n",
      "mse:\n",
      "0.36109524773353463\n",
      "\n",
      "loss:\n",
      "0.180540311388236\n",
      "mse:\n",
      "0.361080622776472\n",
      "\n",
      "loss:\n",
      "0.180532109192383\n",
      "mse:\n",
      "0.361064218384766\n",
      "\n",
      "loss:\n",
      "0.18052494537448993\n",
      "mse:\n",
      "0.36104989074897986\n",
      "\n",
      "loss:\n",
      "0.18051651408950786\n",
      "mse:\n",
      "0.3610330281790157\n",
      "\n",
      "loss:\n",
      "0.18050935485483202\n",
      "mse:\n",
      "0.36101870970966404\n",
      "\n",
      "loss:\n",
      "0.18050199366209935\n",
      "mse:\n",
      "0.3610039873241987\n",
      "\n",
      "loss:\n",
      "0.1804935972592412\n",
      "mse:\n",
      "0.3609871945184824\n",
      "\n",
      "loss:\n",
      "0.1804862003173811\n",
      "mse:\n",
      "0.3609724006347622\n",
      "\n",
      "loss:\n",
      "0.1804779806793708\n",
      "mse:\n",
      "0.3609559613587416\n",
      "\n",
      "loss:\n",
      "0.1804707960653421\n",
      "mse:\n",
      "0.3609415921306842\n",
      "\n",
      "loss:\n",
      "0.18046228131237044\n",
      "mse:\n",
      "0.3609245626247409\n",
      "\n",
      "loss:\n",
      "0.18045508648200775\n",
      "mse:\n",
      "0.3609101729640155\n",
      "\n",
      "loss:\n",
      "0.18044804214032764\n",
      "mse:\n",
      "0.3608960842806553\n",
      "\n",
      "loss:\n",
      "0.18043956423092786\n",
      "mse:\n",
      "0.3608791284618557\n",
      "\n",
      "loss:\n",
      "0.18043233151912103\n",
      "mse:\n",
      "0.36086466303824205\n",
      "\n",
      "loss:\n",
      "0.18042382473761093\n",
      "mse:\n",
      "0.36084764947522185\n",
      "\n",
      "loss:\n",
      "0.18041703510757012\n",
      "mse:\n",
      "0.36083407021514025\n",
      "\n",
      "loss:\n",
      "0.1804086139792003\n",
      "mse:\n",
      "0.3608172279584006\n",
      "\n",
      "loss:\n",
      "0.18040138759364707\n",
      "mse:\n",
      "0.36080277518729414\n",
      "\n",
      "loss:\n",
      "0.18039435609008442\n",
      "mse:\n",
      "0.36078871218016884\n",
      "\n",
      "loss:\n",
      "0.18038622503982005\n",
      "mse:\n",
      "0.3607724500796401\n",
      "\n",
      "loss:\n",
      "0.1803789705557318\n",
      "mse:\n",
      "0.3607579411114636\n",
      "Annealing learning rate. New rate is 0.007153507958942739\n",
      "\n",
      "loss:\n",
      "0.18037157205214205\n",
      "mse:\n",
      "0.3607431441042841\n",
      "\n",
      "loss:\n",
      "0.18036527051608833\n",
      "mse:\n",
      "0.36073054103217667\n",
      "\n",
      "loss:\n",
      "0.18035934734779674\n",
      "mse:\n",
      "0.3607186946955935\n",
      "\n",
      "loss:\n",
      "0.1803519695358421\n",
      "mse:\n",
      "0.3607039390716842\n",
      "\n",
      "loss:\n",
      "0.18034564377005666\n",
      "mse:\n",
      "0.3606912875401133\n",
      "\n",
      "loss:\n",
      "0.18033837275155584\n",
      "mse:\n",
      "0.3606767455031117\n",
      "\n",
      "loss:\n",
      "0.18033198143551035\n",
      "mse:\n",
      "0.3606639628710207\n",
      "\n",
      "loss:\n",
      "0.18032587856958066\n",
      "mse:\n",
      "0.3606517571391613\n",
      "\n",
      "loss:\n",
      "0.18031854222206475\n",
      "mse:\n",
      "0.3606370844441295\n",
      "\n",
      "loss:\n",
      "0.18031230380460808\n",
      "mse:\n",
      "0.36062460760921616\n",
      "\n",
      "loss:\n",
      "0.1803049988425003\n",
      "mse:\n",
      "0.3606099976850006\n",
      "\n",
      "loss:\n",
      "0.18029902714260096\n",
      "mse:\n",
      "0.3605980542852019\n",
      "\n",
      "loss:\n",
      "0.18029175671999736\n",
      "mse:\n",
      "0.36058351343999473\n",
      "\n",
      "loss:\n",
      "0.18028573482256688\n",
      "mse:\n",
      "0.36057146964513376\n",
      "\n",
      "loss:\n",
      "0.18027967088859587\n",
      "mse:\n",
      "0.36055934177719173\n",
      "\n",
      "loss:\n",
      "0.1802727861809434\n",
      "mse:\n",
      "0.3605455723618868\n",
      "\n",
      "loss:\n",
      "0.18026657589148473\n",
      "mse:\n",
      "0.36053315178296946\n",
      "\n",
      "loss:\n",
      "0.18025949997169896\n",
      "mse:\n",
      "0.3605189999433979\n",
      "\n",
      "loss:\n",
      "0.18025331939971964\n",
      "mse:\n",
      "0.3605066387994393\n",
      "\n",
      "loss:\n",
      "0.18024618999495048\n",
      "mse:\n",
      "0.36049237998990097\n",
      "\n",
      "loss:\n",
      "0.18024038112970836\n",
      "mse:\n",
      "0.3604807622594167\n",
      "\n",
      "loss:\n",
      "0.1802333078145262\n",
      "mse:\n",
      "0.3604666156290524\n",
      "\n",
      "loss:\n",
      "0.18022723847399943\n",
      "mse:\n",
      "0.36045447694799887\n",
      "\n",
      "loss:\n",
      "0.18022126599765098\n",
      "mse:\n",
      "0.36044253199530196\n",
      "\n",
      "loss:\n",
      "0.18021422245422922\n",
      "mse:\n",
      "0.36042844490845843\n",
      "\n",
      "loss:\n",
      "0.1802083730749371\n",
      "mse:\n",
      "0.3604167461498742\n",
      "Annealing learning rate. New rate is 0.006438157163048465\n",
      "\n",
      "loss:\n",
      "0.18020201330934926\n",
      "mse:\n",
      "0.3604040266186985\n",
      "\n",
      "loss:\n",
      "0.18019664103245597\n",
      "mse:\n",
      "0.36039328206491195\n",
      "\n",
      "loss:\n",
      "0.1801902804833309\n",
      "mse:\n",
      "0.3603805609666618\n",
      "\n",
      "loss:\n",
      "0.1801851161901793\n",
      "mse:\n",
      "0.3603702323803586\n",
      "\n",
      "loss:\n",
      "0.1801787775851962\n",
      "mse:\n",
      "0.3603575551703924\n",
      "\n",
      "loss:\n",
      "0.18017328002683555\n",
      "mse:\n",
      "0.3603465600536711\n",
      "\n",
      "loss:\n",
      "0.18016781037554375\n",
      "mse:\n",
      "0.3603356207510875\n",
      "\n",
      "loss:\n",
      "0.18016140964618552\n",
      "mse:\n",
      "0.36032281929237103\n",
      "\n",
      "loss:\n",
      "0.1801560907638761\n",
      "mse:\n",
      "0.3603121815277522\n",
      "\n",
      "loss:\n",
      "0.18014965324083548\n",
      "mse:\n",
      "0.36029930648167097\n",
      "\n",
      "loss:\n",
      "0.18014423492642376\n",
      "mse:\n",
      "0.3602884698528475\n",
      "\n",
      "loss:\n",
      "0.18013786171454488\n",
      "mse:\n",
      "0.36027572342908976\n",
      "\n",
      "loss:\n",
      "0.18013262632517701\n",
      "mse:\n",
      "0.36026525265035403\n",
      "\n",
      "loss:\n",
      "0.1801262273007909\n",
      "mse:\n",
      "0.3602524546015818\n",
      "\n",
      "loss:\n",
      "0.18012070405535283\n",
      "mse:\n",
      "0.36024140811070565\n",
      "\n",
      "loss:\n",
      "0.1801142906332946\n",
      "mse:\n",
      "0.3602285812665892\n",
      "\n",
      "loss:\n",
      "0.18010887226953454\n",
      "mse:\n",
      "0.3602177445390691\n",
      "\n",
      "loss:\n",
      "0.18010336663334892\n",
      "mse:\n",
      "0.36020673326669783\n",
      "\n",
      "loss:\n",
      "0.18009695520024985\n",
      "mse:\n",
      "0.3601939104004997\n",
      "\n",
      "loss:\n",
      "0.18009153032846403\n",
      "mse:\n",
      "0.36018306065692807\n",
      "\n",
      "loss:\n",
      "0.18008531219458995\n",
      "mse:\n",
      "0.3601706243891799\n",
      "\n",
      "loss:\n",
      "0.1800798184888687\n",
      "mse:\n",
      "0.3601596369777374\n",
      "\n",
      "loss:\n",
      "0.1800734652561415\n",
      "mse:\n",
      "0.360146930512283\n",
      "\n",
      "loss:\n",
      "0.18006792917646336\n",
      "mse:\n",
      "0.3601358583529267\n",
      "\n",
      "loss:\n",
      "0.1800617082822942\n",
      "mse:\n",
      "0.3601234165645884\n",
      "\n",
      "loss:\n",
      "0.18005622890434897\n",
      "mse:\n",
      "0.36011245780869794\n",
      "Annealing learning rate. New rate is 0.005794341446743619\n",
      "\n",
      "loss:\n",
      "0.180050482994395\n",
      "mse:\n",
      "0.36010096598879\n",
      "\n",
      "loss:\n",
      "0.1800455678377168\n",
      "mse:\n",
      "0.3600911356754336\n",
      "\n",
      "loss:\n",
      "0.18004068199045975\n",
      "mse:\n",
      "0.3600813639809195\n",
      "\n",
      "loss:\n",
      "0.18003510048007326\n",
      "mse:\n",
      "0.3600702009601465\n",
      "\n",
      "loss:\n",
      "0.18003012452004746\n",
      "mse:\n",
      "0.3600602490400949\n",
      "\n",
      "loss:\n",
      "0.18002441152123225\n",
      "mse:\n",
      "0.3600488230424645\n",
      "\n",
      "loss:\n",
      "0.18001946626020396\n",
      "mse:\n",
      "0.3600389325204079\n",
      "\n",
      "loss:\n",
      "0.1800138864796397\n",
      "mse:\n",
      "0.3600277729592794\n",
      "\n",
      "loss:\n",
      "0.18000903145341288\n",
      "mse:\n",
      "0.36001806290682575\n",
      "\n",
      "loss:\n",
      "0.18000332530262386\n",
      "mse:\n",
      "0.3600066506052477\n",
      "\n",
      "loss:\n",
      "0.17999838697237336\n",
      "mse:\n",
      "0.3599967739447467\n",
      "\n",
      "loss:\n",
      "0.17999370164927442\n",
      "mse:\n",
      "0.35998740329854884\n",
      "\n",
      "loss:\n",
      "0.17998799832059026\n",
      "mse:\n",
      "0.3599759966411805\n",
      "\n",
      "loss:\n",
      "0.1799830193129694\n",
      "mse:\n",
      "0.3599660386259388\n",
      "\n",
      "loss:\n",
      "0.17997732471515995\n",
      "mse:\n",
      "0.3599546494303199\n",
      "\n",
      "loss:\n",
      "0.17997254115827496\n",
      "mse:\n",
      "0.3599450823165499\n",
      "\n",
      "loss:\n",
      "0.1799668020367695\n",
      "mse:\n",
      "0.359933604073539\n",
      "\n",
      "loss:\n",
      "0.17996190106878004\n",
      "mse:\n",
      "0.3599238021375601\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0b31661f8aab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m                       \u001b[0mvalidation_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val_normalize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                       \u001b[0mvalidation_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_val_normalize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                       batch_size = batch)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# RESULTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-c0a5c4f3a7c7>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, n_hidden, nodes, activations, lr, validation_X, validation_y, batch_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-c0a5c4f3a7c7>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batchy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mbatched_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mvalidation_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_val_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-c0a5c4f3a7c7>\u001b[0m in \u001b[0;36m_forward_propagation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mweight_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-c0a5c4f3a7c7>\u001b[0m in \u001b[0;36m_activation\u001b[0;34m(self, data, activation)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0mbuff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mbuff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-c0a5c4f3a7c7>\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 5x34 (batch = 0)\n",
    "batch = 0\n",
    "width = 34\n",
    "depth = 5\n",
    "\n",
    "INPUT_SIZE = X_train_normalize.shape[1]\n",
    "OUTPUT_SIZE = 1\n",
    "LEARNING_RATE = 0.75\n",
    "\n",
    "nodes = [INPUT_SIZE] + [width for i in range(depth)] + [OUTPUT_SIZE]\n",
    "activations = [\"relu\" for i in range(len(nodes))]\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "losses, mses = nn.fit(X = X_train_normalize,\n",
    "                      y = y_train_normalize,\n",
    "                      n_hidden = len(nodes) - 2,\n",
    "                      nodes = nodes,\n",
    "                      activations = activations,\n",
    "                      lr = LEARNING_RATE,\n",
    "                      validation_X = X_val_normalize,\n",
    "                      validation_y = y_val_normalize,\n",
    "                      batch_size = batch)\n",
    "\n",
    "# RESULTS\n",
    "## loss = 0.179\n",
    "## mse = 0.359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss:\n",
      "0.446054319495708\n",
      "mse:\n",
      "0.892108638991416\n",
      "\n",
      "loss:\n",
      "0.4456410835021251\n",
      "mse:\n",
      "0.8912821670042502\n",
      "\n",
      "loss:\n",
      "0.44515602252676706\n",
      "mse:\n",
      "0.8903120450535341\n",
      "\n",
      "loss:\n",
      "0.4443780603470162\n",
      "mse:\n",
      "0.8887561206940324\n",
      "\n",
      "loss:\n",
      "0.4434748554042438\n",
      "mse:\n",
      "0.8869497108084876\n",
      "\n",
      "loss:\n",
      "0.44260926185499694\n",
      "mse:\n",
      "0.8852185237099939\n",
      "\n",
      "loss:\n",
      "0.44506370377343796\n",
      "mse:\n",
      "0.8901274075468759\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.5106281003278244\n",
      "mse:\n",
      "1.0212562006556487\n",
      "INCREASE IN LOSS\n",
      "Decreasing learning rate. New rate is 1.5\n",
      "\n",
      "loss:\n",
      "0.49122993432443507\n",
      "mse:\n",
      "0.9824598686488701\n",
      "\n",
      "loss:\n",
      "0.47066555198933563\n",
      "mse:\n",
      "0.9413311039786713\n",
      "\n",
      "loss:\n",
      "0.4561918224683817\n",
      "mse:\n",
      "0.9123836449367634\n",
      "\n",
      "loss:\n",
      "0.45197270692723357\n",
      "mse:\n",
      "0.9039454138544671\n",
      "\n",
      "loss:\n",
      "0.44690473802775\n",
      "mse:\n",
      "0.8938094760555\n",
      "\n",
      "loss:\n",
      "0.4458490382541645\n",
      "mse:\n",
      "0.891698076508329\n",
      "\n",
      "loss:\n",
      "0.44349918258658444\n",
      "mse:\n",
      "0.8869983651731689\n",
      "\n",
      "loss:\n",
      "0.44207789075976645\n",
      "mse:\n",
      "0.8841557815195329\n",
      "\n",
      "loss:\n",
      "0.44008399303463624\n",
      "mse:\n",
      "0.8801679860692725\n",
      "\n",
      "loss:\n",
      "0.4392426061347652\n",
      "mse:\n",
      "0.8784852122695304\n",
      "\n",
      "loss:\n",
      "0.43925696933665287\n",
      "mse:\n",
      "0.8785139386733057\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.44594635496514873\n",
      "mse:\n",
      "0.8918927099302975\n",
      "INCREASE IN LOSS\n",
      "Decreasing learning rate. New rate is 0.75\n",
      "\n",
      "loss:\n",
      "0.4280703718246174\n",
      "mse:\n",
      "0.8561407436492348\n",
      "\n",
      "loss:\n",
      "0.42156671687625913\n",
      "mse:\n",
      "0.8431334337525183\n",
      "\n",
      "loss:\n",
      "0.41540446148982313\n",
      "mse:\n",
      "0.8308089229796463\n",
      "\n",
      "loss:\n",
      "0.40892806923298664\n",
      "mse:\n",
      "0.8178561384659733\n",
      "\n",
      "loss:\n",
      "0.40140617550551894\n",
      "mse:\n",
      "0.8028123510110379\n",
      "\n",
      "loss:\n",
      "0.3953081716434281\n",
      "mse:\n",
      "0.7906163432868561\n",
      "\n",
      "loss:\n",
      "0.40304737100082894\n",
      "mse:\n",
      "0.8060947420016579\n",
      "INCREASE IN LOSS\n",
      "Annealing learning rate. New rate is 0.675\n",
      "\n",
      "loss:\n",
      "0.4388063583185973\n",
      "mse:\n",
      "0.8776127166371946\n",
      "INCREASE IN LOSS\n",
      "Decreasing learning rate. New rate is 0.3375\n",
      "\n",
      "loss:\n",
      "0.3678424218448598\n",
      "mse:\n",
      "0.7356848436897196\n",
      "\n",
      "loss:\n",
      "0.36045719352977806\n",
      "mse:\n",
      "0.7209143870595561\n",
      "\n",
      "loss:\n",
      "0.3529660822035142\n",
      "mse:\n",
      "0.7059321644070284\n",
      "\n",
      "loss:\n",
      "0.34525502103140604\n",
      "mse:\n",
      "0.6905100420628121\n",
      "\n",
      "loss:\n",
      "0.3376160855235378\n",
      "mse:\n",
      "0.6752321710470756\n",
      "\n",
      "loss:\n",
      "0.32999457619763756\n",
      "mse:\n",
      "0.6599891523952751\n",
      "\n",
      "loss:\n",
      "0.32272176106720685\n",
      "mse:\n",
      "0.6454435221344137\n",
      "\n",
      "loss:\n",
      "0.3156947646291555\n",
      "mse:\n",
      "0.631389529258311\n",
      "\n",
      "loss:\n",
      "0.30920720537183344\n",
      "mse:\n",
      "0.6184144107436669\n",
      "\n",
      "loss:\n",
      "0.3030212906735152\n",
      "mse:\n",
      "0.6060425813470304\n",
      "\n",
      "loss:\n",
      "0.2977807713883061\n",
      "mse:\n",
      "0.5955615427766122\n",
      "\n",
      "loss:\n",
      "0.29372756972381325\n",
      "mse:\n",
      "0.5874551394476265\n",
      "\n",
      "loss:\n",
      "0.29611644996787834\n",
      "mse:\n",
      "0.5922328999357567\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.32333313697411725\n",
      "mse:\n",
      "0.6466662739482345\n",
      "INCREASE IN LOSS\n",
      "Decreasing learning rate. New rate is 0.16875\n",
      "\n",
      "loss:\n",
      "0.2928682378076029\n",
      "mse:\n",
      "0.5857364756152058\n",
      "\n",
      "loss:\n",
      "0.2825884186668036\n",
      "mse:\n",
      "0.5651768373336072\n",
      "\n",
      "loss:\n",
      "0.2783510867123705\n",
      "mse:\n",
      "0.556702173424741\n",
      "\n",
      "loss:\n",
      "0.27534803302364197\n",
      "mse:\n",
      "0.5506960660472839\n",
      "\n",
      "loss:\n",
      "0.2731160044258882\n",
      "mse:\n",
      "0.5462320088517764\n",
      "\n",
      "loss:\n",
      "0.2709165518433654\n",
      "mse:\n",
      "0.5418331036867308\n",
      "\n",
      "loss:\n",
      "0.2689810056308779\n",
      "mse:\n",
      "0.5379620112617558\n",
      "\n",
      "loss:\n",
      "0.2669962876825682\n",
      "mse:\n",
      "0.5339925753651364\n",
      "\n",
      "loss:\n",
      "0.2651701583756853\n",
      "mse:\n",
      "0.5303403167513706\n",
      "\n",
      "loss:\n",
      "0.2633095211134014\n",
      "mse:\n",
      "0.5266190422268028\n",
      "\n",
      "loss:\n",
      "0.26156399213162723\n",
      "mse:\n",
      "0.5231279842632545\n",
      "Annealing learning rate. New rate is 0.151875\n",
      "\n",
      "loss:\n",
      "0.259951358133877\n",
      "mse:\n",
      "0.519902716267754\n",
      "\n",
      "loss:\n",
      "0.2584286027784707\n",
      "mse:\n",
      "0.5168572055569414\n",
      "\n",
      "loss:\n",
      "0.2568809336022152\n",
      "mse:\n",
      "0.5137618672044304\n",
      "\n",
      "loss:\n",
      "0.2553990759796114\n",
      "mse:\n",
      "0.5107981519592228\n",
      "\n",
      "loss:\n",
      "0.2538999002862157\n",
      "mse:\n",
      "0.5077998005724313\n",
      "\n",
      "loss:\n",
      "0.25245308648950443\n",
      "mse:\n",
      "0.5049061729790089\n",
      "\n",
      "loss:\n",
      "0.25099635497737444\n",
      "mse:\n",
      "0.5019927099547489\n",
      "\n",
      "loss:\n",
      "0.24959422803027378\n",
      "mse:\n",
      "0.49918845606054757\n",
      "\n",
      "loss:\n",
      "0.2481730198250981\n",
      "mse:\n",
      "0.4963460396501962\n",
      "\n",
      "loss:\n",
      "0.246802044124897\n",
      "mse:\n",
      "0.493604088249794\n",
      "\n",
      "loss:\n",
      "0.24539998792039747\n",
      "mse:\n",
      "0.49079997584079493\n",
      "\n",
      "loss:\n",
      "0.24404807959404254\n",
      "mse:\n",
      "0.4880961591880851\n",
      "\n",
      "loss:\n",
      "0.24266475595876147\n",
      "mse:\n",
      "0.48532951191752294\n",
      "\n",
      "loss:\n",
      "0.2413290454685181\n",
      "mse:\n",
      "0.4826580909370362\n",
      "\n",
      "loss:\n",
      "0.2399593351662655\n",
      "mse:\n",
      "0.479918670332531\n",
      "\n",
      "loss:\n",
      "0.23865259377778897\n",
      "mse:\n",
      "0.47730518755557794\n",
      "\n",
      "loss:\n",
      "0.23731207764304194\n",
      "mse:\n",
      "0.47462415528608387\n",
      "\n",
      "loss:\n",
      "0.2361103322825669\n",
      "mse:\n",
      "0.4722206645651338\n",
      "\n",
      "loss:\n",
      "0.2349153836527494\n",
      "mse:\n",
      "0.4698307673054988\n",
      "\n",
      "loss:\n",
      "0.23406929967961607\n",
      "mse:\n",
      "0.46813859935923213\n",
      "\n",
      "loss:\n",
      "0.233740839859696\n",
      "mse:\n",
      "0.467481679719392\n",
      "\n",
      "loss:\n",
      "0.23530558546165947\n",
      "mse:\n",
      "0.47061117092331894\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.24042211084386872\n",
      "mse:\n",
      "0.48084422168773744\n",
      "INCREASE IN LOSS\n",
      "Decreasing learning rate. New rate is 0.0759375\n",
      "\n",
      "loss:\n",
      "0.23014194300155336\n",
      "mse:\n",
      "0.4602838860031067\n",
      "\n",
      "loss:\n",
      "0.2284672805835913\n",
      "mse:\n",
      "0.4569345611671826\n",
      "\n",
      "loss:\n",
      "0.22779101475992097\n",
      "mse:\n",
      "0.45558202951984195\n",
      "Annealing learning rate. New rate is 0.06834375000000001\n",
      "\n",
      "loss:\n",
      "0.2272298375920318\n",
      "mse:\n",
      "0.4544596751840636\n",
      "\n",
      "loss:\n",
      "0.22669381934059313\n",
      "mse:\n",
      "0.45338763868118626\n",
      "\n",
      "loss:\n",
      "0.22616230088490394\n",
      "mse:\n",
      "0.4523246017698079\n",
      "\n",
      "loss:\n",
      "0.22563583139874055\n",
      "mse:\n",
      "0.4512716627974811\n",
      "\n",
      "loss:\n",
      "0.22511273551242503\n",
      "mse:\n",
      "0.45022547102485005\n",
      "\n",
      "loss:\n",
      "0.22459421681730182\n",
      "mse:\n",
      "0.44918843363460365\n",
      "\n",
      "loss:\n",
      "0.2240811233463847\n",
      "mse:\n",
      "0.4481622466927694\n",
      "\n",
      "loss:\n",
      "0.22357428171175006\n",
      "mse:\n",
      "0.4471485634235001\n",
      "\n",
      "loss:\n",
      "0.22307639059652412\n",
      "mse:\n",
      "0.44615278119304824\n",
      "\n",
      "loss:\n",
      "0.22258609055806974\n",
      "mse:\n",
      "0.4451721811161395\n",
      "\n",
      "loss:\n",
      "0.2221021435330893\n",
      "mse:\n",
      "0.4442042870661786\n",
      "\n",
      "loss:\n",
      "0.22162462155502546\n",
      "mse:\n",
      "0.44324924311005093\n",
      "\n",
      "loss:\n",
      "0.2211536910587755\n",
      "mse:\n",
      "0.442307382117551\n",
      "\n",
      "loss:\n",
      "0.22068939608308033\n",
      "mse:\n",
      "0.44137879216616066\n",
      "\n",
      "loss:\n",
      "0.22023227755016395\n",
      "mse:\n",
      "0.4404645551003279\n",
      "\n",
      "loss:\n",
      "0.21978167959819128\n",
      "mse:\n",
      "0.43956335919638256\n",
      "\n",
      "loss:\n",
      "0.21933868666356635\n",
      "mse:\n",
      "0.4386773733271327\n",
      "\n",
      "loss:\n",
      "0.21890166179011816\n",
      "mse:\n",
      "0.4378033235802363\n",
      "\n",
      "loss:\n",
      "0.2184713621918727\n",
      "mse:\n",
      "0.4369427243837454\n",
      "\n",
      "loss:\n",
      "0.2180486865908401\n",
      "mse:\n",
      "0.4360973731816802\n",
      "\n",
      "loss:\n",
      "0.21763313422371075\n",
      "mse:\n",
      "0.4352662684474215\n",
      "\n",
      "loss:\n",
      "0.2172248319588857\n",
      "mse:\n",
      "0.4344496639177714\n",
      "\n",
      "loss:\n",
      "0.21682259360078224\n",
      "mse:\n",
      "0.4336451872015645\n",
      "\n",
      "loss:\n",
      "0.2164270072069919\n",
      "mse:\n",
      "0.4328540144139838\n",
      "\n",
      "loss:\n",
      "0.21603815142101984\n",
      "mse:\n",
      "0.4320763028420397\n",
      "\n",
      "loss:\n",
      "0.21565621725097223\n",
      "mse:\n",
      "0.43131243450194445\n",
      "Annealing learning rate. New rate is 0.06150937500000001\n",
      "\n",
      "loss:\n",
      "0.21531782189228912\n",
      "mse:\n",
      "0.43063564378457825\n",
      "\n",
      "loss:\n",
      "0.21498555835568595\n",
      "mse:\n",
      "0.4299711167113719\n",
      "\n",
      "loss:\n",
      "0.21465705997705625\n",
      "mse:\n",
      "0.4293141199541125\n",
      "\n",
      "loss:\n",
      "0.21433311001476527\n",
      "mse:\n",
      "0.42866622002953053\n",
      "\n",
      "loss:\n",
      "0.21401338187850266\n",
      "mse:\n",
      "0.4280267637570053\n",
      "\n",
      "loss:\n",
      "0.21369900809463221\n",
      "mse:\n",
      "0.42739801618926443\n",
      "\n",
      "loss:\n",
      "0.21338988466975453\n",
      "mse:\n",
      "0.42677976933950906\n",
      "\n",
      "loss:\n",
      "0.2130845224760402\n",
      "mse:\n",
      "0.4261690449520804\n",
      "\n",
      "loss:\n",
      "0.21278394289009658\n",
      "mse:\n",
      "0.42556788578019317\n",
      "\n",
      "loss:\n",
      "0.2124901869704587\n",
      "mse:\n",
      "0.4249803739409174\n",
      "\n",
      "loss:\n",
      "0.21220158695988267\n",
      "mse:\n",
      "0.42440317391976534\n",
      "\n",
      "loss:\n",
      "0.2119173304941083\n",
      "mse:\n",
      "0.4238346609882166\n",
      "\n",
      "loss:\n",
      "0.21163778122883942\n",
      "mse:\n",
      "0.42327556245767883\n",
      "\n",
      "loss:\n",
      "0.21136353739864971\n",
      "mse:\n",
      "0.42272707479729943\n",
      "\n",
      "loss:\n",
      "0.21109279543538864\n",
      "mse:\n",
      "0.4221855908707773\n",
      "\n",
      "loss:\n",
      "0.2108252447536578\n",
      "mse:\n",
      "0.4216504895073156\n",
      "\n",
      "loss:\n",
      "0.2105613850520935\n",
      "mse:\n",
      "0.421122770104187\n",
      "\n",
      "loss:\n",
      "0.21030239392437805\n",
      "mse:\n",
      "0.4206047878487561\n",
      "\n",
      "loss:\n",
      "0.2100492709795792\n",
      "mse:\n",
      "0.4200985419591584\n",
      "\n",
      "loss:\n",
      "0.20980035773172637\n",
      "mse:\n",
      "0.41960071546345273\n",
      "\n",
      "loss:\n",
      "0.20955684700813512\n",
      "mse:\n",
      "0.41911369401627024\n",
      "\n",
      "loss:\n",
      "0.2093169671254945\n",
      "mse:\n",
      "0.418633934250989\n",
      "\n",
      "loss:\n",
      "0.20908149854777322\n",
      "mse:\n",
      "0.41816299709554644\n",
      "\n",
      "loss:\n",
      "0.20885136825958886\n",
      "mse:\n",
      "0.4177027365191777\n",
      "\n",
      "loss:\n",
      "0.20862571560984908\n",
      "mse:\n",
      "0.41725143121969815\n",
      "\n",
      "loss:\n",
      "0.20840338386676321\n",
      "mse:\n",
      "0.41680676773352643\n",
      "Annealing learning rate. New rate is 0.05535843750000001\n",
      "\n",
      "loss:\n",
      "0.20820626851633373\n",
      "mse:\n",
      "0.41641253703266745\n",
      "\n",
      "loss:\n",
      "0.20801344125879592\n",
      "mse:\n",
      "0.41602688251759185\n",
      "\n",
      "loss:\n",
      "0.20782416777221197\n",
      "mse:\n",
      "0.41564833554442393\n",
      "\n",
      "loss:\n",
      "0.2076379503018915\n",
      "mse:\n",
      "0.415275900603783\n",
      "\n",
      "loss:\n",
      "0.20745484149205318\n",
      "mse:\n",
      "0.41490968298410635\n",
      "\n",
      "loss:\n",
      "0.2072737781653008\n",
      "mse:\n",
      "0.4145475563306016\n",
      "\n",
      "loss:\n",
      "0.20709611827741664\n",
      "mse:\n",
      "0.4141922365548333\n",
      "\n",
      "loss:\n",
      "0.2069219944232354\n",
      "mse:\n",
      "0.4138439888464708\n",
      "\n",
      "loss:\n",
      "0.2067505202122226\n",
      "mse:\n",
      "0.4135010404244452\n"
     ]
    }
   ],
   "source": [
    "# 7x34 (batch = 0)\n",
    "batch = 0\n",
    "width = 34\n",
    "depth = 7\n",
    "\n",
    "INPUT_SIZE = X_train_normalize.shape[1]\n",
    "OUTPUT_SIZE = 1\n",
    "LEARNING_RATE = 3\n",
    "\n",
    "nodes = [INPUT_SIZE] + [width for i in range(depth)] + [OUTPUT_SIZE]\n",
    "activations = [\"relu\" for i in range(len(nodes))]\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "losses, mses = nn.fit(X = X_train_normalize,\n",
    "                      y = y_train_normalize,\n",
    "                      n_hidden = len(nodes) - 2,\n",
    "                      nodes = nodes,\n",
    "                      activations = activations,\n",
    "                      lr = LEARNING_RATE,\n",
    "                      validation_X = X_val_normalize,\n",
    "                      validation_y = y_val_normalize,\n",
    "                      batch_size = batch)\n",
    "\n",
    "# RESULTS\n",
    "## loss = 0.212\n",
    "## mse = 0.424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss:\n",
      "0.446029208327168\n",
      "mse:\n",
      "0.892058416654336\n",
      "\n",
      "loss:\n",
      "0.44604037019737613\n",
      "mse:\n",
      "0.8920807403947523\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.44597707225131206\n",
      "mse:\n",
      "0.8919541445026241\n",
      "\n",
      "loss:\n",
      "0.4467118600236807\n",
      "mse:\n",
      "0.8934237200473614\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.4488302893938686\n",
      "mse:\n",
      "0.8976605787877372\n",
      "INCREASE IN LOSS\n",
      "Decreasing learning rate. New rate is 1.5\n",
      "\n",
      "loss:\n",
      "0.44783336850520583\n",
      "mse:\n",
      "0.8956667370104117\n",
      "\n",
      "loss:\n",
      "0.4462584863518524\n",
      "mse:\n",
      "0.8925169727037048\n",
      "\n",
      "loss:\n",
      "0.4462630286534122\n",
      "mse:\n",
      "0.8925260573068244\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.44598275095472023\n",
      "mse:\n",
      "0.8919655019094405\n",
      "\n",
      "loss:\n",
      "0.4460482869556994\n",
      "mse:\n",
      "0.8920965739113988\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.4459765588431891\n",
      "mse:\n",
      "0.8919531176863782\n",
      "\n",
      "loss:\n",
      "0.4459985322939996\n",
      "mse:\n",
      "0.8919970645879992\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.4459818544139639\n",
      "mse:\n",
      "0.8919637088279277\n",
      "\n",
      "loss:\n",
      "0.44598930443655627\n",
      "mse:\n",
      "0.8919786088731125\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.4459831253621205\n",
      "mse:\n",
      "0.891966250724241\n",
      "\n",
      "loss:\n",
      "0.44598454508593816\n",
      "mse:\n",
      "0.8919690901718763\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.4459813517591987\n",
      "mse:\n",
      "0.8919627035183973\n",
      "\n",
      "loss:\n",
      "0.4459808938347509\n",
      "mse:\n",
      "0.8919617876695018\n",
      "\n",
      "loss:\n",
      "0.445978817616333\n",
      "mse:\n",
      "0.891957635232666\n",
      "\n",
      "loss:\n",
      "0.4459775501753459\n",
      "mse:\n",
      "0.8919551003506918\n",
      "\n",
      "loss:\n",
      "0.4459702128057039\n",
      "mse:\n",
      "0.8919404256114078\n",
      "\n",
      "loss:\n",
      "0.44596852820979654\n",
      "mse:\n",
      "0.8919370564195931\n",
      "\n",
      "loss:\n",
      "0.4459665101504963\n",
      "mse:\n",
      "0.8919330203009926\n",
      "\n",
      "loss:\n",
      "0.44596496376133216\n",
      "mse:\n",
      "0.8919299275226643\n",
      "\n",
      "loss:\n",
      "0.4459630092133861\n",
      "mse:\n",
      "0.8919260184267722\n",
      "\n",
      "loss:\n",
      "0.44596129850201816\n",
      "mse:\n",
      "0.8919225970040363\n",
      "\n",
      "loss:\n",
      "0.44595941046391946\n",
      "mse:\n",
      "0.8919188209278389\n",
      "Annealing learning rate. New rate is 1.35\n",
      "\n",
      "loss:\n",
      "0.44595782974606535\n",
      "mse:\n",
      "0.8919156594921307\n",
      "\n",
      "loss:\n",
      "0.4459561444430692\n",
      "mse:\n",
      "0.8919122888861384\n",
      "\n",
      "loss:\n",
      "0.44595447466202387\n",
      "mse:\n",
      "0.8919089493240477\n",
      "\n",
      "loss:\n",
      "0.4459528271410172\n",
      "mse:\n",
      "0.8919056542820344\n",
      "\n",
      "loss:\n",
      "0.4459510585639922\n",
      "mse:\n",
      "0.8919021171279844\n",
      "\n",
      "loss:\n",
      "0.44594934386862095\n",
      "mse:\n",
      "0.8918986877372419\n",
      "\n",
      "loss:\n",
      "0.44594754655233865\n",
      "mse:\n",
      "0.8918950931046773\n",
      "\n",
      "loss:\n",
      "0.4459457797344132\n",
      "mse:\n",
      "0.8918915594688264\n",
      "\n",
      "loss:\n",
      "0.44594398189586776\n",
      "mse:\n",
      "0.8918879637917355\n",
      "\n",
      "loss:\n",
      "0.4459421412269355\n",
      "mse:\n",
      "0.891884282453871\n",
      "\n",
      "loss:\n",
      "0.44594023146008543\n",
      "mse:\n",
      "0.8918804629201709\n",
      "\n",
      "loss:\n",
      "0.445938257658935\n",
      "mse:\n",
      "0.89187651531787\n",
      "\n",
      "loss:\n",
      "0.44593623541856364\n",
      "mse:\n",
      "0.8918724708371273\n",
      "\n",
      "loss:\n",
      "0.445934136340874\n",
      "mse:\n",
      "0.891868272681748\n",
      "\n",
      "loss:\n",
      "0.44593202153228545\n",
      "mse:\n",
      "0.8918640430645709\n",
      "\n",
      "loss:\n",
      "0.4459297989035287\n",
      "mse:\n",
      "0.8918595978070574\n",
      "\n",
      "loss:\n",
      "0.4459275410812457\n",
      "mse:\n",
      "0.8918550821624914\n",
      "\n",
      "loss:\n",
      "0.44592524699997893\n",
      "mse:\n",
      "0.8918504939999579\n",
      "\n",
      "loss:\n",
      "0.44592287689969157\n",
      "mse:\n",
      "0.8918457537993831\n",
      "\n",
      "loss:\n",
      "0.4459204184458828\n",
      "mse:\n",
      "0.8918408368917656\n",
      "\n",
      "loss:\n",
      "0.44591788589184583\n",
      "mse:\n",
      "0.8918357717836917\n",
      "\n",
      "loss:\n",
      "0.445915258836397\n",
      "mse:\n",
      "0.891830517672794\n",
      "\n",
      "loss:\n",
      "0.4459125380165272\n",
      "mse:\n",
      "0.8918250760330544\n",
      "\n",
      "loss:\n",
      "0.4459098003697292\n",
      "mse:\n",
      "0.8918196007394584\n",
      "\n",
      "loss:\n",
      "0.4459069670374343\n",
      "mse:\n",
      "0.8918139340748686\n",
      "\n",
      "loss:\n",
      "0.44590403560020747\n",
      "mse:\n",
      "0.8918080712004149\n",
      "Annealing learning rate. New rate is 1.215\n",
      "\n",
      "loss:\n",
      "0.4459013071053775\n",
      "mse:\n",
      "0.891802614210755\n",
      "\n",
      "loss:\n",
      "0.4458985020157141\n",
      "mse:\n",
      "0.8917970040314283\n",
      "\n",
      "loss:\n",
      "0.4458955865037159\n",
      "mse:\n",
      "0.8917911730074318\n",
      "\n",
      "loss:\n",
      "0.4458925851724572\n",
      "mse:\n",
      "0.8917851703449144\n",
      "\n",
      "loss:\n",
      "0.4458894995271754\n",
      "mse:\n",
      "0.8917789990543508\n",
      "\n",
      "loss:\n",
      "0.4458862991710159\n",
      "mse:\n",
      "0.8917725983420318\n",
      "\n",
      "loss:\n",
      "0.445882955179931\n",
      "mse:\n",
      "0.891765910359862\n",
      "\n",
      "loss:\n",
      "0.445879373076795\n",
      "mse:\n",
      "0.89175874615359\n",
      "\n",
      "loss:\n",
      "0.44587534068075196\n",
      "mse:\n",
      "0.8917506813615039\n",
      "\n",
      "loss:\n",
      "0.44587076538639675\n",
      "mse:\n",
      "0.8917415307727935\n",
      "\n",
      "loss:\n",
      "0.44586610354818546\n",
      "mse:\n",
      "0.8917322070963709\n",
      "\n",
      "loss:\n",
      "0.4458614823606965\n",
      "mse:\n",
      "0.891722964721393\n",
      "\n",
      "loss:\n",
      "0.445856659858364\n",
      "mse:\n",
      "0.891713319716728\n",
      "\n",
      "loss:\n",
      "0.4458509099404139\n",
      "mse:\n",
      "0.8917018198808278\n",
      "\n",
      "loss:\n",
      "0.4458435535961594\n",
      "mse:\n",
      "0.8916871071923188\n",
      "\n",
      "loss:\n",
      "0.4458379212205994\n",
      "mse:\n",
      "0.8916758424411988\n",
      "\n",
      "loss:\n",
      "0.4458326380535861\n",
      "mse:\n",
      "0.8916652761071722\n",
      "\n",
      "loss:\n",
      "0.44582708358673895\n",
      "mse:\n",
      "0.8916541671734779\n",
      "\n",
      "loss:\n",
      "0.44582140369200485\n",
      "mse:\n",
      "0.8916428073840097\n",
      "\n",
      "loss:\n",
      "0.4458155617720336\n",
      "mse:\n",
      "0.8916311235440672\n",
      "\n",
      "loss:\n",
      "0.4458092857670139\n",
      "mse:\n",
      "0.8916185715340278\n",
      "\n",
      "loss:\n",
      "0.44580249475587247\n",
      "mse:\n",
      "0.8916049895117449\n",
      "\n",
      "loss:\n",
      "0.4457949430301128\n",
      "mse:\n",
      "0.8915898860602256\n",
      "\n",
      "loss:\n",
      "0.445783850527786\n",
      "mse:\n",
      "0.891567701055572\n",
      "\n",
      "loss:\n",
      "0.44574048201622607\n",
      "mse:\n",
      "0.8914809640324521\n",
      "\n",
      "loss:\n",
      "0.4457638239032706\n",
      "mse:\n",
      "0.8915276478065411\n",
      "INCREASE IN LOSS\n",
      "Annealing learning rate. New rate is 1.0935000000000001\n",
      "\n",
      "loss:\n",
      "0.44568398556347855\n",
      "mse:\n",
      "0.8913679711269571\n",
      "\n",
      "loss:\n",
      "0.4456831697049296\n",
      "mse:\n",
      "0.8913663394098592\n",
      "\n",
      "loss:\n",
      "0.44566423159526064\n",
      "mse:\n",
      "0.8913284631905213\n",
      "\n",
      "loss:\n",
      "0.44565089088878723\n",
      "mse:\n",
      "0.8913017817775745\n",
      "\n",
      "loss:\n",
      "0.4456352477130289\n",
      "mse:\n",
      "0.8912704954260579\n",
      "\n",
      "loss:\n",
      "0.4456185911510057\n",
      "mse:\n",
      "0.8912371823020114\n",
      "\n",
      "loss:\n",
      "0.4456013513646731\n",
      "mse:\n",
      "0.8912027027293462\n",
      "\n",
      "loss:\n",
      "0.4455829436943715\n",
      "mse:\n",
      "0.891165887388743\n",
      "\n",
      "loss:\n",
      "0.4455621357587992\n",
      "mse:\n",
      "0.8911242715175984\n",
      "\n",
      "loss:\n",
      "0.44554100563654314\n",
      "mse:\n",
      "0.8910820112730863\n",
      "\n",
      "loss:\n",
      "0.44551020738555736\n",
      "mse:\n",
      "0.8910204147711147\n",
      "\n",
      "loss:\n",
      "0.4454840480445922\n",
      "mse:\n",
      "0.8909680960891844\n",
      "\n",
      "loss:\n",
      "0.4454603832658715\n",
      "mse:\n",
      "0.890920766531743\n",
      "\n",
      "loss:\n",
      "0.4454340012774299\n",
      "mse:\n",
      "0.8908680025548598\n",
      "\n",
      "loss:\n",
      "0.44540705082545784\n",
      "mse:\n",
      "0.8908141016509157\n",
      "\n",
      "loss:\n",
      "0.44537797947407815\n",
      "mse:\n",
      "0.8907559589481563\n",
      "\n",
      "loss:\n",
      "0.44534692394402636\n",
      "mse:\n",
      "0.8906938478880527\n",
      "\n",
      "loss:\n",
      "0.44531399087583773\n",
      "mse:\n",
      "0.8906279817516755\n",
      "\n",
      "loss:\n",
      "0.4452786150730919\n",
      "mse:\n",
      "0.8905572301461838\n",
      "\n",
      "loss:\n",
      "0.44524102296625595\n",
      "mse:\n",
      "0.8904820459325119\n",
      "\n",
      "loss:\n",
      "0.44520012288577826\n",
      "mse:\n",
      "0.8904002457715565\n",
      "\n",
      "loss:\n",
      "0.4451566034403883\n",
      "mse:\n",
      "0.8903132068807766\n",
      "\n",
      "loss:\n",
      "0.4451092980589055\n",
      "mse:\n",
      "0.890218596117811\n",
      "\n",
      "loss:\n",
      "0.4450590873225109\n",
      "mse:\n",
      "0.8901181746450219\n",
      "\n",
      "loss:\n",
      "0.44500343998368874\n",
      "mse:\n",
      "0.8900068799673775\n",
      "\n",
      "loss:\n",
      "0.44494464403354506\n",
      "mse:\n",
      "0.8898892880670901\n",
      "Annealing learning rate. New rate is 0.9841500000000002\n",
      "\n",
      "loss:\n",
      "0.4448867114125203\n",
      "mse:\n",
      "0.8897734228250406\n",
      "\n",
      "loss:\n",
      "0.4448233382400198\n",
      "mse:\n",
      "0.8896466764800396\n",
      "\n",
      "loss:\n",
      "0.444751862015329\n",
      "mse:\n",
      "0.889503724030658\n",
      "\n",
      "loss:\n",
      "0.44460931554161437\n",
      "mse:\n",
      "0.8892186310832287\n",
      "\n",
      "loss:\n",
      "0.44452758670582837\n",
      "mse:\n",
      "0.8890551734116567\n",
      "\n",
      "loss:\n",
      "0.4443842567165618\n",
      "mse:\n",
      "0.8887685134331236\n",
      "\n",
      "loss:\n",
      "0.4442928313685666\n",
      "mse:\n",
      "0.8885856627371332\n",
      "\n",
      "loss:\n",
      "0.4441536254183137\n",
      "mse:\n",
      "0.8883072508366274\n",
      "\n",
      "loss:\n",
      "0.44402454754732307\n",
      "mse:\n",
      "0.8880490950946461\n",
      "\n",
      "loss:\n",
      "0.4438305855598803\n",
      "mse:\n",
      "0.8876611711197606\n",
      "\n",
      "loss:\n",
      "0.4436545881025638\n",
      "mse:\n",
      "0.8873091762051276\n",
      "\n",
      "loss:\n",
      "0.4434314382021636\n",
      "mse:\n",
      "0.8868628764043271\n",
      "\n",
      "loss:\n",
      "0.4432160349376336\n",
      "mse:\n",
      "0.8864320698752672\n",
      "\n",
      "loss:\n",
      "0.442888578466061\n",
      "mse:\n",
      "0.885777156932122\n",
      "\n",
      "loss:\n",
      "0.44261414664582155\n",
      "mse:\n",
      "0.8852282932916431\n",
      "\n",
      "loss:\n",
      "0.4422305446783751\n",
      "mse:\n",
      "0.8844610893567502\n",
      "\n",
      "loss:\n",
      "0.44187847984734197\n",
      "mse:\n",
      "0.8837569596946839\n",
      "\n",
      "loss:\n",
      "0.4413539280296097\n",
      "mse:\n",
      "0.8827078560592194\n",
      "\n",
      "loss:\n",
      "0.44106075122414895\n",
      "mse:\n",
      "0.8821215024482979\n",
      "\n",
      "loss:\n",
      "0.44035537222822574\n",
      "mse:\n",
      "0.8807107444564515\n",
      "\n",
      "loss:\n",
      "0.44006887198349753\n",
      "mse:\n",
      "0.8801377439669951\n",
      "\n",
      "loss:\n",
      "0.43876663818641204\n",
      "mse:\n",
      "0.8775332763728241\n",
      "\n",
      "loss:\n",
      "0.4385766278313084\n",
      "mse:\n",
      "0.8771532556626168\n",
      "\n",
      "loss:\n",
      "0.4372423011728037\n",
      "mse:\n",
      "0.8744846023456074\n",
      "\n",
      "loss:\n",
      "0.43801967757154936\n",
      "mse:\n",
      "0.8760393551430987\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.43787686808985277\n",
      "mse:\n",
      "0.8757537361797055\n",
      "Annealing learning rate. New rate is 0.8857350000000002\n",
      "\n",
      "loss:\n",
      "0.4388477869439875\n",
      "mse:\n",
      "0.877695573887975\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.43645363647234625\n",
      "mse:\n",
      "0.8729072729446925\n",
      "\n",
      "loss:\n",
      "0.4353876187724158\n",
      "mse:\n",
      "0.8707752375448315\n",
      "\n",
      "loss:\n",
      "0.43608221372799016\n",
      "mse:\n",
      "0.8721644274559803\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.44305335547546154\n",
      "mse:\n",
      "0.8861067109509231\n",
      "INCREASE IN LOSS\n",
      "Decreasing learning rate. New rate is 0.4428675000000001\n",
      "\n",
      "loss:\n",
      "0.4152760860992228\n",
      "mse:\n",
      "0.8305521721984456\n",
      "\n",
      "loss:\n",
      "0.40899736954843496\n",
      "mse:\n",
      "0.8179947390968699\n",
      "\n",
      "loss:\n",
      "0.4024361944350953\n",
      "mse:\n",
      "0.8048723888701906\n",
      "\n",
      "loss:\n",
      "0.3949520326519882\n",
      "mse:\n",
      "0.7899040653039764\n",
      "\n",
      "loss:\n",
      "0.3859632770566422\n",
      "mse:\n",
      "0.7719265541132844\n",
      "\n",
      "loss:\n",
      "0.37512336822650844\n",
      "mse:\n",
      "0.7502467364530169\n",
      "\n",
      "loss:\n",
      "0.3623746217315554\n",
      "mse:\n",
      "0.7247492434631108\n",
      "\n",
      "loss:\n",
      "0.3472794636835157\n",
      "mse:\n",
      "0.6945589273670314\n",
      "\n",
      "loss:\n",
      "0.3322232429767119\n",
      "mse:\n",
      "0.6644464859534238\n",
      "\n",
      "loss:\n",
      "0.3184291186787569\n",
      "mse:\n",
      "0.6368582373575138\n",
      "\n",
      "loss:\n",
      "0.33264755539492313\n",
      "mse:\n",
      "0.6652951107898463\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.43239225985253715\n",
      "mse:\n",
      "0.8647845197050743\n",
      "INCREASE IN LOSS\n",
      "Decreasing learning rate. New rate is 0.22143375000000004\n",
      "\n",
      "loss:\n",
      "0.2973088348911794\n",
      "mse:\n",
      "0.5946176697823587\n",
      "\n",
      "loss:\n",
      "0.2719185481986578\n",
      "mse:\n",
      "0.5438370963973156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss:\n",
      "0.26304762958571143\n",
      "mse:\n",
      "0.5260952591714229\n",
      "\n",
      "loss:\n",
      "0.2590589705330074\n",
      "mse:\n",
      "0.5181179410660148\n",
      "\n",
      "loss:\n",
      "0.2559424358010722\n",
      "mse:\n",
      "0.5118848716021444\n",
      "\n",
      "loss:\n",
      "0.2546471763731104\n",
      "mse:\n",
      "0.5092943527462208\n",
      "\n",
      "loss:\n",
      "0.2529749967191195\n",
      "mse:\n",
      "0.505949993438239\n",
      "\n",
      "loss:\n",
      "0.25203365633979746\n",
      "mse:\n",
      "0.5040673126795949\n",
      "\n",
      "loss:\n",
      "0.2506417662526683\n",
      "mse:\n",
      "0.5012835325053366\n",
      "Annealing learning rate. New rate is 0.19929037500000005\n",
      "\n",
      "loss:\n",
      "0.24992215684467692\n",
      "mse:\n",
      "0.49984431368935384\n",
      "\n",
      "loss:\n",
      "0.24880110816632386\n",
      "mse:\n",
      "0.49760221633264773\n",
      "\n",
      "loss:\n",
      "0.24810345611728055\n",
      "mse:\n",
      "0.4962069122345611\n",
      "\n",
      "loss:\n",
      "0.24706570510960266\n",
      "mse:\n",
      "0.4941314102192053\n",
      "\n",
      "loss:\n",
      "0.2463978857858518\n",
      "mse:\n",
      "0.4927957715717036\n",
      "\n",
      "loss:\n",
      "0.24542409079436894\n",
      "mse:\n",
      "0.4908481815887379\n",
      "\n",
      "loss:\n",
      "0.24472966942130944\n",
      "mse:\n",
      "0.4894593388426189\n",
      "\n",
      "loss:\n",
      "0.24384637989416635\n",
      "mse:\n",
      "0.4876927597883327\n",
      "\n",
      "loss:\n",
      "0.24326414249145364\n",
      "mse:\n",
      "0.4865282849829073\n",
      "\n",
      "loss:\n",
      "0.24242629334015467\n",
      "mse:\n",
      "0.48485258668030934\n",
      "\n",
      "loss:\n",
      "0.24198374506816217\n",
      "mse:\n",
      "0.48396749013632434\n",
      "\n",
      "loss:\n",
      "0.2411741920724754\n",
      "mse:\n",
      "0.4823483841449508\n",
      "\n",
      "loss:\n",
      "0.24088142469582371\n",
      "mse:\n",
      "0.48176284939164743\n",
      "\n",
      "loss:\n",
      "0.2399737239680539\n",
      "mse:\n",
      "0.4799474479361078\n",
      "\n",
      "loss:\n",
      "0.2399503346839395\n",
      "mse:\n",
      "0.479900669367879\n",
      "\n",
      "loss:\n",
      "0.2388098371328443\n",
      "mse:\n",
      "0.4776196742656886\n",
      "\n",
      "loss:\n",
      "0.2397222652375363\n",
      "mse:\n",
      "0.4794445304750726\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.23792575229861548\n",
      "mse:\n",
      "0.47585150459723097\n",
      "\n",
      "loss:\n",
      "0.24173370977943895\n",
      "mse:\n",
      "0.4834674195588779\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.24049813439899745\n",
      "mse:\n",
      "0.4809962687979949\n",
      "\n",
      "loss:\n",
      "0.2539851783810428\n",
      "mse:\n",
      "0.5079703567620856\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.2668369502509232\n",
      "mse:\n",
      "0.5336739005018464\n",
      "INCREASE IN LOSS\n",
      "Decreasing learning rate. New rate is 0.09964518750000002\n",
      "\n",
      "loss:\n",
      "0.23635540989679302\n",
      "mse:\n",
      "0.47271081979358603\n",
      "\n",
      "loss:\n",
      "0.2366138100542471\n",
      "mse:\n",
      "0.4732276201084942\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.23596893861318885\n",
      "mse:\n",
      "0.4719378772263777\n",
      "\n",
      "loss:\n",
      "0.23578581901611387\n",
      "mse:\n",
      "0.47157163803222774\n",
      "Annealing learning rate. New rate is 0.08968066875000003\n",
      "\n",
      "loss:\n",
      "0.23547507270631987\n",
      "mse:\n",
      "0.47095014541263974\n",
      "\n",
      "loss:\n",
      "0.23526082235972\n",
      "mse:\n",
      "0.47052164471944\n",
      "\n",
      "loss:\n",
      "0.2350052612424075\n",
      "mse:\n",
      "0.470010522484815\n",
      "\n",
      "loss:\n",
      "0.23479105249595159\n",
      "mse:\n",
      "0.46958210499190317\n",
      "\n",
      "loss:\n",
      "0.23454785845561374\n",
      "mse:\n",
      "0.4690957169112275\n",
      "\n",
      "loss:\n",
      "0.23435889981752775\n",
      "mse:\n",
      "0.4687177996350555\n",
      "\n",
      "loss:\n",
      "0.2341209718462539\n",
      "mse:\n",
      "0.4682419436925078\n",
      "\n",
      "loss:\n",
      "0.23395776107204835\n",
      "mse:\n",
      "0.4679155221440967\n",
      "\n",
      "loss:\n",
      "0.23372426050747416\n",
      "mse:\n",
      "0.4674485210149483\n",
      "\n",
      "loss:\n",
      "0.233565626660121\n",
      "mse:\n",
      "0.467131253320242\n",
      "\n",
      "loss:\n",
      "0.23333638049513958\n",
      "mse:\n",
      "0.46667276099027916\n",
      "\n",
      "loss:\n",
      "0.23317567134064077\n",
      "mse:\n",
      "0.46635134268128153\n",
      "\n",
      "loss:\n",
      "0.2329569235418388\n",
      "mse:\n",
      "0.4659138470836776\n",
      "\n",
      "loss:\n",
      "0.23278593985615328\n",
      "mse:\n",
      "0.46557187971230657\n",
      "\n",
      "loss:\n",
      "0.23257344589492984\n",
      "mse:\n",
      "0.4651468917898597\n",
      "\n",
      "loss:\n",
      "0.23239297289636313\n",
      "mse:\n",
      "0.46478594579272625\n",
      "\n",
      "loss:\n",
      "0.23218760896899004\n",
      "mse:\n",
      "0.4643752179379801\n",
      "\n",
      "loss:\n",
      "0.2320032051602311\n",
      "mse:\n",
      "0.4640064103204622\n",
      "\n",
      "loss:\n",
      "0.23180508229005725\n",
      "mse:\n",
      "0.4636101645801145\n",
      "\n",
      "loss:\n",
      "0.23162358516952472\n",
      "mse:\n",
      "0.46324717033904944\n",
      "\n",
      "loss:\n",
      "0.23142970486159714\n",
      "mse:\n",
      "0.4628594097231943\n",
      "\n",
      "loss:\n",
      "0.23125544224728853\n",
      "mse:\n",
      "0.46251088449457706\n",
      "\n",
      "loss:\n",
      "0.23105777029869562\n",
      "mse:\n",
      "0.46211554059739124\n",
      "\n",
      "loss:\n",
      "0.2308850186656488\n",
      "mse:\n",
      "0.4617700373312976\n",
      "\n",
      "loss:\n",
      "0.23069057179338925\n",
      "mse:\n",
      "0.4613811435867785\n",
      "\n",
      "loss:\n",
      "0.2305119024299073\n",
      "mse:\n",
      "0.4610238048598146\n",
      "Annealing learning rate. New rate is 0.08071260187500003\n",
      "\n",
      "loss:\n",
      "0.23033725967072805\n",
      "mse:\n",
      "0.4606745193414561\n",
      "\n",
      "loss:\n",
      "0.23017577114006424\n",
      "mse:\n",
      "0.4603515422801285\n",
      "\n",
      "loss:\n",
      "0.2300024912684964\n",
      "mse:\n",
      "0.4600049825369928\n",
      "\n",
      "loss:\n",
      "0.22983420308092303\n",
      "mse:\n",
      "0.45966840616184607\n",
      "\n",
      "loss:\n",
      "0.22965713175033023\n",
      "mse:\n",
      "0.45931426350066046\n",
      "\n",
      "loss:\n",
      "0.229487212948299\n",
      "mse:\n",
      "0.458974425896598\n",
      "\n",
      "loss:\n",
      "0.2293070662713037\n",
      "mse:\n",
      "0.4586141325426074\n",
      "\n",
      "loss:\n",
      "0.22913102423973736\n",
      "mse:\n",
      "0.4582620484794747\n",
      "\n",
      "loss:\n",
      "0.2289484559282861\n",
      "mse:\n",
      "0.4578969118565722\n",
      "\n",
      "loss:\n",
      "0.22877060180456713\n",
      "mse:\n",
      "0.45754120360913425\n",
      "\n",
      "loss:\n",
      "0.22858715647759126\n",
      "mse:\n",
      "0.4571743129551825\n",
      "\n",
      "loss:\n",
      "0.2284040020281013\n",
      "mse:\n",
      "0.4568080040562026\n",
      "\n",
      "loss:\n",
      "0.22822234709451875\n",
      "mse:\n",
      "0.4564446941890375\n",
      "\n",
      "loss:\n",
      "0.22803942110235847\n",
      "mse:\n",
      "0.45607884220471695\n",
      "\n",
      "loss:\n",
      "0.22785902651340545\n",
      "mse:\n",
      "0.4557180530268109\n",
      "\n",
      "loss:\n",
      "0.22767363108037453\n",
      "mse:\n",
      "0.45534726216074906\n",
      "\n",
      "loss:\n",
      "0.22749241661154712\n",
      "mse:\n",
      "0.45498483322309424\n",
      "\n",
      "loss:\n",
      "0.22731123689684266\n",
      "mse:\n",
      "0.4546224737936853\n",
      "\n",
      "loss:\n",
      "0.2271261453836861\n",
      "mse:\n",
      "0.4542522907673722\n",
      "\n",
      "loss:\n",
      "0.22694377697930254\n",
      "mse:\n",
      "0.4538875539586051\n",
      "\n",
      "loss:\n",
      "0.22675997927904293\n",
      "mse:\n",
      "0.45351995855808586\n",
      "\n",
      "loss:\n",
      "0.22657504016830723\n",
      "mse:\n",
      "0.45315008033661447\n",
      "\n",
      "loss:\n",
      "0.22639205775497723\n",
      "mse:\n",
      "0.45278411550995445\n",
      "\n",
      "loss:\n",
      "0.22620914014614418\n",
      "mse:\n",
      "0.45241828029228837\n",
      "\n",
      "loss:\n",
      "0.22602486818830375\n",
      "mse:\n",
      "0.4520497363766075\n",
      "\n",
      "loss:\n",
      "0.22583938509999793\n",
      "mse:\n",
      "0.45167877019999586\n",
      "Annealing learning rate. New rate is 0.07264134168750003\n",
      "\n",
      "loss:\n",
      "0.22567248638343196\n",
      "mse:\n",
      "0.4513449727668639\n",
      "\n",
      "loss:\n",
      "0.22550257360635018\n",
      "mse:\n",
      "0.45100514721270035\n",
      "\n",
      "loss:\n",
      "0.2253356634468931\n",
      "mse:\n",
      "0.4506713268937862\n",
      "\n",
      "loss:\n",
      "0.22516477829170167\n",
      "mse:\n",
      "0.45032955658340335\n",
      "\n",
      "loss:\n",
      "0.22499754660495908\n",
      "mse:\n",
      "0.44999509320991815\n",
      "\n",
      "loss:\n",
      "0.22482922571210825\n",
      "mse:\n",
      "0.4496584514242165\n",
      "\n",
      "loss:\n",
      "0.22466064653054318\n",
      "mse:\n",
      "0.44932129306108637\n",
      "\n",
      "loss:\n",
      "0.22449519815232083\n",
      "mse:\n",
      "0.44899039630464166\n",
      "\n",
      "loss:\n",
      "0.224328108823807\n",
      "mse:\n",
      "0.448656217647614\n",
      "\n",
      "loss:\n",
      "0.22416109758045505\n",
      "mse:\n",
      "0.4483221951609101\n",
      "\n",
      "loss:\n",
      "0.2239932377654187\n",
      "mse:\n",
      "0.4479864755308374\n",
      "\n",
      "loss:\n",
      "0.22382348872178776\n",
      "mse:\n",
      "0.4476469774435755\n",
      "\n",
      "loss:\n",
      "0.22365291226766856\n",
      "mse:\n",
      "0.4473058245353371\n",
      "\n",
      "loss:\n",
      "0.22347988737539223\n",
      "mse:\n",
      "0.44695977475078447\n",
      "\n",
      "loss:\n",
      "0.22330493746244093\n",
      "mse:\n",
      "0.44660987492488186\n",
      "\n",
      "loss:\n",
      "0.22313486746238728\n",
      "mse:\n",
      "0.44626973492477456\n",
      "\n",
      "loss:\n",
      "0.22296061602239384\n",
      "mse:\n",
      "0.4459212320447877\n",
      "\n",
      "loss:\n",
      "0.22279321765011545\n",
      "mse:\n",
      "0.4455864353002309\n",
      "\n",
      "loss:\n",
      "0.22261680824142838\n",
      "mse:\n",
      "0.44523361648285675\n",
      "\n",
      "loss:\n",
      "0.22244616632096625\n",
      "mse:\n",
      "0.4448923326419325\n",
      "\n",
      "loss:\n",
      "0.22226885828428627\n",
      "mse:\n",
      "0.44453771656857255\n",
      "\n",
      "loss:\n",
      "0.22209517994074374\n",
      "mse:\n",
      "0.4441903598814875\n",
      "\n",
      "loss:\n",
      "0.2219177957841094\n",
      "mse:\n",
      "0.4438355915682188\n",
      "\n",
      "loss:\n",
      "0.2217457606938809\n",
      "mse:\n",
      "0.4434915213877618\n",
      "\n",
      "loss:\n",
      "0.22157108801940256\n",
      "mse:\n",
      "0.4431421760388051\n",
      "\n",
      "loss:\n",
      "0.22139834823990817\n",
      "mse:\n",
      "0.44279669647981634\n",
      "Annealing learning rate. New rate is 0.06537720751875002\n",
      "\n",
      "loss:\n",
      "0.22124129449734908\n",
      "mse:\n",
      "0.44248258899469817\n",
      "\n",
      "loss:\n",
      "0.22108551252286662\n",
      "mse:\n",
      "0.44217102504573325\n",
      "\n",
      "loss:\n",
      "0.22092652867595444\n",
      "mse:\n",
      "0.4418530573519089\n",
      "\n",
      "loss:\n",
      "0.22076998406228204\n",
      "mse:\n",
      "0.4415399681245641\n",
      "\n",
      "loss:\n",
      "0.22061437769825457\n",
      "mse:\n",
      "0.44122875539650913\n",
      "\n",
      "loss:\n",
      "0.22045704548024836\n",
      "mse:\n",
      "0.4409140909604967\n",
      "\n",
      "loss:\n",
      "0.22030160707248886\n",
      "mse:\n",
      "0.4406032141449777\n",
      "\n",
      "loss:\n",
      "0.22014335090120077\n",
      "mse:\n",
      "0.44028670180240154\n",
      "\n",
      "loss:\n",
      "0.21998825141628092\n",
      "mse:\n",
      "0.43997650283256184\n",
      "\n",
      "loss:\n",
      "0.21983423834965102\n",
      "mse:\n",
      "0.43966847669930204\n",
      "\n",
      "loss:\n",
      "0.21968127212871785\n",
      "mse:\n",
      "0.4393625442574357\n",
      "\n",
      "loss:\n",
      "0.2195239810809486\n",
      "mse:\n",
      "0.4390479621618972\n",
      "\n",
      "loss:\n",
      "0.2193708869544399\n",
      "mse:\n",
      "0.4387417739088798\n",
      "\n",
      "loss:\n",
      "0.2192155549942292\n",
      "mse:\n",
      "0.4384311099884584\n",
      "\n",
      "loss:\n",
      "0.21906172653585146\n",
      "mse:\n",
      "0.4381234530717029\n",
      "\n",
      "loss:\n",
      "0.21890773537367217\n",
      "mse:\n",
      "0.43781547074734434\n",
      "\n",
      "loss:\n",
      "0.21875205067241713\n",
      "mse:\n",
      "0.43750410134483425\n",
      "\n",
      "loss:\n",
      "0.21859809697239302\n",
      "mse:\n",
      "0.43719619394478604\n",
      "\n",
      "loss:\n",
      "0.21844339551148678\n",
      "mse:\n",
      "0.43688679102297356\n",
      "\n",
      "loss:\n",
      "0.2182918823503516\n",
      "mse:\n",
      "0.4365837647007032\n",
      "\n",
      "loss:\n",
      "0.21813855234015633\n",
      "mse:\n",
      "0.43627710468031267\n",
      "\n",
      "loss:\n",
      "0.21798458334595736\n",
      "mse:\n",
      "0.4359691666919147\n",
      "\n",
      "loss:\n",
      "0.2178346044753484\n",
      "mse:\n",
      "0.4356692089506968\n",
      "\n",
      "loss:\n",
      "0.2176840450690624\n",
      "mse:\n",
      "0.4353680901381248\n",
      "\n",
      "loss:\n",
      "0.21753212137172842\n",
      "mse:\n",
      "0.43506424274345684\n",
      "\n",
      "loss:\n",
      "0.2173826417263882\n",
      "mse:\n",
      "0.4347652834527764\n",
      "Annealing learning rate. New rate is 0.05883948676687502\n",
      "\n",
      "loss:\n",
      "0.21724579162036747\n",
      "mse:\n",
      "0.43449158324073495\n",
      "\n",
      "loss:\n",
      "0.2171112385849948\n",
      "mse:\n",
      "0.4342224771699896\n",
      "\n",
      "loss:\n",
      "0.21697708268225663\n",
      "mse:\n",
      "0.43395416536451326\n",
      "\n",
      "loss:\n",
      "0.21684175873003625\n",
      "mse:\n",
      "0.4336835174600725\n",
      "\n",
      "loss:\n",
      "0.21670727033641407\n",
      "mse:\n",
      "0.43341454067282814\n",
      "\n",
      "loss:\n",
      "0.216571190331099\n",
      "mse:\n",
      "0.433142380662198\n",
      "\n",
      "loss:\n",
      "0.21643555056308336\n",
      "mse:\n",
      "0.4328711011261667\n",
      "\n",
      "loss:\n",
      "0.21630161460907482\n",
      "mse:\n",
      "0.43260322921814964\n",
      "\n",
      "loss:\n",
      "0.21616805662952449\n",
      "mse:\n",
      "0.43233611325904897\n",
      "\n",
      "loss:\n",
      "0.21603560663161836\n",
      "mse:\n",
      "0.4320712132632367\n",
      "\n",
      "loss:\n",
      "0.21590280003020043\n",
      "mse:\n",
      "0.43180560006040086\n",
      "\n",
      "loss:\n",
      "0.21576864300093182\n",
      "mse:\n",
      "0.43153728600186364\n",
      "\n",
      "loss:\n",
      "0.21563697170477453\n",
      "mse:\n",
      "0.43127394340954905\n",
      "\n",
      "loss:\n",
      "0.21550248508972106\n",
      "mse:\n",
      "0.4310049701794421\n",
      "\n",
      "loss:\n",
      "0.21536896499122116\n",
      "mse:\n",
      "0.4307379299824423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss:\n",
      "0.21523492801481522\n",
      "mse:\n",
      "0.43046985602963045\n",
      "\n",
      "loss:\n",
      "0.21509910291349923\n",
      "mse:\n",
      "0.43019820582699847\n",
      "\n",
      "loss:\n",
      "0.21496666798115488\n",
      "mse:\n",
      "0.42993333596230976\n",
      "\n",
      "loss:\n",
      "0.21483334284769054\n",
      "mse:\n",
      "0.4296666856953811\n",
      "\n",
      "loss:\n",
      "0.21469925873805637\n",
      "mse:\n",
      "0.42939851747611274\n",
      "\n",
      "loss:\n",
      "0.21456920241606275\n",
      "mse:\n",
      "0.4291384048321255\n",
      "\n",
      "loss:\n",
      "0.21443799435090352\n",
      "mse:\n",
      "0.42887598870180704\n",
      "\n",
      "loss:\n",
      "0.21430942870801298\n",
      "mse:\n",
      "0.42861885741602596\n",
      "\n",
      "loss:\n",
      "0.21417969063441258\n",
      "mse:\n",
      "0.42835938126882517\n",
      "\n",
      "loss:\n",
      "0.2140505482642916\n",
      "mse:\n",
      "0.4281010965285832\n",
      "\n",
      "loss:\n",
      "0.21391984524969562\n",
      "mse:\n",
      "0.42783969049939125\n",
      "Annealing learning rate. New rate is 0.05295553809018752\n",
      "\n",
      "loss:\n",
      "0.21380442738141425\n",
      "mse:\n",
      "0.4276088547628285\n",
      "\n",
      "loss:\n",
      "0.2136909582376938\n",
      "mse:\n",
      "0.4273819164753876\n",
      "\n",
      "loss:\n",
      "0.21357508733136316\n",
      "mse:\n",
      "0.4271501746627263\n",
      "\n",
      "loss:\n",
      "0.21346425212228515\n",
      "mse:\n",
      "0.4269285042445703\n",
      "\n",
      "loss:\n",
      "0.21335064589688152\n",
      "mse:\n",
      "0.42670129179376304\n",
      "\n",
      "loss:\n",
      "0.21323915613908864\n",
      "mse:\n",
      "0.4264783122781773\n",
      "\n",
      "loss:\n",
      "0.2131266102286886\n",
      "mse:\n",
      "0.4262532204573772\n",
      "\n",
      "loss:\n",
      "0.2130141700066878\n",
      "mse:\n",
      "0.4260283400133756\n",
      "\n",
      "loss:\n",
      "0.21290294761019235\n",
      "mse:\n",
      "0.4258058952203847\n",
      "\n",
      "loss:\n",
      "0.21279023261087077\n",
      "mse:\n",
      "0.42558046522174153\n",
      "\n",
      "loss:\n",
      "0.21267759855340657\n",
      "mse:\n",
      "0.42535519710681313\n",
      "\n",
      "loss:\n",
      "0.21256680859228783\n",
      "mse:\n",
      "0.42513361718457565\n",
      "\n",
      "loss:\n",
      "0.21245593985296712\n",
      "mse:\n",
      "0.42491187970593425\n",
      "\n",
      "loss:\n",
      "0.21234583115553865\n",
      "mse:\n",
      "0.4246916623110773\n",
      "\n",
      "loss:\n",
      "0.2122328843070021\n",
      "mse:\n",
      "0.4244657686140042\n",
      "\n",
      "loss:\n",
      "0.21212324145139713\n",
      "mse:\n",
      "0.42424648290279426\n",
      "\n",
      "loss:\n",
      "0.21201330282710937\n",
      "mse:\n",
      "0.42402660565421874\n",
      "\n",
      "loss:\n",
      "0.21190342407773524\n",
      "mse:\n",
      "0.4238068481554705\n",
      "\n",
      "loss:\n",
      "0.2117915730265794\n",
      "mse:\n",
      "0.4235831460531588\n",
      "\n",
      "loss:\n",
      "0.21168188750997258\n",
      "mse:\n",
      "0.42336377501994515\n",
      "\n",
      "loss:\n",
      "0.21157122834119013\n",
      "mse:\n",
      "0.42314245668238026\n",
      "\n",
      "loss:\n",
      "0.2114627851348128\n",
      "mse:\n",
      "0.4229255702696256\n",
      "\n",
      "loss:\n",
      "0.21135600590082257\n",
      "mse:\n",
      "0.42271201180164514\n",
      "\n",
      "loss:\n",
      "0.21124780512157806\n",
      "mse:\n",
      "0.42249561024315613\n",
      "\n",
      "loss:\n",
      "0.2111392409074026\n",
      "mse:\n",
      "0.4222784818148052\n",
      "\n",
      "loss:\n",
      "0.21102986955534567\n",
      "mse:\n",
      "0.42205973911069133\n",
      "Annealing learning rate. New rate is 0.04765998428116877\n",
      "\n",
      "loss:\n",
      "0.2109311763080726\n",
      "mse:\n",
      "0.4218623526161452\n",
      "\n",
      "loss:\n",
      "0.21083548298588475\n",
      "mse:\n",
      "0.4216709659717695\n",
      "\n",
      "loss:\n",
      "0.21073778522140144\n",
      "mse:\n",
      "0.42147557044280287\n",
      "\n",
      "loss:\n",
      "0.21064145179427335\n",
      "mse:\n",
      "0.4212829035885467\n",
      "\n",
      "loss:\n",
      "0.21054452966573617\n",
      "mse:\n",
      "0.42108905933147234\n",
      "\n",
      "loss:\n",
      "0.2104491004624684\n",
      "mse:\n",
      "0.4208982009249368\n",
      "\n",
      "loss:\n",
      "0.21035274164405918\n",
      "mse:\n",
      "0.42070548328811835\n",
      "\n",
      "loss:\n",
      "0.21025676895920256\n",
      "mse:\n",
      "0.4205135379184051\n",
      "\n",
      "loss:\n",
      "0.2101616015771275\n",
      "mse:\n",
      "0.420323203154255\n",
      "\n",
      "loss:\n",
      "0.21006606711832063\n",
      "mse:\n",
      "0.42013213423664125\n",
      "\n",
      "loss:\n",
      "0.2099706284859499\n",
      "mse:\n",
      "0.4199412569718998\n",
      "\n",
      "loss:\n",
      "0.2098758173455319\n",
      "mse:\n",
      "0.4197516346910638\n",
      "\n",
      "loss:\n",
      "0.20978018517282973\n",
      "mse:\n",
      "0.41956037034565946\n",
      "\n",
      "loss:\n",
      "0.2096864348485396\n",
      "mse:\n",
      "0.4193728696970792\n",
      "\n",
      "loss:\n",
      "0.20959163144240603\n",
      "mse:\n",
      "0.41918326288481206\n",
      "\n",
      "loss:\n",
      "0.209497704876573\n",
      "mse:\n",
      "0.418995409753146\n",
      "\n",
      "loss:\n",
      "0.20940530054309422\n",
      "mse:\n",
      "0.41881060108618845\n",
      "\n",
      "loss:\n",
      "0.2093151573468507\n",
      "mse:\n",
      "0.4186303146937014\n",
      "\n",
      "loss:\n",
      "0.20922397393156453\n",
      "mse:\n",
      "0.41844794786312905\n",
      "\n",
      "loss:\n",
      "0.20913236081329567\n",
      "mse:\n",
      "0.41826472162659134\n",
      "\n",
      "loss:\n",
      "0.20904220364373646\n",
      "mse:\n",
      "0.4180844072874729\n",
      "\n",
      "loss:\n",
      "0.2089515535004299\n",
      "mse:\n",
      "0.4179031070008598\n",
      "\n",
      "loss:\n",
      "0.20886160540245652\n",
      "mse:\n",
      "0.41772321080491304\n",
      "\n",
      "loss:\n",
      "0.2087709012083213\n",
      "mse:\n",
      "0.4175418024166426\n",
      "\n",
      "loss:\n",
      "0.20867876878795022\n",
      "mse:\n",
      "0.41735753757590044\n",
      "\n",
      "loss:\n",
      "0.20859092005702023\n",
      "mse:\n",
      "0.41718184011404047\n",
      "Annealing learning rate. New rate is 0.042893985853051896\n",
      "\n",
      "loss:\n",
      "0.2085095877425648\n",
      "mse:\n",
      "0.4170191754851296\n",
      "\n",
      "loss:\n",
      "0.20842986882685358\n",
      "mse:\n",
      "0.41685973765370715\n",
      "\n",
      "loss:\n",
      "0.2083491684224351\n",
      "mse:\n",
      "0.4166983368448702\n",
      "\n",
      "loss:\n",
      "0.20826995940367926\n",
      "mse:\n",
      "0.4165399188073585\n",
      "\n",
      "loss:\n",
      "0.2081900634612645\n",
      "mse:\n",
      "0.416380126922529\n",
      "\n",
      "loss:\n",
      "0.20810976547151447\n",
      "mse:\n",
      "0.41621953094302894\n",
      "\n",
      "loss:\n",
      "0.20803086372130833\n",
      "mse:\n",
      "0.41606172744261666\n",
      "\n",
      "loss:\n",
      "0.20795287095250872\n",
      "mse:\n",
      "0.41590574190501745\n",
      "\n",
      "loss:\n",
      "0.207876031942736\n",
      "mse:\n",
      "0.415752063885472\n",
      "\n",
      "loss:\n",
      "0.2077970458892326\n",
      "mse:\n",
      "0.4155940917784652\n",
      "\n",
      "loss:\n",
      "0.2077207158024465\n",
      "mse:\n",
      "0.415441431604893\n",
      "\n",
      "loss:\n",
      "0.20764175804123156\n",
      "mse:\n",
      "0.4152835160824631\n",
      "\n",
      "loss:\n",
      "0.20756555231825222\n",
      "mse:\n",
      "0.41513110463650443\n",
      "\n",
      "loss:\n",
      "0.20748834999648003\n",
      "mse:\n",
      "0.41497669999296005\n",
      "\n",
      "loss:\n",
      "0.20741207144584572\n",
      "mse:\n",
      "0.41482414289169145\n",
      "\n",
      "loss:\n",
      "0.2073354037149264\n",
      "mse:\n",
      "0.4146708074298528\n",
      "\n",
      "loss:\n",
      "0.2072589281616834\n",
      "mse:\n",
      "0.4145178563233668\n",
      "\n",
      "loss:\n",
      "0.20718116449435722\n",
      "mse:\n",
      "0.41436232898871445\n",
      "\n",
      "loss:\n",
      "0.20710337348170024\n",
      "mse:\n",
      "0.4142067469634005\n",
      "\n",
      "loss:\n",
      "0.20702701866098172\n",
      "mse:\n",
      "0.41405403732196344\n",
      "\n",
      "loss:\n",
      "0.2069503306317021\n",
      "mse:\n",
      "0.4139006612634042\n",
      "\n",
      "loss:\n",
      "0.2068743007865567\n",
      "mse:\n",
      "0.4137486015731134\n",
      "\n",
      "loss:\n",
      "0.20679880337167308\n",
      "mse:\n",
      "0.41359760674334617\n",
      "\n",
      "loss:\n",
      "0.20672393757096363\n",
      "mse:\n",
      "0.41344787514192727\n",
      "\n",
      "loss:\n",
      "0.2066482587047643\n",
      "mse:\n",
      "0.4132965174095286\n",
      "\n",
      "loss:\n",
      "0.2065712016389261\n",
      "mse:\n",
      "0.4131424032778522\n",
      "Annealing learning rate. New rate is 0.03860458726774671\n",
      "\n",
      "loss:\n",
      "0.20650318644818258\n",
      "mse:\n",
      "0.41300637289636516\n",
      "\n",
      "loss:\n",
      "0.20643452789512595\n",
      "mse:\n",
      "0.4128690557902519\n",
      "\n",
      "loss:\n",
      "0.20636670047085232\n",
      "mse:\n",
      "0.41273340094170463\n",
      "\n",
      "loss:\n",
      "0.20629784514936644\n",
      "mse:\n",
      "0.4125956902987329\n",
      "\n",
      "loss:\n",
      "0.20622856818059257\n",
      "mse:\n",
      "0.41245713636118514\n",
      "\n",
      "loss:\n",
      "0.20615952515393088\n",
      "mse:\n",
      "0.41231905030786176\n",
      "\n",
      "loss:\n",
      "0.20609022087727563\n",
      "mse:\n",
      "0.41218044175455126\n",
      "\n",
      "loss:\n",
      "0.20602146692697626\n",
      "mse:\n",
      "0.4120429338539525\n",
      "\n",
      "loss:\n",
      "0.2059522841749516\n",
      "mse:\n",
      "0.4119045683499032\n",
      "\n",
      "loss:\n",
      "0.20588331200074445\n",
      "mse:\n",
      "0.4117666240014889\n",
      "\n",
      "loss:\n",
      "0.20581503548783628\n",
      "mse:\n",
      "0.41163007097567256\n",
      "\n",
      "loss:\n",
      "0.20574826832542656\n",
      "mse:\n",
      "0.4114965366508531\n",
      "\n",
      "loss:\n",
      "0.2056817862917376\n",
      "mse:\n",
      "0.4113635725834752\n",
      "\n",
      "loss:\n",
      "0.2056147440833525\n",
      "mse:\n",
      "0.411229488166705\n",
      "\n",
      "loss:\n",
      "0.2055469956795288\n",
      "mse:\n",
      "0.4110939913590576\n",
      "\n",
      "loss:\n",
      "0.20548008535285786\n",
      "mse:\n",
      "0.4109601707057157\n",
      "\n",
      "loss:\n",
      "0.20541234582080764\n",
      "mse:\n",
      "0.4108246916416153\n",
      "\n",
      "loss:\n",
      "0.2053440623911127\n",
      "mse:\n",
      "0.4106881247822254\n",
      "\n",
      "loss:\n",
      "0.20527771973745523\n",
      "mse:\n",
      "0.41055543947491047\n",
      "\n",
      "loss:\n",
      "0.20521008355568665\n",
      "mse:\n",
      "0.4104201671113733\n",
      "\n",
      "loss:\n",
      "0.20514371148093763\n",
      "mse:\n",
      "0.41028742296187526\n",
      "\n",
      "loss:\n",
      "0.20507651258975573\n",
      "mse:\n",
      "0.41015302517951147\n",
      "\n",
      "loss:\n",
      "0.20500910228634134\n",
      "mse:\n",
      "0.4100182045726827\n",
      "\n",
      "loss:\n",
      "0.20494251509021566\n",
      "mse:\n",
      "0.4098850301804313\n",
      "\n",
      "loss:\n",
      "0.20487506702870542\n",
      "mse:\n",
      "0.40975013405741084\n",
      "\n",
      "loss:\n",
      "0.20480754778071422\n",
      "mse:\n",
      "0.40961509556142844\n",
      "Annealing learning rate. New rate is 0.03474412854097204\n",
      "\n",
      "loss:\n",
      "0.20474758627582754\n",
      "mse:\n",
      "0.4094951725516551\n",
      "\n",
      "loss:\n",
      "0.2046869759313347\n",
      "mse:\n",
      "0.4093739518626694\n",
      "\n",
      "loss:\n",
      "0.20462651559383374\n",
      "mse:\n",
      "0.4092530311876675\n",
      "\n",
      "loss:\n",
      "0.20456796955094697\n",
      "mse:\n",
      "0.40913593910189394\n",
      "\n",
      "loss:\n",
      "0.20450879786545867\n",
      "mse:\n",
      "0.40901759573091734\n",
      "\n",
      "loss:\n",
      "0.20444861616159188\n",
      "mse:\n",
      "0.40889723232318376\n",
      "\n",
      "loss:\n",
      "0.20439056774412193\n",
      "mse:\n",
      "0.40878113548824385\n",
      "\n",
      "loss:\n",
      "0.204333070437605\n",
      "mse:\n",
      "0.40866614087521\n",
      "\n",
      "loss:\n",
      "0.2042759428835308\n",
      "mse:\n",
      "0.4085518857670616\n",
      "\n",
      "loss:\n",
      "0.20421997314440984\n",
      "mse:\n",
      "0.4084399462888197\n",
      "\n",
      "loss:\n",
      "0.2041629881278058\n",
      "mse:\n",
      "0.4083259762556116\n",
      "\n",
      "loss:\n",
      "0.20410460093825428\n",
      "mse:\n",
      "0.40820920187650855\n",
      "\n",
      "loss:\n",
      "0.20404688815945393\n",
      "mse:\n",
      "0.40809377631890786\n",
      "\n",
      "loss:\n",
      "0.20398875760039067\n",
      "mse:\n",
      "0.40797751520078135\n",
      "\n",
      "loss:\n",
      "0.20392969175635017\n",
      "mse:\n",
      "0.40785938351270035\n",
      "\n",
      "loss:\n",
      "0.20387164532567925\n",
      "mse:\n",
      "0.4077432906513585\n",
      "\n",
      "loss:\n",
      "0.2038129363071577\n",
      "mse:\n",
      "0.4076258726143154\n",
      "\n",
      "loss:\n",
      "0.2037546821160415\n",
      "mse:\n",
      "0.407509364232083\n",
      "\n",
      "loss:\n",
      "0.20369711994679193\n",
      "mse:\n",
      "0.40739423989358386\n",
      "\n",
      "loss:\n",
      "0.20363838229536332\n",
      "mse:\n",
      "0.40727676459072665\n",
      "\n",
      "loss:\n",
      "0.20358146879445013\n",
      "mse:\n",
      "0.40716293758890026\n",
      "\n",
      "loss:\n",
      "0.2035238611147211\n",
      "mse:\n",
      "0.4070477222294422\n",
      "\n",
      "loss:\n",
      "0.2034657745869493\n",
      "mse:\n",
      "0.4069315491738986\n",
      "\n",
      "loss:\n",
      "0.20340899009597885\n",
      "mse:\n",
      "0.4068179801919577\n",
      "\n",
      "loss:\n",
      "0.20335085768383057\n",
      "mse:\n",
      "0.40670171536766114\n",
      "\n",
      "loss:\n",
      "0.2032923968023685\n",
      "mse:\n",
      "0.406584793604737\n",
      "Annealing learning rate. New rate is 0.03126971568687483\n",
      "\n",
      "loss:\n",
      "0.20324089026660808\n",
      "mse:\n",
      "0.40648178053321615\n",
      "\n",
      "loss:\n",
      "0.20318904249937186\n",
      "mse:\n",
      "0.4063780849987437\n",
      "\n",
      "loss:\n",
      "0.20313805098050652\n",
      "mse:\n",
      "0.40627610196101305\n",
      "\n",
      "loss:\n",
      "0.20308705046583136\n",
      "mse:\n",
      "0.40617410093166273\n",
      "\n",
      "loss:\n",
      "0.20303507811860688\n",
      "mse:\n",
      "0.40607015623721376\n",
      "\n",
      "loss:\n",
      "0.20298331904686137\n",
      "mse:\n",
      "0.40596663809372274\n",
      "\n",
      "loss:\n",
      "0.20293129504135188\n",
      "mse:\n",
      "0.40586259008270376\n",
      "\n",
      "loss:\n",
      "0.2028795512723503\n",
      "mse:\n",
      "0.4057591025447006\n",
      "\n",
      "loss:\n",
      "0.20282962639971894\n",
      "mse:\n",
      "0.4056592527994379\n",
      "\n",
      "loss:\n",
      "0.20277994925338239\n",
      "mse:\n",
      "0.40555989850676477\n",
      "\n",
      "loss:\n",
      "0.20272993043724788\n",
      "mse:\n",
      "0.40545986087449576\n",
      "\n",
      "loss:\n",
      "0.20268105770720288\n",
      "mse:\n",
      "0.40536211541440575\n",
      "\n",
      "loss:\n",
      "0.20263086836203792\n",
      "mse:\n",
      "0.40526173672407584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss:\n",
      "0.2025807280364383\n",
      "mse:\n",
      "0.4051614560728766\n",
      "\n",
      "loss:\n",
      "0.202531071863119\n",
      "mse:\n",
      "0.405062143726238\n",
      "\n",
      "loss:\n",
      "0.20247994390901117\n",
      "mse:\n",
      "0.40495988781802233\n",
      "\n",
      "loss:\n",
      "0.202428807596\n",
      "mse:\n",
      "0.404857615192\n",
      "\n",
      "loss:\n",
      "0.20237801556165108\n",
      "mse:\n",
      "0.40475603112330216\n",
      "\n",
      "loss:\n",
      "0.20232691534159628\n",
      "mse:\n",
      "0.40465383068319255\n",
      "\n",
      "loss:\n",
      "0.20227553707484916\n",
      "mse:\n",
      "0.4045510741496983\n",
      "\n",
      "loss:\n",
      "0.20222496675531243\n",
      "mse:\n",
      "0.40444993351062486\n",
      "\n",
      "loss:\n",
      "0.20217353535241167\n",
      "mse:\n",
      "0.40434707070482334\n",
      "\n",
      "loss:\n",
      "0.20212318454554626\n",
      "mse:\n",
      "0.4042463690910925\n",
      "\n",
      "loss:\n",
      "0.2020720371960275\n",
      "mse:\n",
      "0.404144074392055\n",
      "\n",
      "loss:\n",
      "0.20202009156502276\n",
      "mse:\n",
      "0.4040401831300455\n",
      "\n",
      "loss:\n",
      "0.20196898794505566\n",
      "mse:\n",
      "0.4039379758901113\n",
      "Annealing learning rate. New rate is 0.02814274411818735\n",
      "\n",
      "loss:\n",
      "0.20192256589291335\n",
      "mse:\n",
      "0.4038451317858267\n",
      "\n",
      "loss:\n",
      "0.2018750282775991\n",
      "mse:\n",
      "0.4037500565551982\n",
      "\n",
      "loss:\n",
      "0.20182797596395058\n",
      "mse:\n",
      "0.40365595192790116\n",
      "\n",
      "loss:\n",
      "0.20178047351677783\n",
      "mse:\n",
      "0.40356094703355566\n",
      "\n",
      "loss:\n",
      "0.20173241054283997\n",
      "mse:\n",
      "0.40346482108567994\n",
      "\n",
      "loss:\n",
      "0.20168537444390383\n",
      "mse:\n",
      "0.40337074888780766\n",
      "\n",
      "loss:\n",
      "0.20163817299985837\n",
      "mse:\n",
      "0.40327634599971673\n",
      "\n",
      "loss:\n",
      "0.2015907669427784\n",
      "mse:\n",
      "0.4031815338855568\n",
      "\n",
      "loss:\n",
      "0.20154437226623753\n",
      "mse:\n",
      "0.40308874453247506\n",
      "\n",
      "loss:\n",
      "0.20149709077794442\n",
      "mse:\n",
      "0.40299418155588884\n",
      "\n",
      "loss:\n",
      "0.20144961481603993\n",
      "mse:\n",
      "0.40289922963207986\n",
      "\n",
      "loss:\n",
      "0.20140248354869386\n",
      "mse:\n",
      "0.4028049670973877\n",
      "\n",
      "loss:\n",
      "0.20135540212592987\n",
      "mse:\n",
      "0.40271080425185973\n",
      "\n",
      "loss:\n",
      "0.20130821488744075\n",
      "mse:\n",
      "0.4026164297748815\n",
      "\n",
      "loss:\n",
      "0.2012617774185602\n",
      "mse:\n",
      "0.4025235548371204\n",
      "\n",
      "loss:\n",
      "0.2012148117263238\n",
      "mse:\n",
      "0.4024296234526476\n",
      "\n",
      "loss:\n",
      "0.2011673345031535\n",
      "mse:\n",
      "0.402334669006307\n",
      "\n",
      "loss:\n",
      "0.20112049226756032\n",
      "mse:\n",
      "0.40224098453512064\n",
      "\n",
      "loss:\n",
      "0.2010733099173025\n",
      "mse:\n",
      "0.402146619834605\n",
      "\n",
      "loss:\n",
      "0.20102588959699613\n",
      "mse:\n",
      "0.40205177919399226\n",
      "\n",
      "loss:\n",
      "0.20097848657357428\n",
      "mse:\n",
      "0.40195697314714857\n",
      "\n",
      "loss:\n",
      "0.2009326742254845\n",
      "mse:\n",
      "0.401865348450969\n",
      "\n",
      "loss:\n",
      "0.20088631556455838\n",
      "mse:\n",
      "0.40177263112911676\n",
      "\n",
      "loss:\n",
      "0.20083987273420661\n",
      "mse:\n",
      "0.40167974546841323\n",
      "\n",
      "loss:\n",
      "0.20079515593988154\n",
      "mse:\n",
      "0.4015903118797631\n",
      "\n",
      "loss:\n",
      "0.20074991131982214\n",
      "mse:\n",
      "0.4014998226396443\n",
      "Annealing learning rate. New rate is 0.025328469706368616\n",
      "\n",
      "loss:\n",
      "0.2007089716018027\n",
      "mse:\n",
      "0.4014179432036054\n",
      "\n",
      "loss:\n",
      "0.20066890432229112\n",
      "mse:\n",
      "0.40133780864458224\n",
      "\n",
      "loss:\n",
      "0.2006285382775688\n",
      "mse:\n",
      "0.4012570765551376\n",
      "\n",
      "loss:\n",
      "0.2005879041569942\n",
      "mse:\n",
      "0.4011758083139884\n",
      "\n",
      "loss:\n",
      "0.20054817722868465\n",
      "mse:\n",
      "0.4010963544573693\n",
      "\n",
      "loss:\n",
      "0.20050764939240903\n",
      "mse:\n",
      "0.40101529878481806\n",
      "\n",
      "loss:\n",
      "0.20046689733967593\n",
      "mse:\n",
      "0.40093379467935186\n",
      "\n",
      "loss:\n",
      "0.2004268720764814\n",
      "mse:\n",
      "0.4008537441529628\n",
      "\n",
      "loss:\n",
      "0.20038645971005561\n",
      "mse:\n",
      "0.40077291942011123\n",
      "\n",
      "loss:\n",
      "0.20034578028346492\n",
      "mse:\n",
      "0.40069156056692984\n",
      "\n",
      "loss:\n",
      "0.20030613877764805\n",
      "mse:\n",
      "0.4006122775552961\n",
      "\n",
      "loss:\n",
      "0.2002659819224722\n",
      "mse:\n",
      "0.4005319638449444\n",
      "\n",
      "loss:\n",
      "0.20022601090769565\n",
      "mse:\n",
      "0.4004520218153913\n",
      "\n",
      "loss:\n",
      "0.2001864968625197\n",
      "mse:\n",
      "0.4003729937250394\n",
      "\n",
      "loss:\n",
      "0.20014616238570054\n",
      "mse:\n",
      "0.4002923247714011\n",
      "\n",
      "loss:\n",
      "0.20010547518343128\n",
      "mse:\n",
      "0.40021095036686255\n",
      "\n",
      "loss:\n",
      "0.2000647071787723\n",
      "mse:\n",
      "0.4001294143575446\n",
      "\n",
      "loss:\n",
      "0.20002453821035893\n",
      "mse:\n",
      "0.40004907642071785\n",
      "\n",
      "loss:\n",
      "0.19998391560255138\n",
      "mse:\n",
      "0.39996783120510276\n",
      "\n",
      "loss:\n",
      "0.19994314732467644\n",
      "mse:\n",
      "0.3998862946493529\n",
      "\n",
      "loss:\n",
      "0.19990309376389367\n",
      "mse:\n",
      "0.39980618752778735\n",
      "\n",
      "loss:\n",
      "0.1998620315533568\n",
      "mse:\n",
      "0.3997240631067136\n",
      "\n",
      "loss:\n",
      "0.1998207419880632\n",
      "mse:\n",
      "0.3996414839761264\n",
      "\n",
      "loss:\n",
      "0.19978007909452644\n",
      "mse:\n",
      "0.3995601581890529\n",
      "\n",
      "loss:\n",
      "0.19973888548516083\n",
      "mse:\n",
      "0.39947777097032167\n",
      "\n",
      "loss:\n",
      "0.19969765173075055\n",
      "mse:\n",
      "0.3993953034615011\n",
      "Annealing learning rate. New rate is 0.022795622735731755\n",
      "\n",
      "loss:\n",
      "0.1996608600576503\n",
      "mse:\n",
      "0.3993217201153006\n",
      "\n",
      "loss:\n",
      "0.19962368281896944\n",
      "mse:\n",
      "0.3992473656379389\n",
      "\n",
      "loss:\n",
      "0.19958689919878145\n",
      "mse:\n",
      "0.3991737983975629\n",
      "\n",
      "loss:\n",
      "0.19955054257230487\n",
      "mse:\n",
      "0.39910108514460974\n",
      "\n",
      "loss:\n",
      "0.1995138489107832\n",
      "mse:\n",
      "0.3990276978215664\n",
      "\n",
      "loss:\n",
      "0.19947734187736235\n",
      "mse:\n",
      "0.3989546837547247\n",
      "\n",
      "loss:\n",
      "0.19944070198663078\n",
      "mse:\n",
      "0.39888140397326155\n",
      "\n",
      "loss:\n",
      "0.19940402357859977\n",
      "mse:\n",
      "0.39880804715719953\n",
      "\n",
      "loss:\n",
      "0.1993662992920803\n",
      "mse:\n",
      "0.3987325985841606\n",
      "\n",
      "loss:\n",
      "0.19932854382544551\n",
      "mse:\n",
      "0.39865708765089103\n",
      "\n",
      "loss:\n",
      "0.19929136323414234\n",
      "mse:\n",
      "0.3985827264682847\n",
      "\n",
      "loss:\n",
      "0.19925376614309956\n",
      "mse:\n",
      "0.3985075322861991\n",
      "\n",
      "loss:\n",
      "0.19921615819750302\n",
      "mse:\n",
      "0.39843231639500604\n",
      "\n",
      "loss:\n",
      "0.19917930968409828\n",
      "mse:\n",
      "0.39835861936819655\n",
      "\n",
      "loss:\n",
      "0.1991429373254453\n",
      "mse:\n",
      "0.3982858746508906\n",
      "\n",
      "loss:\n",
      "0.1991069170181169\n",
      "mse:\n",
      "0.3982138340362338\n",
      "\n",
      "loss:\n",
      "0.19907126016183585\n",
      "mse:\n",
      "0.3981425203236717\n",
      "\n",
      "loss:\n",
      "0.19903523052984418\n",
      "mse:\n",
      "0.39807046105968835\n",
      "\n",
      "loss:\n",
      "0.1989987400673037\n",
      "mse:\n",
      "0.3979974801346074\n",
      "\n",
      "loss:\n",
      "0.1989635117968883\n",
      "mse:\n",
      "0.3979270235937766\n",
      "\n",
      "loss:\n",
      "0.19892788602716466\n",
      "mse:\n",
      "0.3978557720543293\n",
      "\n",
      "loss:\n",
      "0.19889229725317112\n",
      "mse:\n",
      "0.39778459450634224\n",
      "\n",
      "loss:\n",
      "0.19885711883293436\n",
      "mse:\n",
      "0.3977142376658687\n",
      "\n",
      "loss:\n",
      "0.19882173797165614\n",
      "mse:\n",
      "0.3976434759433123\n",
      "\n",
      "loss:\n",
      "0.19878620425051138\n",
      "mse:\n",
      "0.39757240850102277\n",
      "\n",
      "loss:\n",
      "0.19875150000964228\n",
      "mse:\n",
      "0.39750300001928457\n",
      "Annealing learning rate. New rate is 0.02051606046215858\n",
      "\n",
      "loss:\n",
      "0.19871961573227961\n",
      "mse:\n",
      "0.39743923146455923\n",
      "\n",
      "loss:\n",
      "0.1986874576649739\n",
      "mse:\n",
      "0.3973749153299478\n",
      "\n",
      "loss:\n",
      "0.198655634445454\n",
      "mse:\n",
      "0.397311268890908\n",
      "\n",
      "loss:\n",
      "0.1986242582586441\n",
      "mse:\n",
      "0.3972485165172882\n",
      "\n",
      "loss:\n",
      "0.19859269457221052\n",
      "mse:\n",
      "0.39718538914442103\n",
      "\n",
      "loss:\n",
      "0.19856072038308217\n",
      "mse:\n",
      "0.39712144076616435\n",
      "\n",
      "loss:\n",
      "0.1985294420118065\n",
      "mse:\n",
      "0.397058884023613\n",
      "\n",
      "loss:\n",
      "0.1984978035204183\n",
      "mse:\n",
      "0.3969956070408366\n",
      "\n",
      "loss:\n",
      "0.19846592587893488\n",
      "mse:\n",
      "0.39693185175786977\n",
      "\n",
      "loss:\n",
      "0.19843444676359653\n",
      "mse:\n",
      "0.39686889352719307\n",
      "\n",
      "loss:\n",
      "0.1984024917505693\n",
      "mse:\n",
      "0.3968049835011386\n",
      "\n",
      "loss:\n",
      "0.19837045094212666\n",
      "mse:\n",
      "0.3967409018842533\n",
      "\n",
      "loss:\n",
      "0.1983381148280519\n",
      "mse:\n",
      "0.3966762296561038\n",
      "\n",
      "loss:\n",
      "0.19830631759773196\n",
      "mse:\n",
      "0.3966126351954639\n",
      "\n",
      "loss:\n",
      "0.19827427881672724\n",
      "mse:\n",
      "0.3965485576334545\n",
      "\n",
      "loss:\n",
      "0.19824234860912335\n",
      "mse:\n",
      "0.3964846972182467\n",
      "\n",
      "loss:\n",
      "0.19821068865411468\n",
      "mse:\n",
      "0.39642137730822935\n",
      "\n",
      "loss:\n",
      "0.198178651847193\n",
      "mse:\n",
      "0.396357303694386\n",
      "\n",
      "loss:\n",
      "0.19814665953092556\n",
      "mse:\n",
      "0.3962933190618511\n",
      "\n",
      "loss:\n",
      "0.19811493102448138\n",
      "mse:\n",
      "0.39622986204896277\n",
      "\n",
      "loss:\n",
      "0.19808256810634178\n",
      "mse:\n",
      "0.39616513621268357\n",
      "\n",
      "loss:\n",
      "0.19805014835585028\n",
      "mse:\n",
      "0.39610029671170055\n",
      "\n",
      "loss:\n",
      "0.19801865924278442\n",
      "mse:\n",
      "0.39603731848556883\n",
      "\n",
      "loss:\n",
      "0.19798674166969993\n",
      "mse:\n",
      "0.39597348333939986\n",
      "\n",
      "loss:\n",
      "0.1979549125458085\n",
      "mse:\n",
      "0.395909825091617\n",
      "\n",
      "loss:\n",
      "0.19792298683992365\n",
      "mse:\n",
      "0.3958459736798473\n",
      "Annealing learning rate. New rate is 0.018464454415942723\n",
      "\n",
      "loss:\n",
      "0.1978955519838596\n",
      "mse:\n",
      "0.3957911039677192\n",
      "\n",
      "loss:\n",
      "0.19786792756859126\n",
      "mse:\n",
      "0.3957358551371825\n",
      "\n",
      "loss:\n",
      "0.1978400384463082\n",
      "mse:\n",
      "0.3956800768926164\n",
      "\n",
      "loss:\n",
      "0.1978126797694053\n",
      "mse:\n",
      "0.3956253595388106\n",
      "\n",
      "loss:\n",
      "0.1977850716999954\n",
      "mse:\n",
      "0.3955701433999908\n",
      "\n",
      "loss:\n",
      "0.19775788530380342\n",
      "mse:\n",
      "0.39551577060760684\n",
      "\n",
      "loss:\n",
      "0.19773091314950939\n",
      "mse:\n",
      "0.39546182629901877\n",
      "\n",
      "loss:\n",
      "0.19770349011789798\n",
      "mse:\n",
      "0.39540698023579596\n",
      "\n",
      "loss:\n",
      "0.19767598992117383\n",
      "mse:\n",
      "0.39535197984234766\n",
      "\n",
      "loss:\n",
      "0.1976489482145749\n",
      "mse:\n",
      "0.3952978964291498\n",
      "\n",
      "loss:\n",
      "0.19762233423692585\n",
      "mse:\n",
      "0.3952446684738517\n",
      "\n",
      "loss:\n",
      "0.1975953527024258\n",
      "mse:\n",
      "0.3951907054048516\n",
      "\n",
      "loss:\n",
      "0.19756829031744524\n",
      "mse:\n",
      "0.3951365806348905\n",
      "\n",
      "loss:\n",
      "0.19754173034802738\n",
      "mse:\n",
      "0.39508346069605477\n",
      "\n",
      "loss:\n",
      "0.1975141050899901\n",
      "mse:\n",
      "0.3950282101799802\n",
      "\n",
      "loss:\n",
      "0.19748658163614266\n",
      "mse:\n",
      "0.3949731632722853\n",
      "\n",
      "loss:\n",
      "0.19745926278399206\n",
      "mse:\n",
      "0.3949185255679841\n",
      "\n",
      "loss:\n",
      "0.19743253533432578\n",
      "mse:\n",
      "0.39486507066865156\n",
      "\n",
      "loss:\n",
      "0.1974053705040566\n",
      "mse:\n",
      "0.3948107410081132\n",
      "\n",
      "loss:\n",
      "0.19737834372318386\n",
      "mse:\n",
      "0.3947566874463677\n",
      "\n",
      "loss:\n",
      "0.1973519817794375\n",
      "mse:\n",
      "0.394703963558875\n",
      "\n",
      "loss:\n",
      "0.1973244591068806\n",
      "mse:\n",
      "0.3946489182137612\n",
      "\n",
      "loss:\n",
      "0.19729698942062465\n",
      "mse:\n",
      "0.3945939788412493\n",
      "\n",
      "loss:\n",
      "0.1972693900331859\n",
      "mse:\n",
      "0.3945387800663718\n",
      "\n",
      "loss:\n",
      "0.1972423990864291\n",
      "mse:\n",
      "0.3944847981728582\n",
      "\n",
      "loss:\n",
      "0.19721460278963748\n",
      "mse:\n",
      "0.39442920557927497\n",
      "Annealing learning rate. New rate is 0.016618008974348453\n",
      "\n",
      "loss:\n",
      "0.19718954166338204\n",
      "mse:\n",
      "0.3943790833267641\n",
      "\n",
      "loss:\n",
      "0.19716425511696803\n",
      "mse:\n",
      "0.39432851023393606\n",
      "\n",
      "loss:\n",
      "0.19713918288520227\n",
      "mse:\n",
      "0.39427836577040454\n",
      "\n",
      "loss:\n",
      "0.19711451629895566\n",
      "mse:\n",
      "0.3942290325979113\n",
      "\n",
      "loss:\n",
      "0.19708997603514883\n",
      "mse:\n",
      "0.39417995207029766\n",
      "\n",
      "loss:\n",
      "0.19706586291317024\n",
      "mse:\n",
      "0.3941317258263405\n",
      "\n",
      "loss:\n",
      "0.1970413627237357\n",
      "mse:\n",
      "0.3940827254474714\n",
      "\n",
      "loss:\n",
      "0.1970168799742029\n",
      "mse:\n",
      "0.3940337599484058\n",
      "\n",
      "loss:\n",
      "0.19699250982292407\n",
      "mse:\n",
      "0.39398501964584814\n",
      "\n",
      "loss:\n",
      "0.1969684632048556\n",
      "mse:\n",
      "0.3939369264097112\n",
      "\n",
      "loss:\n",
      "0.19694403409479344\n",
      "mse:\n",
      "0.3938880681895869\n",
      "\n",
      "loss:\n",
      "0.19691966508017625\n",
      "mse:\n",
      "0.3938393301603525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss:\n",
      "0.19689555515918664\n",
      "mse:\n",
      "0.39379111031837327\n",
      "\n",
      "loss:\n",
      "0.196871792661308\n",
      "mse:\n",
      "0.393743585322616\n",
      "\n",
      "loss:\n",
      "0.19684763427965224\n",
      "mse:\n",
      "0.3936952685593045\n",
      "\n",
      "loss:\n",
      "0.19682338615755207\n",
      "mse:\n",
      "0.39364677231510414\n",
      "\n",
      "loss:\n",
      "0.1967993860677864\n",
      "mse:\n",
      "0.3935987721355728\n",
      "\n",
      "loss:\n",
      "0.19677511592103672\n",
      "mse:\n",
      "0.39355023184207344\n",
      "\n",
      "loss:\n",
      "0.19675072161115212\n",
      "mse:\n",
      "0.39350144322230424\n",
      "\n",
      "loss:\n",
      "0.19672632901066112\n",
      "mse:\n",
      "0.39345265802132223\n",
      "\n",
      "loss:\n",
      "0.1967034113396772\n",
      "mse:\n",
      "0.3934068226793544\n",
      "\n",
      "loss:\n",
      "0.19667971229074388\n",
      "mse:\n",
      "0.39335942458148776\n",
      "\n",
      "loss:\n",
      "0.1966562865885365\n",
      "mse:\n",
      "0.393312573177073\n",
      "\n",
      "loss:\n",
      "0.19663265220334705\n",
      "mse:\n",
      "0.3932653044066941\n",
      "\n",
      "loss:\n",
      "0.19660883999526968\n",
      "mse:\n",
      "0.39321767999053936\n",
      "\n",
      "loss:\n",
      "0.19658484192108985\n",
      "mse:\n",
      "0.3931696838421797\n",
      "Annealing learning rate. New rate is 0.014956208076913608\n",
      "\n",
      "loss:\n",
      "0.19656306512419394\n",
      "mse:\n",
      "0.3931261302483879\n",
      "\n",
      "loss:\n",
      "0.19654144842829963\n",
      "mse:\n",
      "0.39308289685659925\n",
      "\n",
      "loss:\n",
      "0.19652006821767976\n",
      "mse:\n",
      "0.3930401364353595\n",
      "\n",
      "loss:\n",
      "0.19649870629478663\n",
      "mse:\n",
      "0.39299741258957327\n",
      "\n",
      "loss:\n",
      "0.19647736217769657\n",
      "mse:\n",
      "0.39295472435539314\n",
      "\n",
      "loss:\n",
      "0.19645604176865603\n",
      "mse:\n",
      "0.39291208353731205\n",
      "\n",
      "loss:\n",
      "0.19643497691508396\n",
      "mse:\n",
      "0.39286995383016793\n",
      "\n",
      "loss:\n",
      "0.19641370205672626\n",
      "mse:\n",
      "0.3928274041134525\n",
      "\n",
      "loss:\n",
      "0.19639238834319284\n",
      "mse:\n",
      "0.3927847766863857\n",
      "\n",
      "loss:\n",
      "0.1963711950945618\n",
      "mse:\n",
      "0.3927423901891236\n",
      "\n",
      "loss:\n",
      "0.19635022340695654\n",
      "mse:\n",
      "0.3927004468139131\n",
      "\n",
      "loss:\n",
      "0.19632894363999467\n",
      "mse:\n",
      "0.39265788727998935\n",
      "\n",
      "loss:\n",
      "0.19630753349456073\n",
      "mse:\n",
      "0.39261506698912146\n",
      "\n",
      "loss:\n",
      "0.19628665908196177\n",
      "mse:\n",
      "0.39257331816392355\n",
      "\n",
      "loss:\n",
      "0.19626491652899314\n",
      "mse:\n",
      "0.3925298330579863\n",
      "\n",
      "loss:\n",
      "0.1962432581229186\n",
      "mse:\n",
      "0.3924865162458372\n",
      "\n",
      "loss:\n",
      "0.19622167850845032\n",
      "mse:\n",
      "0.39244335701690064\n",
      "\n",
      "loss:\n",
      "0.19620055156052468\n",
      "mse:\n",
      "0.39240110312104937\n",
      "\n",
      "loss:\n",
      "0.19617904230461602\n",
      "mse:\n",
      "0.39235808460923205\n",
      "\n",
      "loss:\n",
      "0.1961577187578047\n",
      "mse:\n",
      "0.3923154375156094\n",
      "\n",
      "loss:\n",
      "0.19613702201158856\n",
      "mse:\n",
      "0.3922740440231771\n",
      "\n",
      "loss:\n",
      "0.19611691285396488\n",
      "mse:\n",
      "0.39223382570792975\n",
      "\n",
      "loss:\n",
      "0.1960964446359875\n",
      "mse:\n",
      "0.392192889271975\n",
      "\n",
      "loss:\n",
      "0.1960759910552314\n",
      "mse:\n",
      "0.3921519821104628\n",
      "\n",
      "loss:\n",
      "0.1960555651663888\n",
      "mse:\n",
      "0.3921111303327776\n",
      "\n",
      "loss:\n",
      "0.19603568495102486\n",
      "mse:\n",
      "0.3920713699020497\n",
      "Annealing learning rate. New rate is 0.013460587269222246\n",
      "\n",
      "loss:\n",
      "0.19601754868162105\n",
      "mse:\n",
      "0.3920350973632421\n",
      "\n",
      "loss:\n",
      "0.19599944439123945\n",
      "mse:\n",
      "0.3919988887824789\n",
      "\n",
      "loss:\n",
      "0.19598158469879332\n",
      "mse:\n",
      "0.39196316939758663\n",
      "\n",
      "loss:\n",
      "0.19596325986683083\n",
      "mse:\n",
      "0.39192651973366166\n",
      "\n",
      "loss:\n",
      "0.1959449342102346\n",
      "mse:\n",
      "0.3918898684204692\n",
      "\n",
      "loss:\n",
      "0.19592676848236487\n",
      "mse:\n",
      "0.39185353696472974\n",
      "\n",
      "loss:\n",
      "0.19590884932734798\n",
      "mse:\n",
      "0.39181769865469596\n",
      "\n",
      "loss:\n",
      "0.19589070712576473\n",
      "mse:\n",
      "0.39178141425152946\n",
      "\n",
      "loss:\n",
      "0.195872550303762\n",
      "mse:\n",
      "0.391745100607524\n",
      "\n",
      "loss:\n",
      "0.1958547482823673\n",
      "mse:\n",
      "0.3917094965647346\n",
      "\n",
      "loss:\n",
      "0.1958366542570333\n",
      "mse:\n",
      "0.3916733085140666\n",
      "\n",
      "loss:\n",
      "0.19581847762777096\n",
      "mse:\n",
      "0.39163695525554193\n",
      "\n",
      "loss:\n",
      "0.1958002124629814\n",
      "mse:\n",
      "0.3916004249259628\n",
      "\n",
      "loss:\n",
      "0.19578231246586375\n",
      "mse:\n",
      "0.3915646249317275\n",
      "\n",
      "loss:\n",
      "0.19576401222031264\n",
      "mse:\n",
      "0.3915280244406253\n",
      "\n",
      "loss:\n",
      "0.19574568502932854\n",
      "mse:\n",
      "0.3914913700586571\n",
      "\n",
      "loss:\n",
      "0.19572779652648867\n",
      "mse:\n",
      "0.39145559305297734\n",
      "\n",
      "loss:\n",
      "0.1957094833147836\n",
      "mse:\n",
      "0.3914189666295672\n",
      "\n",
      "loss:\n",
      "0.19569110900704656\n",
      "mse:\n",
      "0.3913822180140931\n",
      "\n",
      "loss:\n",
      "0.1956730722432653\n",
      "mse:\n",
      "0.3913461444865306\n",
      "\n",
      "loss:\n",
      "0.19565491948020144\n",
      "mse:\n",
      "0.3913098389604029\n",
      "\n",
      "loss:\n",
      "0.19563723813089753\n",
      "mse:\n",
      "0.39127447626179507\n",
      "\n",
      "loss:\n",
      "0.19561920172640654\n",
      "mse:\n",
      "0.3912384034528131\n",
      "\n",
      "loss:\n",
      "0.19560147230258476\n",
      "mse:\n",
      "0.3912029446051695\n",
      "\n",
      "loss:\n",
      "0.19558340863284623\n",
      "mse:\n",
      "0.39116681726569247\n",
      "\n",
      "loss:\n",
      "0.1955656910565556\n",
      "mse:\n",
      "0.3911313821131112\n",
      "Annealing learning rate. New rate is 0.012114528542300022\n",
      "\n",
      "loss:\n",
      "0.19554950018667897\n",
      "mse:\n",
      "0.39109900037335793\n",
      "\n",
      "loss:\n",
      "0.19553325251185583\n",
      "mse:\n",
      "0.39106650502371165\n",
      "\n",
      "loss:\n",
      "0.19551703368879253\n",
      "mse:\n",
      "0.39103406737758506\n",
      "\n",
      "loss:\n",
      "0.19550059913727277\n",
      "mse:\n",
      "0.39100119827454555\n",
      "\n",
      "loss:\n",
      "0.19548481057256212\n",
      "mse:\n",
      "0.39096962114512424\n",
      "\n",
      "loss:\n",
      "0.19546850058239404\n",
      "mse:\n",
      "0.3909370011647881\n",
      "\n",
      "loss:\n",
      "0.1954524264557018\n",
      "mse:\n",
      "0.3909048529114036\n",
      "\n",
      "loss:\n",
      "0.19543613089980738\n",
      "mse:\n",
      "0.39087226179961476\n",
      "\n",
      "loss:\n",
      "0.19542010865547074\n",
      "mse:\n",
      "0.3908402173109415\n",
      "\n",
      "loss:\n",
      "0.19540423346546268\n",
      "mse:\n",
      "0.39080846693092536\n",
      "\n",
      "loss:\n",
      "0.19538815317515607\n",
      "mse:\n",
      "0.39077630635031213\n",
      "\n",
      "loss:\n",
      "0.19537234129166672\n",
      "mse:\n",
      "0.39074468258333345\n",
      "\n",
      "loss:\n",
      "0.19535635777216936\n",
      "mse:\n",
      "0.39071271554433873\n",
      "\n",
      "loss:\n",
      "0.1953406517279224\n",
      "mse:\n",
      "0.3906813034558448\n",
      "\n",
      "loss:\n",
      "0.19532484959990615\n",
      "mse:\n",
      "0.3906496991998123\n",
      "\n",
      "loss:\n",
      "0.19530910982435257\n",
      "mse:\n",
      "0.39061821964870513\n",
      "\n",
      "loss:\n",
      "0.19529376887123137\n",
      "mse:\n",
      "0.39058753774246274\n",
      "\n",
      "loss:\n",
      "0.19527821374348545\n",
      "mse:\n",
      "0.3905564274869709\n",
      "\n",
      "loss:\n",
      "0.19526271849653948\n",
      "mse:\n",
      "0.39052543699307896\n",
      "\n",
      "loss:\n",
      "0.19524705561568284\n",
      "mse:\n",
      "0.3904941112313657\n",
      "\n",
      "loss:\n",
      "0.1952315337431695\n",
      "mse:\n",
      "0.390463067486339\n",
      "\n",
      "loss:\n",
      "0.19521597167569263\n",
      "mse:\n",
      "0.39043194335138526\n",
      "\n",
      "loss:\n",
      "0.19520037624530467\n",
      "mse:\n",
      "0.39040075249060935\n",
      "\n",
      "loss:\n",
      "0.1951851205798852\n",
      "mse:\n",
      "0.3903702411597704\n",
      "\n",
      "loss:\n",
      "0.19516986389891852\n",
      "mse:\n",
      "0.39033972779783704\n",
      "\n",
      "loss:\n",
      "0.19515497455202144\n",
      "mse:\n",
      "0.3903099491040429\n",
      "Annealing learning rate. New rate is 0.01090307568807002\n",
      "\n",
      "loss:\n",
      "0.19514127631045988\n",
      "mse:\n",
      "0.39028255262091976\n",
      "\n",
      "loss:\n",
      "0.19512814568394524\n",
      "mse:\n",
      "0.3902562913678905\n",
      "\n",
      "loss:\n",
      "0.19511516791789033\n",
      "mse:\n",
      "0.39023033583578065\n",
      "\n",
      "loss:\n",
      "0.19510202467141718\n",
      "mse:\n",
      "0.39020404934283437\n",
      "\n",
      "loss:\n",
      "0.1950892390977275\n",
      "mse:\n",
      "0.390178478195455\n",
      "\n",
      "loss:\n",
      "0.19507624271823593\n",
      "mse:\n",
      "0.39015248543647185\n",
      "\n",
      "loss:\n",
      "0.19506321959557973\n",
      "mse:\n",
      "0.39012643919115947\n",
      "\n",
      "loss:\n",
      "0.19504973204451412\n",
      "mse:\n",
      "0.39009946408902824\n",
      "\n",
      "loss:\n",
      "0.19503623147300367\n",
      "mse:\n",
      "0.39007246294600734\n",
      "\n",
      "loss:\n",
      "0.19502289843976336\n",
      "mse:\n",
      "0.39004579687952673\n",
      "\n",
      "loss:\n",
      "0.19500945852268287\n",
      "mse:\n",
      "0.39001891704536573\n",
      "\n",
      "loss:\n",
      "0.19499613334949692\n",
      "mse:\n",
      "0.38999226669899384\n",
      "\n",
      "loss:\n",
      "0.19498271420314583\n",
      "mse:\n",
      "0.38996542840629167\n",
      "\n",
      "loss:\n",
      "0.19496955369923502\n",
      "mse:\n",
      "0.38993910739847004\n",
      "\n",
      "loss:\n",
      "0.19495615419998494\n",
      "mse:\n",
      "0.3899123083999699\n",
      "\n",
      "loss:\n",
      "0.19494253018648902\n",
      "mse:\n",
      "0.38988506037297804\n",
      "\n",
      "loss:\n",
      "0.19492903422190339\n",
      "mse:\n",
      "0.38985806844380677\n",
      "\n",
      "loss:\n",
      "0.19491547400066894\n",
      "mse:\n",
      "0.3898309480013379\n",
      "\n",
      "loss:\n",
      "0.19490213317152288\n",
      "mse:\n",
      "0.38980426634304577\n",
      "\n",
      "loss:\n",
      "0.19488891100647288\n",
      "mse:\n",
      "0.38977782201294575\n",
      "\n",
      "loss:\n",
      "0.1948760294585009\n",
      "mse:\n",
      "0.3897520589170018\n",
      "\n",
      "loss:\n",
      "0.19486289819321584\n",
      "mse:\n",
      "0.3897257963864317\n",
      "\n",
      "loss:\n",
      "0.19484975537819854\n",
      "mse:\n",
      "0.3896995107563971\n",
      "\n",
      "loss:\n",
      "0.19483664532153172\n",
      "mse:\n",
      "0.38967329064306344\n",
      "\n",
      "loss:\n",
      "0.1948233087639594\n",
      "mse:\n",
      "0.3896466175279188\n",
      "\n",
      "loss:\n",
      "0.19481018461591942\n",
      "mse:\n",
      "0.38962036923183885\n",
      "Annealing learning rate. New rate is 0.009812768119263017\n",
      "\n",
      "loss:\n",
      "0.19479815947657547\n",
      "mse:\n",
      "0.38959631895315094\n",
      "\n",
      "loss:\n",
      "0.19478611646259592\n",
      "mse:\n",
      "0.38957223292519183\n",
      "\n",
      "loss:\n",
      "0.19477443661490818\n",
      "mse:\n",
      "0.38954887322981635\n",
      "\n",
      "loss:\n",
      "0.19476263640280317\n",
      "mse:\n",
      "0.38952527280560634\n",
      "\n",
      "loss:\n",
      "0.19475094366302653\n",
      "mse:\n",
      "0.38950188732605306\n",
      "\n",
      "loss:\n",
      "0.19473913021386308\n",
      "mse:\n",
      "0.38947826042772615\n",
      "\n",
      "loss:\n",
      "0.1947276002347293\n",
      "mse:\n",
      "0.3894552004694586\n",
      "\n",
      "loss:\n",
      "0.19471567853075317\n",
      "mse:\n",
      "0.38943135706150633\n",
      "\n",
      "loss:\n",
      "0.19470375451113128\n",
      "mse:\n",
      "0.38940750902226257\n",
      "\n",
      "loss:\n",
      "0.19469204392443884\n",
      "mse:\n",
      "0.3893840878488777\n",
      "\n",
      "loss:\n",
      "0.19467997388759184\n",
      "mse:\n",
      "0.3893599477751837\n",
      "\n",
      "loss:\n",
      "0.19466818215847026\n",
      "mse:\n",
      "0.3893363643169405\n",
      "\n",
      "loss:\n",
      "0.19465577281656093\n",
      "mse:\n",
      "0.38931154563312187\n",
      "\n",
      "loss:\n",
      "0.19464352428212284\n",
      "mse:\n",
      "0.3892870485642457\n",
      "\n",
      "loss:\n",
      "0.19463152146075438\n",
      "mse:\n",
      "0.38926304292150876\n",
      "\n",
      "loss:\n",
      "0.19461930898871413\n",
      "mse:\n",
      "0.38923861797742826\n",
      "\n",
      "loss:\n",
      "0.1946072594543703\n",
      "mse:\n",
      "0.3892145189087406\n",
      "\n",
      "loss:\n",
      "0.19459502385332972\n",
      "mse:\n",
      "0.38919004770665944\n",
      "\n",
      "loss:\n",
      "0.19458284432118345\n",
      "mse:\n",
      "0.3891656886423669\n",
      "\n",
      "loss:\n",
      "0.19457100973195599\n",
      "mse:\n",
      "0.38914201946391197\n",
      "\n",
      "loss:\n",
      "0.19455922989044744\n",
      "mse:\n",
      "0.3891184597808949\n",
      "\n",
      "loss:\n",
      "0.19454730135915288\n",
      "mse:\n",
      "0.38909460271830576\n",
      "\n",
      "loss:\n",
      "0.19453541046551273\n",
      "mse:\n",
      "0.38907082093102546\n",
      "\n",
      "loss:\n",
      "0.19452340643115462\n",
      "mse:\n",
      "0.38904681286230924\n",
      "\n",
      "loss:\n",
      "0.19451174399603852\n",
      "mse:\n",
      "0.38902348799207703\n",
      "\n",
      "loss:\n",
      "0.1944998652117974\n",
      "mse:\n",
      "0.3889997304235948\n",
      "Annealing learning rate. New rate is 0.008831491307336715\n",
      "\n",
      "loss:\n",
      "0.19448907852780067\n",
      "mse:\n",
      "0.38897815705560135\n",
      "\n",
      "loss:\n",
      "0.19447854790841373\n",
      "mse:\n",
      "0.38895709581682747\n",
      "\n",
      "loss:\n",
      "0.19446784901369152\n",
      "mse:\n",
      "0.38893569802738304\n",
      "\n",
      "loss:\n",
      "0.19445729910911702\n",
      "mse:\n",
      "0.38891459821823404\n",
      "\n",
      "loss:\n",
      "0.1944464832486802\n",
      "mse:\n",
      "0.3888929664973604\n",
      "\n",
      "loss:\n",
      "0.19443566047016844\n",
      "mse:\n",
      "0.3888713209403369\n",
      "\n",
      "loss:\n",
      "0.19442497599124856\n",
      "mse:\n",
      "0.3888499519824971\n",
      "\n",
      "loss:\n",
      "0.19441401568021777\n",
      "mse:\n",
      "0.38882803136043553\n",
      "\n",
      "loss:\n",
      "0.19440311339970673\n",
      "mse:\n",
      "0.38880622679941346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss:\n",
      "0.19439243800927208\n",
      "mse:\n",
      "0.38878487601854417\n",
      "\n",
      "loss:\n",
      "0.19438161896417233\n",
      "mse:\n",
      "0.38876323792834466\n",
      "\n",
      "loss:\n",
      "0.19437093036406197\n",
      "mse:\n",
      "0.38874186072812394\n",
      "\n",
      "loss:\n",
      "0.1943601342976826\n",
      "mse:\n",
      "0.3887202685953652\n",
      "\n",
      "loss:\n",
      "0.19434930766983907\n",
      "mse:\n",
      "0.38869861533967814\n",
      "\n",
      "loss:\n",
      "0.1943389762485927\n",
      "mse:\n",
      "0.3886779524971854\n",
      "\n",
      "loss:\n",
      "0.1943283093913574\n",
      "mse:\n",
      "0.3886566187827148\n",
      "\n",
      "loss:\n",
      "0.19431736899829447\n",
      "mse:\n",
      "0.38863473799658893\n",
      "\n",
      "loss:\n",
      "0.19430666462130958\n",
      "mse:\n",
      "0.38861332924261915\n",
      "\n",
      "loss:\n",
      "0.19429567327151492\n",
      "mse:\n",
      "0.38859134654302985\n",
      "\n",
      "loss:\n",
      "0.19428481263707573\n",
      "mse:\n",
      "0.38856962527415145\n",
      "\n",
      "loss:\n",
      "0.1942740893960928\n",
      "mse:\n",
      "0.3885481787921856\n",
      "\n",
      "loss:\n",
      "0.19426343221038225\n",
      "mse:\n",
      "0.3885268644207645\n",
      "\n",
      "loss:\n",
      "0.1942529779304203\n",
      "mse:\n",
      "0.3885059558608406\n",
      "\n",
      "loss:\n",
      "0.19424239201207125\n",
      "mse:\n",
      "0.3884847840241425\n",
      "\n",
      "loss:\n",
      "0.19423199691189796\n",
      "mse:\n",
      "0.38846399382379593\n",
      "\n",
      "loss:\n",
      "0.19422140105564617\n",
      "mse:\n",
      "0.38844280211129234\n",
      "Annealing learning rate. New rate is 0.007948342176603044\n",
      "\n",
      "loss:\n",
      "0.19421186360586395\n",
      "mse:\n",
      "0.3884237272117279\n",
      "\n",
      "loss:\n",
      "0.19420256692970186\n",
      "mse:\n",
      "0.38840513385940373\n",
      "\n",
      "loss:\n",
      "0.19419309393037837\n",
      "mse:\n",
      "0.38838618786075674\n",
      "\n",
      "loss:\n",
      "0.19418355326095774\n",
      "mse:\n",
      "0.3883671065219155\n",
      "\n",
      "loss:\n",
      "0.19417412729979852\n",
      "mse:\n",
      "0.38834825459959704\n",
      "\n",
      "loss:\n",
      "0.19416452149830823\n",
      "mse:\n",
      "0.38832904299661647\n",
      "\n",
      "loss:\n",
      "0.19415512082377015\n",
      "mse:\n",
      "0.3883102416475403\n",
      "\n",
      "loss:\n",
      "0.19414551670533367\n",
      "mse:\n",
      "0.38829103341066734\n",
      "\n",
      "loss:\n",
      "0.19413589525416328\n",
      "mse:\n",
      "0.38827179050832655\n",
      "\n",
      "loss:\n",
      "0.19412651062035127\n",
      "mse:\n",
      "0.38825302124070254\n",
      "\n",
      "loss:\n",
      "0.19411700715088848\n",
      "mse:\n",
      "0.38823401430177695\n",
      "\n",
      "loss:\n",
      "0.19410768897938552\n",
      "mse:\n",
      "0.38821537795877104\n",
      "\n",
      "loss:\n",
      "0.19409825947569245\n",
      "mse:\n",
      "0.3881965189513849\n",
      "\n",
      "loss:\n",
      "0.19408874997583714\n",
      "mse:\n",
      "0.3881774999516743\n",
      "\n",
      "loss:\n",
      "0.19407938364695876\n",
      "mse:\n",
      "0.3881587672939175\n",
      "\n",
      "loss:\n",
      "0.1940698262474826\n",
      "mse:\n",
      "0.3881396524949652\n",
      "\n",
      "loss:\n",
      "0.19406021435194185\n",
      "mse:\n",
      "0.3881204287038837\n",
      "\n",
      "loss:\n",
      "0.1940506936953744\n",
      "mse:\n",
      "0.3881013873907488\n",
      "\n",
      "loss:\n",
      "0.19404100628685317\n",
      "mse:\n",
      "0.38808201257370634\n",
      "\n",
      "loss:\n",
      "0.19403152497862813\n",
      "mse:\n",
      "0.38806304995725627\n",
      "\n",
      "loss:\n",
      "0.19402184966765582\n",
      "mse:\n",
      "0.38804369933531163\n",
      "\n",
      "loss:\n",
      "0.1940121982740494\n",
      "mse:\n",
      "0.3880243965480988\n",
      "\n",
      "loss:\n",
      "0.19400274218259328\n",
      "mse:\n",
      "0.38800548436518656\n",
      "\n",
      "loss:\n",
      "0.19399315584726995\n",
      "mse:\n",
      "0.3879863116945399\n",
      "\n",
      "loss:\n",
      "0.19398352751094733\n",
      "mse:\n",
      "0.38796705502189466\n",
      "\n",
      "loss:\n",
      "0.1939739728071942\n",
      "mse:\n",
      "0.3879479456143884\n",
      "Annealing learning rate. New rate is 0.007153507958942739\n",
      "\n",
      "loss:\n",
      "0.19396577012439203\n",
      "mse:\n",
      "0.38793154024878407\n",
      "\n",
      "loss:\n",
      "0.19395764590855774\n",
      "mse:\n",
      "0.3879152918171155\n",
      "\n",
      "loss:\n",
      "0.19394936565842885\n",
      "mse:\n",
      "0.3878987313168577\n",
      "\n",
      "loss:\n",
      "0.1939410485941671\n",
      "mse:\n",
      "0.3878820971883342\n",
      "\n",
      "loss:\n",
      "0.19393285855966477\n",
      "mse:\n",
      "0.38786571711932954\n",
      "\n",
      "loss:\n",
      "0.1939244862035372\n",
      "mse:\n",
      "0.3878489724070744\n",
      "\n",
      "loss:\n",
      "0.19391606238500655\n",
      "mse:\n",
      "0.3878321247700131\n",
      "\n",
      "loss:\n",
      "0.1939079186845409\n",
      "mse:\n",
      "0.3878158373690818\n",
      "\n",
      "loss:\n",
      "0.19389969811949098\n",
      "mse:\n",
      "0.38779939623898196\n",
      "\n",
      "loss:\n",
      "0.19389157543571445\n",
      "mse:\n",
      "0.3877831508714289\n",
      "\n",
      "loss:\n",
      "0.19388329158737244\n",
      "mse:\n",
      "0.3877665831747449\n",
      "\n",
      "loss:\n",
      "0.19387492422056882\n",
      "mse:\n",
      "0.38774984844113763\n",
      "\n",
      "loss:\n",
      "0.19386676052928684\n",
      "mse:\n",
      "0.38773352105857367\n",
      "\n",
      "loss:\n",
      "0.19385826759252905\n",
      "mse:\n",
      "0.3877165351850581\n",
      "\n",
      "loss:\n",
      "0.1938498593113953\n",
      "mse:\n",
      "0.3876997186227906\n",
      "\n",
      "loss:\n",
      "0.19384155848119414\n",
      "mse:\n",
      "0.3876831169623883\n",
      "\n",
      "loss:\n",
      "0.19383298625650575\n",
      "mse:\n",
      "0.3876659725130115\n",
      "\n",
      "loss:\n",
      "0.19382439666967394\n",
      "mse:\n",
      "0.3876487933393479\n",
      "\n",
      "loss:\n",
      "0.1938159759836319\n",
      "mse:\n",
      "0.3876319519672638\n",
      "\n",
      "loss:\n",
      "0.1938074241744282\n",
      "mse:\n",
      "0.3876148483488564\n",
      "\n",
      "loss:\n",
      "0.193798715127581\n",
      "mse:\n",
      "0.387597430255162\n",
      "\n",
      "loss:\n",
      "0.1937901179369475\n",
      "mse:\n",
      "0.387580235873895\n",
      "\n",
      "loss:\n",
      "0.19378141732840762\n",
      "mse:\n",
      "0.38756283465681524\n",
      "\n",
      "loss:\n",
      "0.1937729965215197\n",
      "mse:\n",
      "0.3875459930430394\n",
      "\n",
      "loss:\n",
      "0.19376432116647532\n",
      "mse:\n",
      "0.38752864233295065\n",
      "\n",
      "loss:\n",
      "0.19375566377170447\n",
      "mse:\n",
      "0.38751132754340895\n",
      "Annealing learning rate. New rate is 0.006438157163048465\n",
      "\n",
      "loss:\n",
      "0.19374797690536757\n",
      "mse:\n",
      "0.38749595381073515\n",
      "\n",
      "loss:\n",
      "0.19374006309679512\n",
      "mse:\n",
      "0.38748012619359024\n",
      "\n",
      "loss:\n",
      "0.19373224897469474\n",
      "mse:\n",
      "0.3874644979493895\n",
      "\n",
      "loss:\n",
      "0.19372457489999037\n",
      "mse:\n",
      "0.38744914979998074\n",
      "\n",
      "loss:\n",
      "0.19371675727970933\n",
      "mse:\n",
      "0.38743351455941866\n",
      "\n",
      "loss:\n",
      "0.19370901666113902\n",
      "mse:\n",
      "0.38741803332227803\n",
      "\n",
      "loss:\n",
      "0.19370112039077306\n",
      "mse:\n",
      "0.3874022407815461\n",
      "\n",
      "loss:\n",
      "0.19369325617489738\n",
      "mse:\n",
      "0.38738651234979476\n",
      "\n",
      "loss:\n",
      "0.1936852871230559\n",
      "mse:\n",
      "0.3873705742461118\n",
      "\n",
      "loss:\n",
      "0.1936772092177401\n",
      "mse:\n",
      "0.3873544184354802\n",
      "\n",
      "loss:\n",
      "0.19366911938847484\n",
      "mse:\n",
      "0.3873382387769497\n",
      "\n",
      "loss:\n",
      "0.19366112817516304\n",
      "mse:\n",
      "0.3873222563503261\n",
      "\n",
      "loss:\n",
      "0.19365304083451942\n",
      "mse:\n",
      "0.38730608166903885\n",
      "\n",
      "loss:\n",
      "0.19364492112470189\n",
      "mse:\n",
      "0.38728984224940377\n",
      "\n",
      "loss:\n",
      "0.19363702837130828\n",
      "mse:\n",
      "0.38727405674261656\n",
      "\n",
      "loss:\n",
      "0.19362889410367717\n",
      "mse:\n",
      "0.38725778820735435\n",
      "\n",
      "loss:\n",
      "0.19362091933247674\n",
      "mse:\n",
      "0.3872418386649535\n",
      "\n",
      "loss:\n",
      "0.19361281069975542\n",
      "mse:\n",
      "0.38722562139951083\n",
      "\n",
      "loss:\n",
      "0.19360471736769833\n",
      "mse:\n",
      "0.38720943473539665\n",
      "\n",
      "loss:\n",
      "0.19359677913272538\n",
      "mse:\n",
      "0.38719355826545077\n",
      "\n",
      "loss:\n",
      "0.19358868489379824\n",
      "mse:\n",
      "0.3871773697875965\n",
      "\n",
      "loss:\n",
      "0.1935804808203957\n",
      "mse:\n",
      "0.3871609616407914\n",
      "\n",
      "loss:\n",
      "0.1935725301247258\n",
      "mse:\n",
      "0.3871450602494516\n",
      "\n",
      "loss:\n",
      "0.19356447332355292\n",
      "mse:\n",
      "0.38712894664710584\n",
      "\n",
      "loss:\n",
      "0.1935565539022301\n",
      "mse:\n",
      "0.3871131078044602\n",
      "\n",
      "loss:\n",
      "0.1935485703373537\n",
      "mse:\n",
      "0.3870971406747074\n",
      "Annealing learning rate. New rate is 0.005794341446743619\n",
      "\n",
      "loss:\n",
      "0.19354139617692068\n",
      "mse:\n",
      "0.38708279235384135\n",
      "\n",
      "loss:\n",
      "0.19353433038979448\n",
      "mse:\n",
      "0.38706866077958896\n",
      "\n",
      "loss:\n",
      "0.19352710726870712\n",
      "mse:\n",
      "0.38705421453741423\n",
      "\n",
      "loss:\n",
      "0.19352066180194666\n",
      "mse:\n",
      "0.3870413236038933\n",
      "\n",
      "loss:\n",
      "0.19351428784139996\n",
      "mse:\n",
      "0.3870285756827999\n",
      "\n",
      "loss:\n",
      "0.19350773169266933\n",
      "mse:\n",
      "0.38701546338533865\n",
      "\n",
      "loss:\n",
      "0.19350116441565693\n",
      "mse:\n",
      "0.38700232883131386\n",
      "\n",
      "loss:\n",
      "0.19349460023533277\n",
      "mse:\n",
      "0.38698920047066554\n",
      "\n",
      "loss:\n",
      "0.19348788551111032\n",
      "mse:\n",
      "0.38697577102222064\n",
      "\n",
      "loss:\n",
      "0.19348120931907714\n",
      "mse:\n",
      "0.3869624186381543\n",
      "\n",
      "loss:\n",
      "0.1934746375179314\n",
      "mse:\n",
      "0.3869492750358628\n",
      "\n",
      "loss:\n",
      "0.1934679496803304\n",
      "mse:\n",
      "0.3869358993606608\n",
      "\n",
      "loss:\n",
      "0.1934612703083386\n",
      "mse:\n",
      "0.3869225406166772\n",
      "\n",
      "loss:\n",
      "0.19345473141842123\n",
      "mse:\n",
      "0.38690946283684247\n",
      "\n",
      "loss:\n",
      "0.1934479558551066\n",
      "mse:\n",
      "0.3868959117102132\n",
      "\n",
      "loss:\n",
      "0.1934411844922446\n",
      "mse:\n",
      "0.3868823689844892\n",
      "\n",
      "loss:\n",
      "0.1934345343473306\n",
      "mse:\n",
      "0.3868690686946612\n",
      "\n",
      "loss:\n",
      "0.1934277908849043\n",
      "mse:\n",
      "0.3868555817698086\n",
      "\n",
      "loss:\n",
      "0.19342105079156768\n",
      "mse:\n",
      "0.38684210158313537\n",
      "\n",
      "loss:\n",
      "0.1934143459525497\n",
      "mse:\n",
      "0.3868286919050994\n",
      "\n",
      "loss:\n",
      "0.19340756447799615\n",
      "mse:\n",
      "0.3868151289559923\n",
      "\n",
      "loss:\n",
      "0.19340085816344757\n",
      "mse:\n",
      "0.38680171632689514\n",
      "\n",
      "loss:\n",
      "0.19339426975933258\n",
      "mse:\n",
      "0.38678853951866515\n",
      "\n",
      "loss:\n",
      "0.19338755494994722\n",
      "mse:\n",
      "0.38677510989989444\n",
      "\n",
      "loss:\n",
      "0.19338087404696233\n",
      "mse:\n",
      "0.38676174809392466\n",
      "\n",
      "loss:\n",
      "0.19337414225564115\n",
      "mse:\n",
      "0.3867482845112823\n",
      "Annealing learning rate. New rate is 0.005214907302069257\n",
      "\n",
      "loss:\n",
      "0.19336821381927874\n",
      "mse:\n",
      "0.3867364276385575\n",
      "\n",
      "loss:\n",
      "0.19336220707889418\n",
      "mse:\n",
      "0.38672441415778835\n",
      "\n",
      "loss:\n",
      "0.1933561941676762\n",
      "mse:\n",
      "0.3867123883353524\n",
      "\n",
      "loss:\n",
      "0.1933502872426729\n",
      "mse:\n",
      "0.3867005744853458\n",
      "\n",
      "loss:\n",
      "0.1933442385556671\n",
      "mse:\n",
      "0.3866884771113342\n",
      "\n",
      "loss:\n",
      "0.19333811191306138\n",
      "mse:\n",
      "0.38667622382612277\n",
      "\n",
      "loss:\n",
      "0.19333216402969636\n",
      "mse:\n",
      "0.3866643280593927\n",
      "\n",
      "loss:\n",
      "0.1933261109999071\n",
      "mse:\n",
      "0.3866522219998142\n",
      "\n",
      "loss:\n",
      "0.1933200408771766\n",
      "mse:\n",
      "0.3866400817543532\n",
      "\n",
      "loss:\n",
      "0.1933140689452619\n",
      "mse:\n",
      "0.3866281378905238\n",
      "\n",
      "loss:\n",
      "0.19330800307209486\n",
      "mse:\n",
      "0.38661600614418973\n",
      "\n",
      "loss:\n",
      "0.19330183913355756\n",
      "mse:\n",
      "0.3866036782671151\n",
      "\n",
      "loss:\n",
      "0.19329584614325243\n",
      "mse:\n",
      "0.38659169228650486\n",
      "\n",
      "loss:\n",
      "0.19328972069043887\n",
      "mse:\n",
      "0.38657944138087774\n",
      "\n",
      "loss:\n",
      "0.19328370618272406\n",
      "mse:\n",
      "0.3865674123654481\n",
      "\n",
      "loss:\n",
      "0.19327778720810154\n",
      "mse:\n",
      "0.3865555744162031\n",
      "\n",
      "loss:\n",
      "0.19327176746292507\n",
      "mse:\n",
      "0.38654353492585014\n",
      "\n",
      "loss:\n",
      "0.19326570797613546\n",
      "mse:\n",
      "0.3865314159522709\n",
      "\n",
      "loss:\n",
      "0.19325978625066204\n",
      "mse:\n",
      "0.38651957250132407\n",
      "\n",
      "loss:\n",
      "0.19325371319274745\n",
      "mse:\n",
      "0.3865074263854949\n",
      "\n",
      "loss:\n",
      "0.19324766835348664\n",
      "mse:\n",
      "0.3864953367069733\n",
      "\n",
      "loss:\n",
      "0.19324175103944066\n",
      "mse:\n",
      "0.3864835020788813\n",
      "\n",
      "loss:\n",
      "0.19323571270072484\n",
      "mse:\n",
      "0.3864714254014497\n",
      "\n",
      "loss:\n",
      "0.19322966769321528\n",
      "mse:\n",
      "0.38645933538643057\n",
      "\n",
      "loss:\n",
      "0.1932238351609756\n",
      "mse:\n",
      "0.3864476703219512\n",
      "\n",
      "loss:\n",
      "0.19321802522260978\n",
      "mse:\n",
      "0.38643605044521956\n",
      "Annealing learning rate. New rate is 0.0046934165718623315\n",
      "\n",
      "loss:\n",
      "0.1932128743314596\n",
      "mse:\n",
      "0.3864257486629192\n",
      "\n",
      "loss:\n",
      "0.19320767744427608\n",
      "mse:\n",
      "0.38641535488855216\n",
      "\n",
      "loss:\n",
      "0.19320258235745205\n",
      "mse:\n",
      "0.3864051647149041\n",
      "\n",
      "loss:\n",
      "0.19319740259507076\n",
      "mse:\n",
      "0.3863948051901415\n",
      "\n",
      "loss:\n",
      "0.193192174913976\n",
      "mse:\n",
      "0.386384349827952\n",
      "\n",
      "loss:\n",
      "0.1931870901521688\n",
      "mse:\n",
      "0.3863741803043376\n",
      "\n",
      "loss:\n",
      "0.19318187642055928\n",
      "mse:\n",
      "0.38636375284111857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss:\n",
      "0.19317665667328102\n",
      "mse:\n",
      "0.38635331334656203\n",
      "\n",
      "loss:\n",
      "0.19317153931729236\n",
      "mse:\n",
      "0.3863430786345847\n",
      "\n",
      "loss:\n",
      "0.19316638436404854\n",
      "mse:\n",
      "0.3863327687280971\n",
      "\n",
      "loss:\n",
      "0.19316114867721607\n",
      "mse:\n",
      "0.38632229735443213\n",
      "\n",
      "loss:\n",
      "0.1931560672599848\n",
      "mse:\n",
      "0.3863121345199696\n",
      "\n",
      "loss:\n",
      "0.19315086222966693\n",
      "mse:\n",
      "0.38630172445933386\n",
      "\n",
      "loss:\n",
      "0.19314565910649611\n",
      "mse:\n",
      "0.38629131821299223\n",
      "\n",
      "loss:\n",
      "0.19314056469253463\n",
      "mse:\n",
      "0.38628112938506926\n",
      "\n",
      "loss:\n",
      "0.19313536570184747\n",
      "mse:\n",
      "0.38627073140369494\n",
      "\n",
      "loss:\n",
      "0.1931301277525525\n",
      "mse:\n",
      "0.386260255505105\n",
      "\n",
      "loss:\n",
      "0.1931250347345132\n",
      "mse:\n",
      "0.3862500694690264\n",
      "\n",
      "loss:\n",
      "0.19311986347157736\n",
      "mse:\n",
      "0.3862397269431547\n",
      "\n",
      "loss:\n",
      "0.19311467238351776\n",
      "mse:\n",
      "0.3862293447670355\n",
      "\n",
      "loss:\n",
      "0.1931095827111414\n",
      "mse:\n",
      "0.3862191654222828\n",
      "\n",
      "loss:\n",
      "0.1931045688780269\n",
      "mse:\n",
      "0.3862091377560538\n",
      "\n",
      "loss:\n",
      "0.1930994962063064\n",
      "mse:\n",
      "0.3861989924126128\n",
      "\n",
      "loss:\n",
      "0.19309459453112238\n",
      "mse:\n",
      "0.38618918906224475\n",
      "\n",
      "loss:\n",
      "0.19308965679980075\n",
      "mse:\n",
      "0.3861793135996015\n",
      "\n",
      "loss:\n",
      "0.19308472913938227\n",
      "mse:\n",
      "0.38616945827876453\n",
      "Annealing learning rate. New rate is 0.004224074914676098\n",
      "\n",
      "loss:\n",
      "0.1930803782549136\n",
      "mse:\n",
      "0.3861607565098272\n",
      "\n",
      "loss:\n",
      "0.19307601459440402\n",
      "mse:\n",
      "0.38615202918880803\n",
      "\n",
      "loss:\n",
      "0.19307160266750795\n",
      "mse:\n",
      "0.3861432053350159\n",
      "\n",
      "loss:\n",
      "0.1930673483298959\n",
      "mse:\n",
      "0.3861346966597918\n",
      "\n",
      "loss:\n",
      "0.1930629090468883\n",
      "mse:\n",
      "0.3861258180937766\n",
      "\n",
      "loss:\n",
      "0.19305856525357118\n",
      "mse:\n",
      "0.38611713050714236\n",
      "\n",
      "loss:\n",
      "0.19305421038667744\n",
      "mse:\n",
      "0.3861084207733549\n",
      "\n",
      "loss:\n",
      "0.19304991266733804\n",
      "mse:\n",
      "0.38609982533467607\n",
      "\n",
      "loss:\n",
      "0.19304550382980337\n",
      "mse:\n",
      "0.38609100765960674\n",
      "\n",
      "loss:\n",
      "0.19304125163724833\n",
      "mse:\n",
      "0.38608250327449667\n",
      "\n",
      "loss:\n",
      "0.19303686259722688\n",
      "mse:\n",
      "0.38607372519445377\n",
      "\n",
      "loss:\n",
      "0.19303253236454523\n",
      "mse:\n",
      "0.38606506472909047\n",
      "\n",
      "loss:\n",
      "0.19302831176222857\n",
      "mse:\n",
      "0.38605662352445713\n",
      "\n",
      "loss:\n",
      "0.19302399286477562\n",
      "mse:\n",
      "0.38604798572955124\n",
      "\n",
      "loss:\n",
      "0.19301974172054157\n",
      "mse:\n",
      "0.38603948344108313\n",
      "\n",
      "loss:\n",
      "0.19301554021635908\n",
      "mse:\n",
      "0.38603108043271817\n",
      "\n",
      "loss:\n",
      "0.19301126202614446\n",
      "mse:\n",
      "0.38602252405228893\n",
      "\n",
      "loss:\n",
      "0.1930069847024078\n",
      "mse:\n",
      "0.3860139694048156\n",
      "\n",
      "loss:\n",
      "0.19300281024809102\n",
      "mse:\n",
      "0.38600562049618203\n",
      "\n",
      "loss:\n",
      "0.19299854679242798\n",
      "mse:\n",
      "0.38599709358485595\n",
      "\n",
      "loss:\n",
      "0.19299417581714184\n",
      "mse:\n",
      "0.3859883516342837\n",
      "\n",
      "loss:\n",
      "0.192989956289583\n",
      "mse:\n",
      "0.385979912579166\n",
      "\n",
      "loss:\n",
      "0.1929856340451833\n",
      "mse:\n",
      "0.3859712680903666\n",
      "\n",
      "loss:\n",
      "0.19298132501286885\n",
      "mse:\n",
      "0.3859626500257377\n",
      "\n",
      "loss:\n",
      "0.1929771294999505\n",
      "mse:\n",
      "0.385954258999901\n",
      "\n",
      "loss:\n",
      "0.19297278087405365\n",
      "mse:\n",
      "0.3859455617481073\n",
      "Annealing learning rate. New rate is 0.0038016674232084885\n",
      "\n",
      "loss:\n",
      "0.19296892302232554\n",
      "mse:\n",
      "0.3859378460446511\n",
      "\n",
      "loss:\n",
      "0.1929651339837105\n",
      "mse:\n",
      "0.385930267967421\n",
      "\n",
      "loss:\n",
      "0.19296128505904578\n",
      "mse:\n",
      "0.38592257011809156\n",
      "\n",
      "loss:\n",
      "0.19295742621976927\n",
      "mse:\n",
      "0.38591485243953855\n",
      "\n",
      "loss:\n",
      "0.19295365383104576\n",
      "mse:\n",
      "0.3859073076620915\n",
      "\n",
      "loss:\n",
      "0.1929497698145235\n",
      "mse:\n",
      "0.385899539629047\n",
      "\n",
      "loss:\n",
      "0.1929459078246017\n",
      "mse:\n",
      "0.3858918156492034\n",
      "\n",
      "loss:\n",
      "0.19294215551241561\n",
      "mse:\n",
      "0.38588431102483123\n",
      "\n",
      "loss:\n",
      "0.19293832703134303\n",
      "mse:\n",
      "0.38587665406268606\n",
      "\n",
      "loss:\n",
      "0.19293456100128373\n",
      "mse:\n",
      "0.38586912200256746\n",
      "\n",
      "loss:\n",
      "0.19293087310755141\n",
      "mse:\n",
      "0.38586174621510283\n",
      "\n",
      "loss:\n",
      "0.19292705233646767\n",
      "mse:\n",
      "0.38585410467293535\n",
      "\n",
      "loss:\n",
      "0.19292323966613595\n",
      "mse:\n",
      "0.3858464793322719\n",
      "\n",
      "loss:\n",
      "0.1929195176214183\n",
      "mse:\n",
      "0.3858390352428366\n",
      "\n",
      "loss:\n",
      "0.19291568473875031\n",
      "mse:\n",
      "0.38583136947750063\n",
      "\n",
      "loss:\n",
      "0.19291184579443701\n",
      "mse:\n",
      "0.38582369158887403\n",
      "\n",
      "loss:\n",
      "0.19290807423561754\n",
      "mse:\n",
      "0.3858161484712351\n",
      "\n",
      "loss:\n",
      "0.19290424461701733\n",
      "mse:\n",
      "0.38580848923403466\n",
      "\n",
      "loss:\n",
      "0.19290042496483412\n",
      "mse:\n",
      "0.38580084992966823\n",
      "\n",
      "loss:\n",
      "0.19289668660975334\n",
      "mse:\n",
      "0.3857933732195067\n",
      "\n",
      "loss:\n",
      "0.19289285607625173\n",
      "mse:\n",
      "0.38578571215250346\n",
      "\n",
      "loss:\n",
      "0.1928889605185469\n",
      "mse:\n",
      "0.3857779210370938\n",
      "\n",
      "loss:\n",
      "0.19288503835843104\n",
      "mse:\n",
      "0.3857700767168621\n",
      "\n",
      "loss:\n",
      "0.1928810384643826\n",
      "mse:\n",
      "0.3857620769287652\n",
      "\n",
      "loss:\n",
      "0.19287701757439854\n",
      "mse:\n",
      "0.3857540351487971\n",
      "\n",
      "loss:\n",
      "0.19287307943503124\n",
      "mse:\n",
      "0.38574615887006247\n",
      "Annealing learning rate. New rate is 0.0034215006808876397\n",
      "\n",
      "loss:\n",
      "0.1928694203881247\n",
      "mse:\n",
      "0.3857388407762494\n",
      "\n",
      "loss:\n",
      "0.19286579438241075\n",
      "mse:\n",
      "0.3857315887648215\n",
      "\n",
      "loss:\n",
      "0.19286224136592653\n",
      "mse:\n",
      "0.38572448273185306\n",
      "\n",
      "loss:\n",
      "0.1928585893959168\n",
      "mse:\n",
      "0.3857171787918336\n",
      "\n",
      "loss:\n",
      "0.1928549134781153\n",
      "mse:\n",
      "0.3857098269562306\n",
      "\n",
      "loss:\n",
      "0.1928513541189829\n",
      "mse:\n",
      "0.3857027082379658\n",
      "\n",
      "loss:\n",
      "0.19284769893960235\n",
      "mse:\n",
      "0.3856953978792047\n",
      "\n",
      "loss:\n",
      "0.19284403074107567\n",
      "mse:\n",
      "0.38568806148215135\n",
      "\n",
      "loss:\n",
      "0.19284040074578088\n",
      "mse:\n",
      "0.38568080149156175\n",
      "\n",
      "loss:\n",
      "0.1928367289020242\n",
      "mse:\n",
      "0.3856734578040484\n",
      "\n",
      "loss:\n",
      "0.1928330573416202\n",
      "mse:\n",
      "0.3856661146832404\n",
      "\n",
      "loss:\n",
      "0.1928294647247809\n",
      "mse:\n",
      "0.3856589294495618\n",
      "\n",
      "loss:\n",
      "0.19282575972311247\n",
      "mse:\n",
      "0.38565151944622494\n",
      "\n",
      "loss:\n",
      "0.19282217838519686\n",
      "mse:\n",
      "0.3856443567703937\n",
      "\n",
      "loss:\n",
      "0.1928185373397533\n",
      "mse:\n",
      "0.3856370746795066\n",
      "\n",
      "loss:\n",
      "0.19281488804627453\n",
      "mse:\n",
      "0.38562977609254906\n",
      "\n",
      "loss:\n",
      "0.19281129431301836\n",
      "mse:\n",
      "0.3856225886260367\n",
      "\n",
      "loss:\n",
      "0.192807670054178\n",
      "mse:\n",
      "0.385615340108356\n",
      "\n",
      "loss:\n",
      "0.1928040738041669\n",
      "mse:\n",
      "0.3856081476083338\n",
      "\n",
      "loss:\n",
      "0.19280057214159343\n",
      "mse:\n",
      "0.38560114428318687\n",
      "\n",
      "loss:\n",
      "0.19279695008424394\n",
      "mse:\n",
      "0.3855939001684879\n",
      "\n",
      "loss:\n",
      "0.19279338099123267\n",
      "mse:\n",
      "0.38558676198246533\n",
      "\n",
      "loss:\n",
      "0.19278990260433146\n",
      "mse:\n",
      "0.3855798052086629\n",
      "\n",
      "loss:\n",
      "0.19278635548847056\n",
      "mse:\n",
      "0.3855727109769411\n",
      "\n",
      "loss:\n",
      "0.1927827457728184\n",
      "mse:\n",
      "0.3855654915456368\n",
      "\n",
      "loss:\n",
      "0.19277926312574312\n",
      "mse:\n",
      "0.38555852625148623\n",
      "Annealing learning rate. New rate is 0.0030793506127988757\n",
      "\n",
      "loss:\n",
      "0.19277604004839216\n",
      "mse:\n",
      "0.3855520800967843\n",
      "\n",
      "loss:\n",
      "0.19277283131546774\n",
      "mse:\n",
      "0.3855456626309355\n",
      "\n",
      "loss:\n",
      "0.19276955197886889\n",
      "mse:\n",
      "0.38553910395773777\n",
      "\n",
      "loss:\n",
      "0.19276637354222206\n",
      "mse:\n",
      "0.3855327470844441\n",
      "\n",
      "loss:\n",
      "0.192763121151946\n",
      "mse:\n",
      "0.385526242303892\n",
      "\n",
      "loss:\n",
      "0.19275987050406268\n",
      "mse:\n",
      "0.38551974100812536\n",
      "\n",
      "loss:\n",
      "0.19275665579005502\n",
      "mse:\n",
      "0.38551331158011004\n",
      "\n",
      "loss:\n",
      "0.19275342203549495\n",
      "mse:\n",
      "0.3855068440709899\n",
      "\n",
      "loss:\n",
      "0.19275026284682423\n",
      "mse:\n",
      "0.38550052569364845\n",
      "\n",
      "loss:\n",
      "0.1927470084601522\n",
      "mse:\n",
      "0.3854940169203044\n",
      "\n",
      "loss:\n",
      "0.1927437415568805\n",
      "mse:\n",
      "0.385487483113761\n",
      "\n",
      "loss:\n",
      "0.19274095730751786\n",
      "mse:\n",
      "0.3854819146150357\n",
      "\n",
      "loss:\n",
      "0.19273811649016426\n",
      "mse:\n",
      "0.3854762329803285\n",
      "\n",
      "loss:\n",
      "0.19273525278746592\n",
      "mse:\n",
      "0.38547050557493184\n",
      "\n",
      "loss:\n",
      "0.19273240909230005\n",
      "mse:\n",
      "0.3854648181846001\n",
      "\n",
      "loss:\n",
      "0.19272962199631305\n",
      "mse:\n",
      "0.3854592439926261\n",
      "\n",
      "loss:\n",
      "0.19272674918671664\n",
      "mse:\n",
      "0.3854534983734333\n",
      "\n",
      "loss:\n",
      "0.19272385180084992\n",
      "mse:\n",
      "0.38544770360169983\n",
      "\n",
      "loss:\n",
      "0.1927210566323599\n",
      "mse:\n",
      "0.3854421132647198\n",
      "\n",
      "loss:\n",
      "0.19271818152712505\n",
      "mse:\n",
      "0.3854363630542501\n",
      "\n",
      "loss:\n",
      "0.19271528153577708\n",
      "mse:\n",
      "0.38543056307155416\n",
      "\n",
      "loss:\n",
      "0.19271245467770914\n",
      "mse:\n",
      "0.3854249093554183\n",
      "\n",
      "loss:\n",
      "0.1927095705808536\n",
      "mse:\n",
      "0.3854191411617072\n",
      "\n",
      "loss:\n",
      "0.1927066603946874\n",
      "mse:\n",
      "0.3854133207893748\n",
      "\n",
      "loss:\n",
      "0.192703806177977\n",
      "mse:\n",
      "0.385407612355954\n",
      "\n",
      "loss:\n",
      "0.19270092031630895\n",
      "mse:\n",
      "0.3854018406326179\n",
      "Annealing learning rate. New rate is 0.002771415551518988\n",
      "\n",
      "loss:\n",
      "0.19269833669290906\n",
      "mse:\n",
      "0.3853966733858181\n",
      "\n",
      "loss:\n",
      "0.19269570676660508\n",
      "mse:\n",
      "0.38539141353321016\n",
      "\n",
      "loss:\n",
      "0.1926931990084083\n",
      "mse:\n",
      "0.3853863980168166\n",
      "\n",
      "loss:\n",
      "0.1926906134232625\n",
      "mse:\n",
      "0.385381226846525\n",
      "\n",
      "loss:\n",
      "0.19268802567723256\n",
      "mse:\n",
      "0.3853760513544651\n",
      "\n",
      "loss:\n",
      "0.1926854793122328\n",
      "mse:\n",
      "0.3853709586244656\n",
      "\n",
      "loss:\n",
      "0.19268290665959045\n",
      "mse:\n",
      "0.3853658133191809\n",
      "\n",
      "loss:\n",
      "0.19268032696571535\n",
      "mse:\n",
      "0.3853606539314307\n",
      "\n",
      "loss:\n",
      "0.19267777103777345\n",
      "mse:\n",
      "0.3853555420755469\n",
      "\n",
      "loss:\n",
      "0.19267517521293603\n",
      "mse:\n",
      "0.38535035042587207\n",
      "\n",
      "loss:\n",
      "0.1926725948916214\n",
      "mse:\n",
      "0.3853451897832428\n",
      "\n",
      "loss:\n",
      "0.19266999709477145\n",
      "mse:\n",
      "0.3853399941895429\n",
      "\n",
      "loss:\n",
      "0.19266741510495441\n",
      "mse:\n",
      "0.38533483020990883\n",
      "\n",
      "loss:\n",
      "0.19266480662585503\n",
      "mse:\n",
      "0.38532961325171006\n",
      "\n",
      "loss:\n",
      "0.19266218747328562\n",
      "mse:\n",
      "0.38532437494657124\n",
      "\n",
      "loss:\n",
      "0.19265960712157632\n",
      "mse:\n",
      "0.38531921424315263\n",
      "\n",
      "loss:\n",
      "0.19265698921237814\n",
      "mse:\n",
      "0.3853139784247563\n",
      "\n",
      "loss:\n",
      "0.19265437360482254\n",
      "mse:\n",
      "0.3853087472096451\n",
      "\n",
      "loss:\n",
      "0.19265180709709195\n",
      "mse:\n",
      "0.3853036141941839\n",
      "\n",
      "loss:\n",
      "0.19264914880747006\n",
      "mse:\n",
      "0.3852982976149401\n",
      "\n",
      "loss:\n",
      "0.19264650416375775\n",
      "mse:\n",
      "0.3852930083275155\n",
      "\n",
      "loss:\n",
      "0.19264386013752519\n",
      "mse:\n",
      "0.38528772027505037\n",
      "\n",
      "loss:\n",
      "0.1926412515703734\n",
      "mse:\n",
      "0.3852825031407468\n",
      "\n",
      "loss:\n",
      "0.19263862905153203\n",
      "mse:\n",
      "0.38527725810306407\n",
      "\n",
      "loss:\n",
      "0.19263598042892444\n",
      "mse:\n",
      "0.38527196085784887\n",
      "\n",
      "loss:\n",
      "0.19263338343701314\n",
      "mse:\n",
      "0.3852667668740263\n",
      "Annealing learning rate. New rate is 0.0024942739963670895\n",
      "\n",
      "loss:\n",
      "0.19263094153635515\n",
      "mse:\n",
      "0.3852618830727103\n",
      "\n",
      "loss:\n",
      "0.19262856382582827\n",
      "mse:\n",
      "0.38525712765165654\n",
      "\n",
      "loss:\n",
      "0.19262617499811874\n",
      "mse:\n",
      "0.3852523499962375\n",
      "\n",
      "loss:\n",
      "0.19262381807668869\n",
      "mse:\n",
      "0.38524763615337737\n",
      "\n",
      "loss:\n",
      "0.1926214366337297\n",
      "mse:\n",
      "0.3852428732674594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss:\n",
      "0.1926190629920726\n",
      "mse:\n",
      "0.3852381259841452\n",
      "\n",
      "loss:\n",
      "0.1926167426730931\n",
      "mse:\n",
      "0.3852334853461862\n",
      "\n",
      "loss:\n",
      "0.19261433299668415\n",
      "mse:\n",
      "0.3852286659933683\n",
      "\n",
      "loss:\n",
      "0.1926119567495638\n",
      "mse:\n",
      "0.3852239134991276\n",
      "\n",
      "loss:\n",
      "0.19260958085159396\n",
      "mse:\n",
      "0.38521916170318793\n",
      "\n",
      "loss:\n",
      "0.19260723098368807\n",
      "mse:\n",
      "0.38521446196737613\n",
      "\n",
      "loss:\n",
      "0.1926048373726862\n",
      "mse:\n",
      "0.3852096747453724\n",
      "\n",
      "loss:\n",
      "0.19260246118695146\n",
      "mse:\n",
      "0.3852049223739029\n",
      "\n",
      "loss:\n",
      "0.19260013423925926\n",
      "mse:\n",
      "0.3852002684785185\n",
      "\n",
      "loss:\n",
      "0.19259772106265566\n",
      "mse:\n",
      "0.3851954421253113\n",
      "\n",
      "loss:\n",
      "0.19259533993269287\n",
      "mse:\n",
      "0.38519067986538574\n",
      "\n",
      "loss:\n",
      "0.19259294470091587\n",
      "mse:\n",
      "0.38518588940183174\n",
      "\n",
      "loss:\n",
      "0.19259058309212798\n",
      "mse:\n",
      "0.38518116618425596\n",
      "\n",
      "loss:\n",
      "0.19258817666410852\n",
      "mse:\n",
      "0.38517635332821704\n",
      "\n",
      "loss:\n",
      "0.19258576044175568\n",
      "mse:\n",
      "0.38517152088351136\n",
      "\n",
      "loss:\n",
      "0.1925834035334543\n",
      "mse:\n",
      "0.3851668070669086\n",
      "\n",
      "loss:\n",
      "0.19258095810746206\n",
      "mse:\n",
      "0.3851619162149241\n",
      "\n",
      "loss:\n",
      "0.19257854939863364\n",
      "mse:\n",
      "0.3851570987972673\n",
      "\n",
      "loss:\n",
      "0.19257611981256692\n",
      "mse:\n",
      "0.38515223962513384\n",
      "\n",
      "loss:\n",
      "0.19257373453744833\n",
      "mse:\n",
      "0.38514746907489666\n",
      "\n",
      "loss:\n",
      "0.19257130077973372\n",
      "mse:\n",
      "0.38514260155946745\n",
      "Annealing learning rate. New rate is 0.002244846596730381\n",
      "\n",
      "loss:\n",
      "0.1925691176933007\n",
      "mse:\n",
      "0.3851382353866014\n",
      "\n",
      "loss:\n",
      "0.19256699132392704\n",
      "mse:\n",
      "0.3851339826478541\n",
      "\n",
      "loss:\n",
      "0.1925647922027764\n",
      "mse:\n",
      "0.3851295844055528\n",
      "\n",
      "loss:\n",
      "0.19256261177210007\n",
      "mse:\n",
      "0.38512522354420015\n",
      "\n",
      "loss:\n",
      "0.1925604195715081\n",
      "mse:\n",
      "0.3851208391430162\n",
      "\n",
      "loss:\n",
      "0.1925582889096693\n",
      "mse:\n",
      "0.3851165778193386\n",
      "\n",
      "loss:\n",
      "0.1925560941820837\n",
      "mse:\n",
      "0.3851121883641674\n",
      "\n",
      "loss:\n",
      "0.19255392752426254\n",
      "mse:\n",
      "0.38510785504852507\n",
      "\n",
      "loss:\n",
      "0.19255178808380147\n",
      "mse:\n",
      "0.38510357616760293\n",
      "\n",
      "loss:\n",
      "0.19254956625139252\n",
      "mse:\n",
      "0.38509913250278505\n",
      "\n",
      "loss:\n",
      "0.1925473762756469\n",
      "mse:\n",
      "0.3850947525512938\n",
      "\n",
      "loss:\n",
      "0.1925452491866615\n",
      "mse:\n",
      "0.385090498373323\n",
      "\n",
      "loss:\n",
      "0.19254307178291988\n",
      "mse:\n",
      "0.38508614356583976\n",
      "\n",
      "loss:\n",
      "0.1925408708137783\n",
      "mse:\n",
      "0.3850817416275566\n",
      "\n",
      "loss:\n",
      "0.1925386894540955\n",
      "mse:\n",
      "0.385077378908191\n",
      "\n",
      "loss:\n",
      "0.19253654114808905\n",
      "mse:\n",
      "0.3850730822961781\n",
      "\n",
      "loss:\n",
      "0.1925342973357551\n",
      "mse:\n",
      "0.3850685946715102\n",
      "\n",
      "loss:\n",
      "0.19253211379034676\n",
      "mse:\n",
      "0.38506422758069353\n",
      "\n",
      "loss:\n",
      "0.19252993446232478\n",
      "mse:\n",
      "0.38505986892464955\n",
      "\n",
      "loss:\n",
      "0.19252770412145273\n",
      "mse:\n",
      "0.38505540824290546\n",
      "\n",
      "loss:\n",
      "0.19252543393570343\n",
      "mse:\n",
      "0.38505086787140685\n",
      "\n",
      "loss:\n",
      "0.19252320020524508\n",
      "mse:\n",
      "0.38504640041049015\n",
      "\n",
      "loss:\n",
      "0.19252100552838977\n",
      "mse:\n",
      "0.38504201105677954\n",
      "\n",
      "loss:\n",
      "0.19251873710788925\n",
      "mse:\n",
      "0.3850374742157785\n",
      "\n",
      "loss:\n",
      "0.19251649173506025\n",
      "mse:\n",
      "0.3850329834701205\n",
      "\n",
      "loss:\n",
      "0.1925143152181796\n",
      "mse:\n",
      "0.3850286304363592\n",
      "Annealing learning rate. New rate is 0.0020203619370573428\n",
      "\n",
      "loss:\n",
      "0.19251230184566567\n",
      "mse:\n",
      "0.38502460369133135\n",
      "\n",
      "loss:\n",
      "0.19251027507314547\n",
      "mse:\n",
      "0.38502055014629094\n",
      "\n",
      "loss:\n",
      "0.19250825375265754\n",
      "mse:\n",
      "0.3850165075053151\n",
      "\n",
      "loss:\n",
      "0.192506286048631\n",
      "mse:\n",
      "0.385012572097262\n",
      "\n",
      "loss:\n",
      "0.19250425634436397\n",
      "mse:\n",
      "0.38500851268872793\n",
      "\n",
      "loss:\n",
      "0.1925022328662319\n",
      "mse:\n",
      "0.3850044657324638\n",
      "\n",
      "loss:\n",
      "0.1925002124665513\n",
      "mse:\n",
      "0.3850004249331026\n",
      "\n",
      "loss:\n",
      "0.19249822888598495\n",
      "mse:\n",
      "0.3849964577719699\n",
      "\n",
      "loss:\n",
      "0.19249619917285174\n",
      "mse:\n",
      "0.3849923983457035\n",
      "\n",
      "loss:\n",
      "0.19249419298668352\n",
      "mse:\n",
      "0.38498838597336704\n",
      "\n",
      "loss:\n",
      "0.19249223167860394\n",
      "mse:\n",
      "0.3849844633572079\n",
      "\n",
      "loss:\n",
      "0.192490218010234\n",
      "mse:\n",
      "0.384980436020468\n",
      "\n",
      "loss:\n",
      "0.19248821308235664\n",
      "mse:\n",
      "0.3849764261647133\n",
      "\n",
      "loss:\n",
      "0.19248625499953728\n",
      "mse:\n",
      "0.38497250999907456\n",
      "\n",
      "loss:\n",
      "0.19248426161585927\n",
      "mse:\n",
      "0.38496852323171854\n",
      "\n",
      "loss:\n",
      "0.1924822553705988\n",
      "mse:\n",
      "0.3849645107411976\n",
      "\n",
      "loss:\n",
      "0.19248024907934197\n",
      "mse:\n",
      "0.38496049815868394\n",
      "\n",
      "loss:\n",
      "0.19247829566276214\n",
      "mse:\n",
      "0.3849565913255243\n",
      "\n",
      "loss:\n",
      "0.19247627740846002\n",
      "mse:\n",
      "0.38495255481692003\n",
      "\n",
      "loss:\n",
      "0.19247427631501926\n",
      "mse:\n",
      "0.3849485526300385\n",
      "\n",
      "loss:\n",
      "0.19247231609152865\n",
      "mse:\n",
      "0.3849446321830573\n",
      "\n",
      "loss:\n",
      "0.1924702983192778\n",
      "mse:\n",
      "0.3849405966385556\n",
      "\n",
      "loss:\n",
      "0.19246826954251686\n",
      "mse:\n",
      "0.3849365390850337\n",
      "\n",
      "loss:\n",
      "0.1924662651149631\n",
      "mse:\n",
      "0.3849325302299262\n",
      "\n",
      "loss:\n",
      "0.19246430006185491\n",
      "mse:\n",
      "0.38492860012370983\n",
      "\n",
      "loss:\n",
      "0.1924622785054593\n",
      "mse:\n",
      "0.3849245570109186\n",
      "Annealing learning rate. New rate is 0.0018183257433516085\n",
      "\n",
      "loss:\n",
      "0.19246046834082198\n",
      "mse:\n",
      "0.38492093668164395\n",
      "\n",
      "loss:\n",
      "0.19245866824698807\n",
      "mse:\n",
      "0.38491733649397614\n",
      "\n",
      "loss:\n",
      "0.19245690336542842\n",
      "mse:\n",
      "0.38491380673085684\n",
      "\n",
      "loss:\n",
      "0.19245508231345831\n",
      "mse:\n",
      "0.38491016462691663\n",
      "\n",
      "loss:\n",
      "0.19245327038283477\n",
      "mse:\n",
      "0.38490654076566955\n",
      "\n",
      "loss:\n",
      "0.19245150539632133\n",
      "mse:\n",
      "0.38490301079264266\n",
      "\n",
      "loss:\n",
      "0.19244969933383746\n",
      "mse:\n",
      "0.3848993986676749\n",
      "\n",
      "loss:\n",
      "0.19244787461905327\n",
      "mse:\n",
      "0.38489574923810654\n",
      "\n",
      "loss:\n",
      "0.1924461134217808\n",
      "mse:\n",
      "0.3848922268435616\n",
      "\n",
      "loss:\n",
      "0.1924443050526402\n",
      "mse:\n",
      "0.3848886101052804\n",
      "\n",
      "loss:\n",
      "0.1924424880816696\n",
      "mse:\n",
      "0.3848849761633392\n",
      "\n",
      "loss:\n",
      "0.19244068509590798\n",
      "mse:\n",
      "0.38488137019181595\n",
      "\n",
      "loss:\n",
      "0.1924389275639679\n",
      "mse:\n",
      "0.3848778551279358\n",
      "\n",
      "loss:\n",
      "0.19243714157705133\n",
      "mse:\n",
      "0.38487428315410266\n",
      "\n",
      "loss:\n",
      "0.19243533134130134\n",
      "mse:\n",
      "0.38487066268260267\n",
      "\n",
      "loss:\n",
      "0.1924335758899902\n",
      "mse:\n",
      "0.3848671517799804\n",
      "\n",
      "loss:\n",
      "0.19243177759391614\n",
      "mse:\n",
      "0.3848635551878323\n",
      "\n",
      "loss:\n",
      "0.19242997587217206\n",
      "mse:\n",
      "0.3848599517443441\n",
      "\n",
      "loss:\n",
      "0.19242815279398323\n",
      "mse:\n",
      "0.38485630558796646\n",
      "\n",
      "loss:\n",
      "0.19242639475432116\n",
      "mse:\n",
      "0.3848527895086423\n",
      "\n",
      "loss:\n",
      "0.19242459412536495\n",
      "mse:\n",
      "0.3848491882507299\n",
      "\n",
      "loss:\n",
      "0.1924227732427559\n",
      "mse:\n",
      "0.3848455464855118\n",
      "\n",
      "loss:\n",
      "0.19242100163278106\n",
      "mse:\n",
      "0.3848420032655621\n",
      "\n",
      "loss:\n",
      "0.19241917548359247\n",
      "mse:\n",
      "0.38483835096718494\n",
      "\n",
      "loss:\n",
      "0.19241734355172066\n",
      "mse:\n",
      "0.3848346871034413\n",
      "\n",
      "loss:\n",
      "0.19241549715210407\n",
      "mse:\n",
      "0.38483099430420814\n",
      "Annealing learning rate. New rate is 0.0016364931690164477\n",
      "\n",
      "loss:\n",
      "0.19241388124221848\n",
      "mse:\n",
      "0.38482776248443695\n",
      "\n",
      "loss:\n",
      "0.19241222636738073\n",
      "mse:\n",
      "0.38482445273476146\n",
      "\n",
      "loss:\n",
      "0.1924105576226311\n",
      "mse:\n",
      "0.3848211152452622\n",
      "\n",
      "loss:\n",
      "0.19240894044899332\n",
      "mse:\n",
      "0.38481788089798663\n",
      "\n",
      "loss:\n",
      "0.19240730099482145\n",
      "mse:\n",
      "0.3848146019896429\n",
      "\n",
      "loss:\n",
      "0.1924056408481249\n",
      "mse:\n",
      "0.3848112816962498\n",
      "\n",
      "loss:\n",
      "0.19240395244342595\n",
      "mse:\n",
      "0.3848079048868519\n",
      "\n",
      "loss:\n",
      "0.1924023176341144\n",
      "mse:\n",
      "0.3848046352682288\n",
      "\n",
      "loss:\n",
      "0.19240064797351916\n",
      "mse:\n",
      "0.3848012959470383\n",
      "\n",
      "loss:\n",
      "0.1923989719320308\n",
      "mse:\n",
      "0.3847979438640616\n",
      "\n",
      "loss:\n",
      "0.1923973050073077\n",
      "mse:\n",
      "0.3847946100146154\n",
      "\n",
      "loss:\n",
      "0.19239563079391148\n",
      "mse:\n",
      "0.38479126158782295\n",
      "\n",
      "loss:\n",
      "0.19239395761435718\n",
      "mse:\n",
      "0.38478791522871436\n",
      "\n",
      "loss:\n",
      "0.1923922630117086\n",
      "mse:\n",
      "0.3847845260234172\n",
      "\n",
      "loss:\n",
      "0.19239061121487325\n",
      "mse:\n",
      "0.3847812224297465\n",
      "\n",
      "loss:\n",
      "0.19238892147087994\n",
      "mse:\n",
      "0.3847778429417599\n",
      "\n",
      "loss:\n",
      "0.1923872314705862\n",
      "mse:\n",
      "0.3847744629411724\n",
      "\n",
      "loss:\n",
      "0.1923855626493154\n",
      "mse:\n",
      "0.3847711252986308\n",
      "\n",
      "loss:\n",
      "0.19238387441618665\n",
      "mse:\n",
      "0.3847677488323733\n",
      "\n",
      "loss:\n",
      "0.19238218730035753\n",
      "mse:\n",
      "0.38476437460071505\n",
      "\n",
      "loss:\n",
      "0.19238050245092353\n",
      "mse:\n",
      "0.38476100490184706\n",
      "\n",
      "loss:\n",
      "0.19237884161636087\n",
      "mse:\n",
      "0.38475768323272175\n",
      "\n",
      "loss:\n",
      "0.19237715800503938\n",
      "mse:\n",
      "0.38475431601007876\n",
      "\n",
      "loss:\n",
      "0.1923754721158131\n",
      "mse:\n",
      "0.3847509442316262\n",
      "\n",
      "loss:\n",
      "0.19237381984839141\n",
      "mse:\n",
      "0.38474763969678283\n",
      "\n",
      "loss:\n",
      "0.19237214892490337\n",
      "mse:\n",
      "0.38474429784980674\n",
      "Annealing learning rate. New rate is 0.001472843852114803\n",
      "\n",
      "loss:\n",
      "0.1923706392719364\n",
      "mse:\n",
      "0.3847412785438728\n",
      "\n",
      "loss:\n",
      "0.19236913797605099\n",
      "mse:\n",
      "0.38473827595210197\n",
      "\n",
      "loss:\n",
      "0.19236764151687843\n",
      "mse:\n",
      "0.38473528303375687\n",
      "\n",
      "loss:\n",
      "0.19236613302411554\n",
      "mse:\n",
      "0.3847322660482311\n",
      "\n",
      "loss:\n",
      "0.19236462536606627\n",
      "mse:\n",
      "0.38472925073213254\n",
      "\n",
      "loss:\n",
      "0.19236315101059479\n",
      "mse:\n",
      "0.38472630202118957\n",
      "\n",
      "loss:\n",
      "0.19236162251311392\n",
      "mse:\n",
      "0.38472324502622784\n",
      "\n",
      "loss:\n",
      "0.19236011446978904\n",
      "mse:\n",
      "0.3847202289395781\n",
      "\n",
      "loss:\n",
      "0.19235862506959958\n",
      "mse:\n",
      "0.38471725013919916\n",
      "\n",
      "loss:\n",
      "0.19235717276094422\n",
      "mse:\n",
      "0.38471434552188843\n",
      "\n",
      "loss:\n",
      "0.19235566935901138\n",
      "mse:\n",
      "0.38471133871802277\n",
      "\n",
      "loss:\n",
      "0.19235417511901132\n",
      "mse:\n",
      "0.38470835023802263\n",
      "\n",
      "loss:\n",
      "0.19235271904332046\n",
      "mse:\n",
      "0.3847054380866409\n",
      "\n",
      "loss:\n",
      "0.1923512210187122\n",
      "mse:\n",
      "0.3847024420374244\n",
      "\n",
      "loss:\n",
      "0.1923497310820646\n",
      "mse:\n",
      "0.3846994621641292\n",
      "\n",
      "loss:\n",
      "0.19234823570069765\n",
      "mse:\n",
      "0.3846964714013953\n",
      "\n",
      "loss:\n",
      "0.19234675027326437\n",
      "mse:\n",
      "0.38469350054652873\n",
      "\n",
      "loss:\n",
      "0.19234522014689376\n",
      "mse:\n",
      "0.3846904402937875\n",
      "\n",
      "loss:\n",
      "0.19234370419703273\n",
      "mse:\n",
      "0.38468740839406546\n",
      "\n",
      "loss:\n",
      "0.19234222297888245\n",
      "mse:\n",
      "0.3846844459577649\n",
      "\n",
      "loss:\n",
      "0.19234068922287278\n",
      "mse:\n",
      "0.38468137844574557\n",
      "\n",
      "loss:\n",
      "0.19233918615506673\n",
      "mse:\n",
      "0.38467837231013347\n",
      "\n",
      "loss:\n",
      "0.1923376849472763\n",
      "mse:\n",
      "0.3846753698945526\n",
      "\n",
      "loss:\n",
      "0.19233623494654048\n",
      "mse:\n",
      "0.38467246989308096\n",
      "\n",
      "loss:\n",
      "0.19233473554501987\n",
      "mse:\n",
      "0.38466947109003974\n",
      "\n",
      "loss:\n",
      "0.19233324593152443\n",
      "mse:\n",
      "0.38466649186304885\n",
      "Annealing learning rate. New rate is 0.0013255594669033227\n",
      "\n",
      "loss:\n",
      "0.192331937153171\n",
      "mse:\n",
      "0.384663874306342\n",
      "\n",
      "loss:\n",
      "0.1923306003379054\n",
      "mse:\n",
      "0.3846612006758108\n",
      "\n",
      "loss:\n",
      "0.19232925301151463\n",
      "mse:\n",
      "0.38465850602302926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss:\n",
      "0.19232793879949223\n",
      "mse:\n",
      "0.38465587759898445\n",
      "\n",
      "loss:\n",
      "0.19232659772618851\n",
      "mse:\n",
      "0.38465319545237703\n",
      "\n",
      "loss:\n",
      "0.19232524594115855\n",
      "mse:\n",
      "0.3846504918823171\n",
      "\n",
      "loss:\n",
      "0.19232391135541058\n",
      "mse:\n",
      "0.38464782271082115\n",
      "\n",
      "loss:\n",
      "0.1923226031942999\n",
      "mse:\n",
      "0.3846452063885998\n",
      "\n",
      "loss:\n",
      "0.19232124886330806\n",
      "mse:\n",
      "0.38464249772661613\n",
      "\n",
      "loss:\n",
      "0.19231988802020705\n",
      "mse:\n",
      "0.3846397760404141\n",
      "\n",
      "loss:\n",
      "0.19231857377512135\n",
      "mse:\n",
      "0.3846371475502427\n",
      "\n",
      "loss:\n",
      "0.19231722754821828\n",
      "mse:\n",
      "0.38463445509643657\n",
      "\n",
      "loss:\n",
      "0.19231585991319528\n",
      "mse:\n",
      "0.38463171982639055\n",
      "\n",
      "loss:\n",
      "0.19231451373425293\n",
      "mse:\n",
      "0.38462902746850586\n",
      "\n",
      "loss:\n",
      "0.19231319811031605\n",
      "mse:\n",
      "0.3846263962206321\n",
      "\n",
      "loss:\n",
      "0.19231185020236005\n",
      "mse:\n",
      "0.3846237004047201\n",
      "\n",
      "loss:\n",
      "0.1923104833723555\n",
      "mse:\n",
      "0.384620966744711\n",
      "\n",
      "loss:\n",
      "0.19230916594856035\n",
      "mse:\n",
      "0.3846183318971207\n",
      "\n",
      "loss:\n",
      "0.19230781972157077\n",
      "mse:\n",
      "0.38461563944314153\n",
      "\n",
      "loss:\n",
      "0.1923064729182115\n",
      "mse:\n",
      "0.384612945836423\n",
      "\n",
      "loss:\n",
      "0.19230511103357606\n",
      "mse:\n",
      "0.3846102220671521\n",
      "\n",
      "loss:\n",
      "0.19230379029305922\n",
      "mse:\n",
      "0.38460758058611844\n",
      "\n",
      "loss:\n",
      "0.19230244076757144\n",
      "mse:\n",
      "0.3846048815351429\n",
      "\n",
      "loss:\n",
      "0.19230108903351736\n",
      "mse:\n",
      "0.38460217806703473\n",
      "\n",
      "loss:\n",
      "0.19229977751372085\n",
      "mse:\n",
      "0.3845995550274417\n",
      "\n",
      "loss:\n",
      "0.1922984377247755\n",
      "mse:\n",
      "0.384596875449551\n",
      "Annealing learning rate. New rate is 0.0011930035202129905\n",
      "\n",
      "loss:\n",
      "0.19229723268717067\n",
      "mse:\n",
      "0.38459446537434133\n",
      "\n",
      "loss:\n",
      "0.19229604314158003\n",
      "mse:\n",
      "0.38459208628316005\n",
      "\n",
      "loss:\n",
      "0.19229483905533595\n",
      "mse:\n",
      "0.3845896781106719\n",
      "\n",
      "loss:\n",
      "0.19229363465389326\n",
      "mse:\n",
      "0.38458726930778653\n",
      "\n",
      "loss:\n",
      "0.1922924273159501\n",
      "mse:\n",
      "0.3845848546319002\n",
      "\n",
      "loss:\n",
      "0.1922912358740859\n",
      "mse:\n",
      "0.3845824717481718\n",
      "\n",
      "loss:\n",
      "0.19229003019438168\n",
      "mse:\n",
      "0.38458006038876336\n",
      "\n",
      "loss:\n",
      "0.19228882261524413\n",
      "mse:\n",
      "0.38457764523048826\n",
      "\n",
      "loss:\n",
      "0.1922876260809747\n",
      "mse:\n",
      "0.3845752521619494\n",
      "\n",
      "loss:\n",
      "0.19228641981049324\n",
      "mse:\n",
      "0.3845728396209865\n",
      "\n",
      "loss:\n",
      "0.19228521158425943\n",
      "mse:\n",
      "0.38457042316851886\n",
      "\n",
      "loss:\n",
      "0.1922840029294406\n",
      "mse:\n",
      "0.3845680058588812\n",
      "\n",
      "loss:\n",
      "0.19228280606437098\n",
      "mse:\n",
      "0.38456561212874196\n",
      "\n",
      "loss:\n",
      "0.19228159784355695\n",
      "mse:\n",
      "0.3845631956871139\n",
      "\n",
      "loss:\n",
      "0.19228038785168042\n",
      "mse:\n",
      "0.38456077570336084\n",
      "\n",
      "loss:\n",
      "0.19227920367638482\n",
      "mse:\n",
      "0.38455840735276964\n",
      "\n",
      "loss:\n",
      "0.1922779775367286\n",
      "mse:\n",
      "0.3845559550734572\n",
      "\n",
      "loss:\n",
      "0.1922767534399897\n",
      "mse:\n",
      "0.3845535068799794\n",
      "\n",
      "loss:\n",
      "0.19227556420495803\n",
      "mse:\n",
      "0.38455112840991607\n",
      "\n",
      "loss:\n",
      "0.19227433237992414\n",
      "mse:\n",
      "0.3845486647598483\n",
      "\n",
      "loss:\n",
      "0.19227311783718506\n",
      "mse:\n",
      "0.38454623567437013\n",
      "\n",
      "loss:\n",
      "0.19227189366831127\n",
      "mse:\n",
      "0.38454378733662253\n",
      "\n",
      "loss:\n",
      "0.19227070120090192\n",
      "mse:\n",
      "0.38454140240180384\n",
      "\n",
      "loss:\n",
      "0.1922694696678872\n",
      "mse:\n",
      "0.3845389393357744\n",
      "\n",
      "loss:\n",
      "0.19226825269176864\n",
      "mse:\n",
      "0.3845365053835373\n",
      "\n",
      "loss:\n",
      "0.19226705538715966\n",
      "mse:\n",
      "0.3845341107743193\n",
      "Annealing learning rate. New rate is 0.0010737031681916915\n",
      "\n",
      "loss:\n",
      "0.19226595918603814\n",
      "mse:\n",
      "0.3845319183720763\n",
      "\n",
      "loss:\n",
      "0.19226485306457455\n",
      "mse:\n",
      "0.3845297061291491\n",
      "\n",
      "loss:\n",
      "0.1922637576042331\n",
      "mse:\n",
      "0.3845275152084662\n",
      "\n",
      "loss:\n",
      "0.1922626822700569\n",
      "mse:\n",
      "0.3845253645401138\n",
      "\n",
      "loss:\n",
      "0.19226157002245214\n",
      "mse:\n",
      "0.3845231400449043\n",
      "\n",
      "loss:\n",
      "0.1922604734124672\n",
      "mse:\n",
      "0.3845209468249344\n",
      "\n",
      "loss:\n",
      "0.19225940390264298\n",
      "mse:\n",
      "0.38451880780528597\n",
      "\n",
      "loss:\n",
      "0.192258309698822\n",
      "mse:\n",
      "0.384516619397644\n",
      "\n",
      "loss:\n",
      "0.192257204068557\n",
      "mse:\n",
      "0.384514408137114\n",
      "\n",
      "loss:\n",
      "0.1922561051545757\n",
      "mse:\n",
      "0.3845122103091514\n",
      "\n",
      "loss:\n",
      "0.19225503729099322\n",
      "mse:\n",
      "0.38451007458198644\n",
      "\n",
      "loss:\n",
      "0.19225394242636626\n",
      "mse:\n",
      "0.3845078848527325\n",
      "\n",
      "loss:\n",
      "0.19225283934164758\n",
      "mse:\n",
      "0.38450567868329516\n",
      "\n",
      "loss:\n",
      "0.1922517681783101\n",
      "mse:\n",
      "0.3845035363566202\n",
      "\n",
      "loss:\n",
      "0.1922506795481932\n",
      "mse:\n",
      "0.3845013590963864\n",
      "\n",
      "loss:\n",
      "0.19224957645761728\n",
      "mse:\n",
      "0.38449915291523457\n",
      "\n",
      "loss:\n",
      "0.19224851020301126\n",
      "mse:\n",
      "0.3844970204060225\n",
      "\n",
      "loss:\n",
      "0.1922474180598134\n",
      "mse:\n",
      "0.3844948361196268\n",
      "\n",
      "loss:\n",
      "0.1922463227917653\n",
      "mse:\n",
      "0.3844926455835306\n",
      "\n",
      "loss:\n",
      "0.19224521812393566\n",
      "mse:\n",
      "0.3844904362478713\n",
      "\n",
      "loss:\n",
      "0.1922441471617756\n",
      "mse:\n",
      "0.3844882943235512\n",
      "\n",
      "loss:\n",
      "0.1922430529839234\n",
      "mse:\n",
      "0.3844861059678468\n",
      "\n",
      "loss:\n",
      "0.19224196013835632\n",
      "mse:\n",
      "0.38448392027671263\n",
      "\n",
      "loss:\n",
      "0.19224088621388047\n",
      "mse:\n",
      "0.38448177242776094\n",
      "\n",
      "loss:\n",
      "0.19223979992473866\n",
      "mse:\n",
      "0.3844795998494773\n",
      "\n",
      "loss:\n",
      "0.19223870969569085\n",
      "mse:\n",
      "0.3844774193913817\n",
      "Annealing learning rate. New rate is 0.0009663328513725224\n",
      "\n",
      "loss:\n",
      "0.19223772386754076\n",
      "mse:\n",
      "0.3844754477350815\n",
      "\n",
      "loss:\n",
      "0.19223676454430322\n",
      "mse:\n",
      "0.38447352908860644\n",
      "\n",
      "loss:\n",
      "0.19223578378671258\n",
      "mse:\n",
      "0.38447156757342515\n",
      "\n",
      "loss:\n",
      "0.19223480582414113\n",
      "mse:\n",
      "0.38446961164828225\n",
      "\n",
      "loss:\n",
      "0.19223384426412368\n",
      "mse:\n",
      "0.38446768852824736\n",
      "\n",
      "loss:\n",
      "0.19223286778321683\n",
      "mse:\n",
      "0.38446573556643365\n",
      "\n",
      "loss:\n",
      "0.1922318937165192\n",
      "mse:\n",
      "0.3844637874330384\n",
      "\n",
      "loss:\n",
      "0.19223092026763497\n",
      "mse:\n",
      "0.38446184053526994\n",
      "\n",
      "loss:\n",
      "0.19222995723816236\n",
      "mse:\n",
      "0.3844599144763247\n",
      "\n",
      "loss:\n",
      "0.19222898738100122\n",
      "mse:\n",
      "0.38445797476200244\n",
      "\n",
      "loss:\n",
      "0.1922280175408749\n",
      "mse:\n",
      "0.3844560350817498\n",
      "\n",
      "loss:\n",
      "0.19222706002454215\n",
      "mse:\n",
      "0.3844541200490843\n",
      "\n",
      "loss:\n",
      "0.19222608867885368\n",
      "mse:\n",
      "0.38445217735770737\n",
      "\n",
      "loss:\n",
      "0.19222512158609054\n",
      "mse:\n",
      "0.38445024317218107\n",
      "\n",
      "loss:\n",
      "0.19222416896754121\n",
      "mse:\n",
      "0.38444833793508243\n",
      "\n",
      "loss:\n",
      "0.1922231894985593\n",
      "mse:\n",
      "0.3844463789971186\n",
      "\n",
      "loss:\n",
      "0.19222221754281382\n",
      "mse:\n",
      "0.38444443508562764\n",
      "\n",
      "loss:\n",
      "0.19222125018787498\n",
      "mse:\n",
      "0.38444250037574995\n",
      "\n",
      "loss:\n",
      "0.19222030052917438\n",
      "mse:\n",
      "0.38444060105834876\n",
      "\n",
      "loss:\n",
      "0.19221931562707534\n",
      "mse:\n",
      "0.3844386312541507\n",
      "\n",
      "loss:\n",
      "0.19221834401019908\n",
      "mse:\n",
      "0.38443668802039815\n",
      "\n",
      "loss:\n",
      "0.19221739853807307\n",
      "mse:\n",
      "0.38443479707614614\n",
      "\n",
      "loss:\n",
      "0.19221641885434093\n",
      "mse:\n",
      "0.38443283770868186\n",
      "\n",
      "loss:\n",
      "0.1922154434457855\n",
      "mse:\n",
      "0.384430886891571\n",
      "\n",
      "loss:\n",
      "0.19221446807581224\n",
      "mse:\n",
      "0.3844289361516245\n",
      "\n",
      "loss:\n",
      "0.19221351797443578\n",
      "mse:\n",
      "0.38442703594887156\n",
      "Annealing learning rate. New rate is 0.0008696995662352702\n",
      "\n",
      "loss:\n",
      "0.1922126316995367\n",
      "mse:\n",
      "0.3844252633990734\n",
      "\n",
      "loss:\n",
      "0.19221175315019706\n",
      "mse:\n",
      "0.3844235063003941\n",
      "\n",
      "loss:\n",
      "0.1922108914212422\n",
      "mse:\n",
      "0.3844217828424844\n",
      "\n",
      "loss:\n",
      "0.1922100125299533\n",
      "mse:\n",
      "0.3844200250599066\n",
      "\n",
      "loss:\n",
      "0.1922091269480091\n",
      "mse:\n",
      "0.3844182538960182\n",
      "\n",
      "loss:\n",
      "0.1922082474685574\n",
      "mse:\n",
      "0.3844164949371148\n",
      "\n",
      "loss:\n",
      "0.1922073845758959\n",
      "mse:\n",
      "0.3844147691517918\n",
      "\n",
      "loss:\n",
      "0.19220650307055837\n",
      "mse:\n",
      "0.38441300614111673\n",
      "\n",
      "loss:\n",
      "0.19220563037613955\n",
      "mse:\n",
      "0.3844112607522791\n",
      "\n",
      "loss:\n",
      "0.19220477675352377\n",
      "mse:\n",
      "0.38440955350704753\n",
      "\n",
      "loss:\n",
      "0.1922039034015655\n",
      "mse:\n",
      "0.384407806803131\n",
      "\n",
      "loss:\n",
      "0.1922030167474243\n",
      "mse:\n",
      "0.3844060334948486\n",
      "\n",
      "loss:\n",
      "0.19220214574452663\n",
      "mse:\n",
      "0.38440429148905325\n",
      "\n",
      "loss:\n",
      "0.19220129128773786\n",
      "mse:\n",
      "0.3844025825754757\n",
      "\n",
      "loss:\n",
      "0.19220041687485356\n",
      "mse:\n",
      "0.3844008337497071\n",
      "\n",
      "loss:\n",
      "0.19219953563957037\n",
      "mse:\n",
      "0.38439907127914075\n",
      "\n",
      "loss:\n",
      "0.19219868103291743\n",
      "mse:\n",
      "0.38439736206583486\n",
      "\n",
      "loss:\n",
      "0.19219780669197017\n",
      "mse:\n",
      "0.38439561338394035\n",
      "\n",
      "loss:\n",
      "0.19219693396286927\n",
      "mse:\n",
      "0.38439386792573854\n",
      "\n",
      "loss:\n",
      "0.19219606469856734\n",
      "mse:\n",
      "0.3843921293971347\n",
      "\n",
      "loss:\n",
      "0.19219522474752734\n",
      "mse:\n",
      "0.3843904494950547\n",
      "\n",
      "loss:\n",
      "0.19219436649241836\n",
      "mse:\n",
      "0.3843887329848367\n",
      "\n",
      "loss:\n",
      "0.19219349956281698\n",
      "mse:\n",
      "0.38438699912563395\n",
      "\n",
      "loss:\n",
      "0.19219265914102757\n",
      "mse:\n",
      "0.38438531828205513\n",
      "\n",
      "loss:\n",
      "0.19219180034427844\n",
      "mse:\n",
      "0.38438360068855687\n",
      "\n",
      "loss:\n",
      "0.19219093724326664\n",
      "mse:\n",
      "0.38438187448653327\n",
      "Annealing learning rate. New rate is 0.0007827296096117432\n",
      "\n",
      "loss:\n",
      "0.19219017154713278\n",
      "mse:\n",
      "0.38438034309426555\n",
      "\n",
      "loss:\n",
      "0.19218939366884522\n",
      "mse:\n",
      "0.38437878733769043\n",
      "\n",
      "loss:\n",
      "0.19218861501596185\n",
      "mse:\n",
      "0.3843772300319237\n",
      "\n",
      "loss:\n",
      "0.1921878364911841\n",
      "mse:\n",
      "0.3843756729823682\n",
      "\n",
      "loss:\n",
      "0.19218706429331406\n",
      "mse:\n",
      "0.3843741285866281\n",
      "\n",
      "loss:\n",
      "0.1921862859265852\n",
      "mse:\n",
      "0.3843725718531704\n",
      "\n",
      "loss:\n",
      "0.1921855062717758\n",
      "mse:\n",
      "0.3843710125435516\n",
      "\n",
      "loss:\n",
      "0.1921847386838642\n",
      "mse:\n",
      "0.3843694773677284\n",
      "\n",
      "loss:\n",
      "0.19218395461664806\n",
      "mse:\n",
      "0.3843679092332961\n",
      "\n",
      "loss:\n",
      "0.19218317468302426\n",
      "mse:\n",
      "0.3843663493660485\n",
      "\n",
      "loss:\n",
      "0.19218239378704605\n",
      "mse:\n",
      "0.3843647875740921\n",
      "\n",
      "loss:\n",
      "0.19218162323340168\n",
      "mse:\n",
      "0.38436324646680337\n",
      "\n",
      "loss:\n",
      "0.19218084584050843\n",
      "mse:\n",
      "0.38436169168101686\n",
      "\n",
      "loss:\n",
      "0.19218006444083863\n",
      "mse:\n",
      "0.38436012888167725\n",
      "\n",
      "loss:\n",
      "0.19217930431781624\n",
      "mse:\n",
      "0.3843586086356325\n",
      "\n",
      "loss:\n",
      "0.19217852042770803\n",
      "mse:\n",
      "0.38435704085541605\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-defddd631a3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m                       \u001b[0mvalidation_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val_normalize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                       \u001b[0mvalidation_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_val_normalize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                       batch_size = batch)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# RESULTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c0a5c4f3a7c7>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, n_hidden, nodes, activations, lr, validation_X, validation_y, batch_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c0a5c4f3a7c7>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batchy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mbatched_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mvalidation_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_val_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c0a5c4f3a7c7>\u001b[0m in \u001b[0;36m_forward_propagation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mweight_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c0a5c4f3a7c7>\u001b[0m in \u001b[0;36m_activation\u001b[0;34m(self, data, activation)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0mbuff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mbuff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 8x34 (batch = 0)\n",
    "batch = 0\n",
    "width = 34\n",
    "depth = 8\n",
    "\n",
    "INPUT_SIZE = X_train_normalize.shape[1]\n",
    "OUTPUT_SIZE = 1\n",
    "LEARNING_RATE = 3\n",
    "\n",
    "nodes = [INPUT_SIZE] + [width for i in range(depth)] + [OUTPUT_SIZE]\n",
    "activations = [\"relu\" for i in range(len(nodes))]\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "losses, mses = nn.fit(X = X_train_normalize,\n",
    "                      y = y_train_normalize,\n",
    "                      n_hidden = len(nodes) - 2,\n",
    "                      nodes = nodes,\n",
    "                      activations = activations,\n",
    "                      lr = LEARNING_RATE,\n",
    "                      validation_X = X_val_normalize,\n",
    "                      validation_y = y_val_normalize,\n",
    "                      batch_size = batch)\n",
    "\n",
    "# RESULTS\n",
    "## loss = 0.192\n",
    "## mse = 0.383"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10x34 (batch = 0)\n",
    "batch = 0\n",
    "width = 34\n",
    "depth = 10\n",
    "\n",
    "INPUT_SIZE = X_train_normalize.shape[1]\n",
    "OUTPUT_SIZE = 1\n",
    "LEARNING_RATE = 5\n",
    "\n",
    "nodes = [INPUT_SIZE] + [width for i in range(depth)] + [OUTPUT_SIZE]\n",
    "activations = [\"relu\" for i in range(len(nodes))]\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "losses, mses = nn.fit(X = X_train_normalize,\n",
    "                      y = y_train_normalize,\n",
    "                      n_hidden = len(nodes) - 2,\n",
    "                      nodes = nodes,\n",
    "                      activations = activations,\n",
    "                      lr = LEARNING_RATE,\n",
    "                      validation_X = X_val_normalize,\n",
    "                      validation_y = y_val_normalize,\n",
    "                      batch_size = batch)\n",
    "\n",
    "# RESULTS\n",
    "## loss = 0.446\n",
    "## mse = 0.892"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_for_B(x, y, n):\n",
    "    # takes a data frame, and a numpy array\n",
    "    x = np.concatenate((np.ones((n,1)), x.to_numpy()), axis=1)\n",
    "    x_transpose = np.transpose(x)\n",
    "    return np.linalg.solve(np.matmul(x_transpose, x), x_transpose @ y)\n",
    "\n",
    "def get_predicted_values(beta, design_matrix, n):\n",
    "    return np.matmul(np.concatenate((np.ones((n,1)), design_matrix), axis = 1), beta)\n",
    "\n",
    "def BIC(predictions, actuals, d, n):\n",
    "    # numpy array - predictions, numpy array - actual \n",
    "    return (np.square(actuals - predictions).sum()) + (d * np.log(n))\n",
    "\n",
    "def RSquaredAdj(predictions, actuals, d, n):\n",
    "    x1 = np.square(actuals - actuals.mean()).sum()\n",
    "    x2 = np.square(predictions - actuals.mean()).sum()\n",
    "    r2 = x2/x1\n",
    "    return (1 - ((1 - r2) * (n - 1) / (n - d - 1)))\n",
    "\n",
    "def RMSE(predictions, actuals, n):\n",
    "    return np.sqrt((np.square(actuals - predictions).sum()) / n) \n",
    "\n",
    "def train_model(design_matrix, dependent_variable_series):\n",
    "    n = design_matrix.shape[0]\n",
    "    beta = solve_for_B(design_matrix, dependent_variable_series, n)\n",
    "    predicted_values = get_predicted_values(beta, design_matrix, n)\n",
    "    calculated_BIC = BIC(predicted_values, dependent_variable_series, d = design_matrix.shape[1], n = n)\n",
    "    calculated_RMSE = RMSE(predicted_values, dependent_variable_series, n = n)\n",
    "    residuals = predicted_values - dependent_variable_series\n",
    "    return beta, calculated_BIC, calculated_RMSE, residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_crossv_model(df, y): \n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(.25 * m)\n",
    "    train_1 = df.iloc[perm[:train_end]]\n",
    "    test_1 = y.iloc[perm[:train_end]]\n",
    "\n",
    "    train_2 = df.iloc[perm[train_end:train_end*2]]\n",
    "    test_2 = y.iloc[perm[train_end:train_end*2]]\n",
    "\n",
    "    train_3 = df.iloc[perm[train_end*2:train_end*3]]\n",
    "    test_3 = y.iloc[perm[train_end*2:train_end*3]]\n",
    "\n",
    "    train_4 = df.iloc[perm[train_end*3:]]\n",
    "    test_4 = y.iloc[perm[train_end*3:]]\n",
    "    return train_1, test_1, train_2, test_2, train_3, test_3, train_4, test_4\n",
    "\n",
    "\n",
    "def get_average_RMSE_for_one_model(df, y):\n",
    "    train_1, test_1, train_2, test_2, train_3, test_3, train_4, test_4 = train_crossv_model(df, y)\n",
    "\n",
    "    beta4, bic4, RMSE4, _ = train_model(train_1.append([train_2, train_3]), test_1.append([test_2, test_3]))\n",
    "\n",
    "    beta2, bic3,RMSE2, _ = train_model(train_1.append([train_4, train_3]), test_1.append([test_4, test_3]))\n",
    "\n",
    "    beta3, bic2, RMSE3, _ = train_model(train_1.append([train_2, train_4]), test_1.append([test_2, test_4]))\n",
    "\n",
    "    beta1, bic1, RMSE1, _ = train_model(train_4.append([train_2, train_3]), test_4.append([test_2, test_3]))\n",
    "\n",
    "    return sum([RMSE1, RMSE2, RMSE3, RMSE4]) / 4, sum([bic1, bic2, bic3, bic4]) / 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward cross validated stepwise\n",
    "\n",
    "train_data = X_train.copy()\n",
    "df_with_col = pd.DataFrame()\n",
    "\n",
    "#min_RMSE, min_bic = get_average_RMSE_for_one_model(train_data, y_train_sample)\n",
    "\n",
    "col_added = []\n",
    "rmses = []\n",
    "\n",
    "for i in range(0,50):\n",
    "\n",
    "    min_col = None\n",
    "    min_RMSE = 100000000000000000000\n",
    "    min_bic = 10000000000000000000\n",
    "\n",
    "    for col in train_data.columns:\n",
    "\n",
    "        df_with_col[col] = train_data[col]\n",
    "        \n",
    "        new_RMSE, new_bic = get_average_RMSE_for_one_model(df_with_col, y_train)\n",
    "                \n",
    "        if new_RMSE[0] <= min_RMSE:\n",
    "            min_bic = new_bic[0]\n",
    "            min_RMSE = new_RMSE[0]\n",
    "            min_col = col\n",
    "            \n",
    "        df_with_col.drop(columns=[col], inplace=True)\n",
    "\n",
    "    if min_col is not None:\n",
    "        print(min_col)\n",
    "        df_with_col[min_col] = train_data[min_col]\n",
    "        train_data.drop(columns=[min_col], inplace=True)\n",
    "        rmses.append(min_RMSE)\n",
    "        print(\"bic: \" + str(min_bic))\n",
    "        print(\"rmse: \" + str(min_RMSE))\n",
    "        col_added.append(min_col)\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(\"Failed #2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSEs\n",
    "plt.plot(range(0,50), rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
