{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"/datasets/sberbank-russian-housing-market/train.csv\", index_col=0)\n",
    "df_macro = pd.read_csv(\"/datasets/sberbank-russian-housing-market/macro.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping na's and processing ONLY for the forward stepwise\n",
    "X_train.dropna(inplace=True)\n",
    "X_train.reset_index(inplace=True)\n",
    "X_train.drop(columns=[\"id\"], inplace=True)\n",
    "X_train = X_train.select_dtypes(include=np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train[[\"price_doc\"]]\n",
    "X_train = X_train.drop(\"price_doc\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample = X_train[[\"full_sq\", \"life_sq\",\"floor\"]].fillna(0).sample(5000)\n",
    "y_train_sample = y_train.loc[X_train_sample.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entire normalized and cleaned dataset\n",
    "X_train = pd.read_csv(\"../Data/df_train_final\").drop(\"Unnamed: 0\", axis = 1)\n",
    "y_train = pd.read_csv(\"../Data/y_train_final\").drop(\"Unnamed: 0\", axis = 1)\n",
    "X_train_normalize = (X_train - X_train.mean())/X_train.std()\n",
    "y_train_normalize = (y_train - y_train.mean())/y_train.std()\n",
    "\n",
    "X_test = pd.read_csv(\"../Data/df_test_final\").drop(\"Unnamed: 0\", axis = 1)\n",
    "y_test = pd.read_csv(\"../Data/y_test_final\").drop(\"Unnamed: 0\", axis = 1)\n",
    "X_test_normalize = (X_test - X_train.mean())/X_train.std()\n",
    "y_test_normalize = (y_test - y_train.mean())/y_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample = X_test_normalize.sample(frac=0.1)\n",
    "X_test_normalize = X_test_normalize.drop(X_train_sample.index)\n",
    "\n",
    "y_train_sample = y_test_normalize.loc[X_train_sample.index]\n",
    "y_test_normalize = y_test_normalize.drop(X_train_sample.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(): \n",
    "    def fit(self, X, y, n_hidden, nodes, activations, lr, validation_X, validation_y, batch_size = 0):\n",
    "        self._lr = lr\n",
    "        self._X = X.values\n",
    "        self._y = y.values\n",
    "        self._n_hidden = n_hidden\n",
    "        self._nodes = nodes\n",
    "        self._weights = self._generate_weights()\n",
    "        self._biases = self._generate_bias()\n",
    "        self._activations = activations\n",
    "        self._forward_inputs = []\n",
    "        self._val_X = validation_X.values\n",
    "        self._val_y = validation_y.values\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "\n",
    "        return self._train()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pred = X\n",
    "        weights = self._weights\n",
    "        biases = self._biases\n",
    "        activations = self._activations[1:-1]\n",
    "    \n",
    "        for idx, layer in enumerate(weights):\n",
    "            if idx == (len(weights) - 1):\n",
    "                pred = (pred @ layer) + biases[idx].T,\n",
    "            else:\n",
    "                weight_output = (pred @ layer) + biases[idx].T\n",
    "                pred = self._activation(data = weight_output, activation = activations[idx])\n",
    "\n",
    "        return pred[0]\n",
    "        \n",
    "    def _activation(self, data, activation = \"relu\"):\n",
    "        if activation == \"relu\":\n",
    "            def relu(data):\n",
    "                return np.array([max(0,i) for i in data]).reshape(data.shape)\n",
    "            return np.apply_along_axis(relu, 1, data)\n",
    "        if activation == \"sigmoid\":\n",
    "            def sigmoid(data):\n",
    "                return (1/(1 + np.exp(-data))).reshape(data.shape)\n",
    "            return np.apply_along_axis(sigmoid, 1, data)\n",
    "    \n",
    "    def _der_activation(self, points, activation = \"relu\"):\n",
    "        if activation == \"relu\":\n",
    "            def d_relu(point):\n",
    "                return np.array([0 if y <= 0 else 1 for y in point])\n",
    "            return np.apply_along_axis(d_relu, 1, points)\n",
    "        if activation == \"sigmoid\":\n",
    "            ## todo\n",
    "            return\n",
    "    \n",
    "    def _loss_function(self, ypred, loss = \"l2\"):\n",
    "        y = self._val_y\n",
    "        if loss == \"mse\":\n",
    "            return ((ypred - y) ** 2).mean()\n",
    "        if loss == \"l2\":\n",
    "            return (((ypred - y) ** 2)/2).mean()\n",
    "    \n",
    "    def _loss_jacobian(self, ypred, loss = \"l2\"):\n",
    "        if self._batch_size > 0:\n",
    "            y = self._batchy\n",
    "        else:\n",
    "            y = self._y\n",
    "        if loss == \"l2\":\n",
    "            return (ypred - y)/(len(ypred))\n",
    "    \n",
    "    def _generate_weights(self):\n",
    "        hidden_weights = []\n",
    "        nodes = self._nodes\n",
    "        for idx in range(1,len(nodes)):\n",
    "            hidden_weights.append(0.1 * np.random.randn(nodes[idx -1], nodes[idx]))\n",
    "            #hidden_weights.append(0.01 *np.random.randn(nodes[idx -1], nodes[idx]))\n",
    "\n",
    "        return hidden_weights\n",
    "    \n",
    "    def _generate_bias(self):\n",
    "        hidden_layers = []\n",
    "        nodes = self._nodes\n",
    "        for i in range(self._n_hidden + 1):\n",
    "            hidden_layers.append(np.zeros((nodes[i + 1], 1)))\n",
    "        return hidden_layers\n",
    "    \n",
    "    \n",
    "    def _forward_propagation(self):\n",
    "        \"\"\"\n",
    "        Suppose 2 observations\n",
    "        \n",
    "        Suppose previous layer is 3 nodes\n",
    "        Suppose current layer is 2 nodes\n",
    "        \n",
    "        prev shape (2,3)\n",
    "        prev = ob1 [prev_node_1 val, prev_node_2 val, prev_node_3 val]\n",
    "               ob2 [prev_node_1 val, prev_node_2 val, prev_node_3 val]\n",
    "               \n",
    "        layer shape (3,2)\n",
    "        layer = [weight for current_node_1 for prev_node_1, weight for current_node_2 for prev_node_1]\n",
    "                [weight for current_node_1 for prev_node_2, weight for current_node_2 for prev_node_2]\n",
    "                [weight for current_node_1 for prev_node_3, weight for current_node_2 for prev_node_3]\n",
    "                \n",
    "        output shape (2,2) # since 2 observations and 2 layers\n",
    "        output = ob1 [current_node_1 val, current_node_2 val]\n",
    "                 ob2 [current_node_1 val, current_node_2 val]\n",
    "                 \n",
    "        Then for bias in current layer it is (2,1) since 2 nodes in current layer\n",
    "        \n",
    "        So for each row in output we add the bias row wise and apply the activation function to each row\n",
    "        \n",
    "        prev <- ouput\n",
    "        \n",
    "        Move onto next layer...\n",
    "        \"\"\"\n",
    "        if self._batch_size > 0:\n",
    "            prev = self._batchX\n",
    "        else:\n",
    "            prev = self._X\n",
    "        weights = self._weights\n",
    "        biases = self._biases\n",
    "        activations = self._activations[1:-1]\n",
    "    \n",
    "        for idx, layer in enumerate(weights):\n",
    "            if idx == (len(weights) - 1):\n",
    "                self._forward_inputs.append((prev, None))\n",
    "                prev = (prev @ layer) + biases[idx].T,\n",
    "            else:\n",
    "                weight_output = (prev @ layer) + biases[idx].T\n",
    "                self._forward_inputs.append((prev, weight_output))\n",
    "                prev = self._activation(data = weight_output, activation = activations[idx])\n",
    "\n",
    "        return prev\n",
    "    \n",
    "    def _backward_propagation(self, ypred):\n",
    "            \n",
    "        j = self._loss_jacobian(ypred)\n",
    "                \n",
    "        for i in range(len(self._forward_inputs)-1, -1, -1):\n",
    "            if i != (len(self._forward_inputs) - 1):\n",
    "                # activation func on all layers except the last\n",
    "                der_acti = self._der_activation(self._forward_inputs[i][1])\n",
    "                j = np.multiply(j,der_acti)\n",
    "\n",
    "            x = self._forward_inputs[i][0]\n",
    "\n",
    "            jw = x.T.dot(j)\n",
    "\n",
    "            b = np.ones((j.shape[0],1))\n",
    "            jb = j.T.dot(b)\n",
    "            \n",
    "            j = j.dot(self._weights[i].T)\n",
    "            \n",
    "            self._weights[i] -= self._lr * jw\n",
    "            self._biases[i] -= self._lr * jb\n",
    "            \n",
    "        self._forward_inputs = []        \n",
    "    \n",
    "    def _train(self):\n",
    "        min_loss = old_loss = np.inf\n",
    "        losses = []\n",
    "        mses = []\n",
    "        tol = 0.00001\n",
    "        terminate_count = anneal_count = 0\n",
    "        while True:\n",
    "            if self._batch_size > 0:\n",
    "                X_index = np.arange(self._X.shape[0])\n",
    "                np.random.shuffle(X_index)\n",
    "                batch_index = X_index[:self._batch_size]\n",
    "                self._batchX = self._X[batch_index,:]\n",
    "                self._batchy = self._y[batch_index,:]\n",
    "            \n",
    "            batched_out = self._forward_propagation()\n",
    "            validation_out = self.predict(self._val_X)\n",
    "                \n",
    "            loss = self._loss_function(validation_out)\n",
    "            mse = self._loss_function(validation_out, loss = \"mse\")\n",
    "            print(\"\\nloss:\")\n",
    "            print(loss)\n",
    "            print(\"mse:\")\n",
    "            print(mse)\n",
    "            if loss <= min_loss:\n",
    "                min_loss = loss\n",
    "                terminate_count = anneal_count = 0\n",
    "            if loss <= old_loss:\n",
    "                 anneal_count = 0\n",
    "                print(\"INCREASE IN LOSS\")\n",
    "            else:\n",
    "                terminate_count += 1\n",
    "                anneal_count += 1\n",
    "                print(\"INCREASE IN LOSS\")\n",
    "                if anneal_count >= 2:\n",
    "                    anneal_count = 0\n",
    "                    self._lr = self._lr / 2\n",
    "                    print(\"Decreasing learning rate. New rate is \" + str(self._lr))\n",
    "                if terminate_count > 20 or self._lr < tol:\n",
    "                    break\n",
    "            \n",
    "            losses.append(loss)\n",
    "            mses.append(mse)\n",
    "            self._backward_propagation(batched_out[0])\n",
    "            old_loss = loss\n",
    "            \n",
    "        return losses, mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss:\n",
      "0.6522860317227002\n",
      "mse:\n",
      "1.3045720634454003\n",
      "\n",
      "loss:\n",
      "0.5617899533033948\n",
      "mse:\n",
      "1.1235799066067895\n",
      "\n",
      "loss:\n",
      "0.6081912607809744\n",
      "mse:\n",
      "1.2163825215619488\n",
      "INCREASE IN LOSS\n",
      "\n",
      "loss:\n",
      "0.9233543270380938\n",
      "mse:\n",
      "1.8467086540761877\n",
      "INCREASE IN LOSS\n",
      "Decreasing learning rate. New rate is 0.25\n",
      "\n",
      "loss:\n",
      "0.4142724861558191\n",
      "mse:\n",
      "0.8285449723116382\n",
      "\n",
      "loss:\n",
      "0.38737863655968563\n",
      "mse:\n",
      "0.7747572731193713\n",
      "\n",
      "loss:\n",
      "0.36896433063677053\n",
      "mse:\n",
      "0.7379286612735411\n",
      "\n",
      "loss:\n",
      "0.3564472711874229\n",
      "mse:\n",
      "0.7128945423748458\n",
      "\n",
      "loss:\n",
      "0.3479192556350571\n",
      "mse:\n",
      "0.6958385112701142\n",
      "\n",
      "loss:\n",
      "0.3416604243252234\n",
      "mse:\n",
      "0.6833208486504468\n",
      "\n",
      "loss:\n",
      "0.3367696743879446\n",
      "mse:\n",
      "0.6735393487758892\n",
      "\n",
      "loss:\n",
      "0.332920780699344\n",
      "mse:\n",
      "0.665841561398688\n",
      "\n",
      "loss:\n",
      "0.3298252667304897\n",
      "mse:\n",
      "0.6596505334609793\n",
      "\n",
      "loss:\n",
      "0.3274537652647585\n",
      "mse:\n",
      "0.654907530529517\n",
      "\n",
      "loss:\n",
      "0.3255927080818482\n",
      "mse:\n",
      "0.6511854161636964\n",
      "\n",
      "loss:\n",
      "0.3240636368161275\n",
      "mse:\n",
      "0.648127273632255\n",
      "\n",
      "loss:\n",
      "0.32281507865162906\n",
      "mse:\n",
      "0.6456301573032581\n",
      "\n",
      "loss:\n",
      "0.3217828660601582\n",
      "mse:\n",
      "0.6435657321203164\n",
      "\n",
      "loss:\n",
      "0.32085496147250714\n",
      "mse:\n",
      "0.6417099229450143\n",
      "\n",
      "loss:\n",
      "0.32005988837516414\n",
      "mse:\n",
      "0.6401197767503283\n",
      "\n",
      "loss:\n",
      "0.31931380417653565\n",
      "mse:\n",
      "0.6386276083530713\n",
      "\n",
      "loss:\n",
      "0.3186435134413029\n",
      "mse:\n",
      "0.6372870268826057\n",
      "\n",
      "loss:\n",
      "0.31803057070022095\n",
      "mse:\n",
      "0.6360611414004419\n",
      "\n",
      "loss:\n",
      "0.31744563827695443\n",
      "mse:\n",
      "0.6348912765539089\n",
      "\n",
      "loss:\n",
      "0.31688999815638214\n",
      "mse:\n",
      "0.6337799963127643\n",
      "\n",
      "loss:\n",
      "0.3163744564049183\n",
      "mse:\n",
      "0.6327489128098366\n",
      "\n",
      "loss:\n",
      "0.3159030386097945\n",
      "mse:\n",
      "0.631806077219589\n",
      "\n",
      "loss:\n",
      "0.31542675044546\n",
      "mse:\n",
      "0.63085350089092\n",
      "\n",
      "loss:\n",
      "0.31495535044767126\n",
      "mse:\n",
      "0.6299107008953425\n",
      "\n",
      "loss:\n",
      "0.31447009552704763\n",
      "mse:\n",
      "0.6289401910540953\n",
      "\n",
      "loss:\n",
      "0.31394774457990793\n",
      "mse:\n",
      "0.6278954891598159\n",
      "\n",
      "loss:\n",
      "0.31344627553873716\n",
      "mse:\n",
      "0.6268925510774743\n",
      "\n",
      "loss:\n",
      "0.3129260680692828\n",
      "mse:\n",
      "0.6258521361385656\n",
      "\n",
      "loss:\n",
      "0.31239072860396466\n",
      "mse:\n",
      "0.6247814572079293\n",
      "\n",
      "loss:\n",
      "0.3117742120357308\n",
      "mse:\n",
      "0.6235484240714616\n",
      "\n",
      "loss:\n",
      "0.3111047984216748\n",
      "mse:\n",
      "0.6222095968433496\n",
      "\n",
      "loss:\n",
      "0.3104095809674078\n",
      "mse:\n",
      "0.6208191619348156\n",
      "\n",
      "loss:\n",
      "0.30970031391835584\n",
      "mse:\n",
      "0.6194006278367117\n",
      "\n",
      "loss:\n",
      "0.30896574033657936\n",
      "mse:\n",
      "0.6179314806731587\n",
      "\n",
      "loss:\n",
      "0.3082228452308667\n",
      "mse:\n",
      "0.6164456904617334\n",
      "\n",
      "loss:\n",
      "0.30741927798088964\n",
      "mse:\n",
      "0.6148385559617793\n",
      "\n",
      "loss:\n",
      "0.30667392629452705\n",
      "mse:\n",
      "0.6133478525890541\n",
      "\n",
      "loss:\n",
      "0.3059341645527906\n",
      "mse:\n",
      "0.6118683291055812\n",
      "\n",
      "loss:\n",
      "0.3051940065884029\n",
      "mse:\n",
      "0.6103880131768058\n",
      "\n",
      "loss:\n",
      "0.3044871110439831\n",
      "mse:\n",
      "0.6089742220879663\n",
      "\n",
      "loss:\n",
      "0.30374524111680384\n",
      "mse:\n",
      "0.6074904822336077\n",
      "\n",
      "loss:\n",
      "0.3029876265504139\n",
      "mse:\n",
      "0.6059752531008278\n",
      "\n",
      "loss:\n",
      "0.3022738893207672\n",
      "mse:\n",
      "0.6045477786415344\n",
      "\n",
      "loss:\n",
      "0.3015376781512027\n",
      "mse:\n",
      "0.6030753563024054\n",
      "\n",
      "loss:\n",
      "0.30078099072652587\n",
      "mse:\n",
      "0.6015619814530517\n",
      "\n",
      "loss:\n",
      "0.30003521806205563\n",
      "mse:\n",
      "0.6000704361241113\n",
      "\n",
      "loss:\n",
      "0.29929250395567125\n",
      "mse:\n",
      "0.5985850079113425\n",
      "\n",
      "loss:\n",
      "0.29855305916517044\n",
      "mse:\n",
      "0.5971061183303409\n",
      "\n",
      "loss:\n",
      "0.2978209856469943\n",
      "mse:\n",
      "0.5956419712939885\n",
      "\n",
      "loss:\n",
      "0.29707815057387593\n",
      "mse:\n",
      "0.5941563011477519\n",
      "\n",
      "loss:\n",
      "0.296356683151756\n",
      "mse:\n",
      "0.592713366303512\n",
      "\n",
      "loss:\n",
      "0.29561432325892995\n",
      "mse:\n",
      "0.5912286465178599\n",
      "\n",
      "loss:\n",
      "0.2948549133918022\n",
      "mse:\n",
      "0.5897098267836044\n",
      "\n",
      "loss:\n",
      "0.29407995929829617\n",
      "mse:\n",
      "0.5881599185965923\n",
      "\n",
      "loss:\n",
      "0.29332457617785834\n",
      "mse:\n",
      "0.5866491523557167\n",
      "\n",
      "loss:\n",
      "0.29255891985363036\n",
      "mse:\n",
      "0.5851178397072607\n",
      "\n",
      "loss:\n",
      "0.29181635570499265\n",
      "mse:\n",
      "0.5836327114099853\n",
      "\n",
      "loss:\n",
      "0.29103756566201233\n",
      "mse:\n",
      "0.5820751313240247\n",
      "\n",
      "loss:\n",
      "0.29027503742099503\n",
      "mse:\n",
      "0.5805500748419901\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-6d04711bb73e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m        \u001b[0mvalidation_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m        \u001b[0mvalidation_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m        batch_size = 0)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-e585dbad9e9d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, n_hidden, nodes, activations, lr, validation_X, validation_y, batch_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-e585dbad9e9d>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batchy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mbatched_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mvalidation_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_val_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-e585dbad9e9d>\u001b[0m in \u001b[0;36m_forward_propagation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mweight_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-e585dbad9e9d>\u001b[0m in \u001b[0;36m_activation\u001b[0;34m(self, data, activation)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0mbuff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mbuff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = X_train_normalize.shape[1]\n",
    "OUTPUT_SIZE = 1\n",
    "LEARNING_RATE = 0.5\n",
    "nodes = [INPUT_SIZE,50,OUTPUT_SIZE]\n",
    "activations = [\"relu\" for i in range(len(nodes))]\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "losses, mses = nn.fit(X = X_train_normalize,\n",
    "       y = y_train_normalize,\n",
    "       n_hidden = len(nodes) - 2,\n",
    "       nodes = nodes,\n",
    "       activations = activations,\n",
    "       lr = LEARNING_RATE,\n",
    "       validation_X = X_train_sample,\n",
    "       validation_y = y_train_sample,\n",
    "       batch_size = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f29076af7b8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RMSEs\n",
    "plt.plot(range(0,len(mses)), mses, color='red', linewidth=2, label=\"mse\")\n",
    "plt.plot(range(0,len(mses)), losses, color='blue', linewidth=2, linestyle='dashed', label=\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5545446573100815"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(mses) # 1x50, 500 batch, validation, lr=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53332650909424"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(mses) # 1x50, 100 batch, validation, lr=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42089988591625715"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(mses) # 1x50, no batch, validation, lr=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30592902612462475"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(mses) # 5x50, no batch, lr=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRID SEARCH\n",
    "INPUT_SIZE = X_train_normalize.shape[1]\n",
    "OUTPUT_SIZE = 1\n",
    "LEARNING_RATE = 0.5\n",
    "\n",
    "\n",
    "batch_size_param = [32, 64, 128, 512, 1024, 2048, 5000, 10000]\n",
    "width_param = [50, 75, 100]\n",
    "depth_param = [3, 5, 7, 8, 9, 10]\n",
    "\n",
    "for batch in batch_size_param:\n",
    "    for width in width_param:\n",
    "        for depth in depth_param:\n",
    "            \n",
    "            nodes = [INPUT_SIZE] + [width for i in range(depth)] + [OUTPUT_SIZE]\n",
    "            activations = [\"relu\" for i in range(len(nodes))]\n",
    "\n",
    "            nn = NeuralNetwork()\n",
    "\n",
    "            losses, mses = nn.fit(X = X_train_normalize,\n",
    "                   y = y_train_normalize,\n",
    "                   n_hidden = len(nodes) - 2,\n",
    "                   nodes = nodes,\n",
    "                   activations = activations,\n",
    "                   lr = LEARNING_RATE,\n",
    "                   validation_X = X_train_sample,\n",
    "                   validation_y = y_train_sample,\n",
    "                   batch_size = batch)\n",
    "        \n",
    "        print((batch, width, depth))\n",
    "        print(min(mses))\n",
    "        plt.figure()\n",
    "        plt.plot(range(0,len(mses)), mses, color='red', linewidth=2, label=\"mse\")\n",
    "        plt.plot(range(0,len(mses)), losses, color='blue', linewidth=2, linestyle='dashed', label=\"loss\")\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_for_B(x, y, n):\n",
    "    # takes a data frame, and a numpy array\n",
    "    x = np.concatenate((np.ones((n,1)), x.to_numpy()), axis=1)\n",
    "    x_transpose = np.transpose(x)\n",
    "    return np.linalg.solve(np.matmul(x_transpose, x), x_transpose @ y)\n",
    "\n",
    "def get_predicted_values(beta, design_matrix, n):\n",
    "    return np.matmul(np.concatenate((np.ones((n,1)), design_matrix), axis = 1), beta)\n",
    "\n",
    "def BIC(predictions, actuals, d, n):\n",
    "    # numpy array - predictions, numpy array - actual \n",
    "    return (np.square(actuals - predictions).sum()) + (d * np.log(n))\n",
    "\n",
    "def RSquaredAdj(predictions, actuals, d, n):\n",
    "    x1 = np.square(actuals - actuals.mean()).sum()\n",
    "    x2 = np.square(predictions - actuals.mean()).sum()\n",
    "    r2 = x2/x1\n",
    "    return (1 - ((1 - r2) * (n - 1) / (n - d - 1)))\n",
    "\n",
    "def RMSE(predictions, actuals, n):\n",
    "    return np.sqrt((np.square(actuals - predictions).sum()) / n) \n",
    "\n",
    "def train_model(design_matrix, dependent_variable_series):\n",
    "    n = design_matrix.shape[0]\n",
    "    beta = solve_for_B(design_matrix, dependent_variable_series, n)\n",
    "    predicted_values = get_predicted_values(beta, design_matrix, n)\n",
    "    calculated_BIC = BIC(predicted_values, dependent_variable_series, d = design_matrix.shape[1], n = n)\n",
    "    calculated_RMSE = RMSE(predicted_values, dependent_variable_series, n = n)\n",
    "    residuals = predicted_values - dependent_variable_series\n",
    "    return beta, calculated_BIC, calculated_RMSE, residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_crossv_model(df, y): \n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(.25 * m)\n",
    "    train_1 = df.iloc[perm[:train_end]]\n",
    "    test_1 = y.iloc[perm[:train_end]]\n",
    "\n",
    "    train_2 = df.iloc[perm[train_end:train_end*2]]\n",
    "    test_2 = y.iloc[perm[train_end:train_end*2]]\n",
    "\n",
    "    train_3 = df.iloc[perm[train_end*2:train_end*3]]\n",
    "    test_3 = y.iloc[perm[train_end*2:train_end*3]]\n",
    "\n",
    "    train_4 = df.iloc[perm[train_end*3:]]\n",
    "    test_4 = y.iloc[perm[train_end*3:]]\n",
    "    return train_1, test_1, train_2, test_2, train_3, test_3, train_4, test_4\n",
    "\n",
    "\n",
    "def get_average_RMSE_for_one_model(df, y):\n",
    "    train_1, test_1, train_2, test_2, train_3, test_3, train_4, test_4 = train_crossv_model(df, y)\n",
    "\n",
    "    beta4, bic4, RMSE4, _ = train_model(train_1.append([train_2, train_3]), test_1.append([test_2, test_3]))\n",
    "\n",
    "    beta2, bic3,RMSE2, _ = train_model(train_1.append([train_4, train_3]), test_1.append([test_4, test_3]))\n",
    "\n",
    "    beta3, bic2, RMSE3, _ = train_model(train_1.append([train_2, train_4]), test_1.append([test_2, test_4]))\n",
    "\n",
    "    beta1, bic1, RMSE1, _ = train_model(train_4.append([train_2, train_3]), test_4.append([test_2, test_3]))\n",
    "\n",
    "    return sum([RMSE1, RMSE2, RMSE3, RMSE4]) / 4, sum([bic1, bic2, bic3, bic4]) / 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_sq\n",
      "bic: 9.626303946636282e+16\n",
      "rmse: 4606838.482945066\n",
      "\n",
      "office_sqm_5000\n",
      "bic: 9.236088392607434e+16\n",
      "rmse: 4509890.259458901\n",
      "\n",
      "church_count_500\n",
      "bic: 8.745980531475309e+16\n",
      "rmse: 4390793.45502309\n",
      "\n",
      "prom_part_2000\n",
      "bic: 8.511633546360696e+16\n",
      "rmse: 4333342.953272082\n",
      "\n",
      "kitch_sq\n",
      "bic: 8.310655887323078e+16\n",
      "rmse: 4281289.175857935\n",
      "\n",
      "life_sq\n",
      "bic: 8.215130287457203e+16\n",
      "rmse: 4254329.778610688\n",
      "\n",
      "leisure_count_1000\n",
      "bic: 8.161106327089005e+16\n",
      "rmse: 4236389.35523205\n",
      "\n",
      "big_church_count_1500\n",
      "bic: 8.058786114158811e+16\n",
      "rmse: 4215470.631885044\n",
      "\n",
      "mosque_km\n",
      "bic: 7.944348729565469e+16\n",
      "rmse: 4185852.331020752\n",
      "\n",
      "cafe_count_1500_price_1500\n",
      "bic: 7.896509252549352e+16\n",
      "rmse: 4169259.28286633\n",
      "\n",
      "office_sqm_1500\n",
      "bic: 7.847669871204554e+16\n",
      "rmse: 4160174.121158242\n",
      "\n",
      "cafe_count_5000\n",
      "bic: 7.824552931615512e+16\n",
      "rmse: 4152199.015860779\n",
      "\n",
      "office_count_3000\n",
      "bic: 7.778728666210142e+16\n",
      "rmse: 4142233.1379193245\n",
      "\n",
      "cafe_count_500_price_4000\n",
      "bic: 7.692101040364656e+16\n",
      "rmse: 4116071.3189909803\n",
      "\n",
      "green_part_2000\n",
      "bic: 7.674373145853638e+16\n",
      "rmse: 4113657.250657238\n",
      "\n",
      "build_count_mix\n",
      "bic: 7.680311786238806e+16\n",
      "rmse: 4113563.837153145\n",
      "\n",
      "floor\n",
      "bic: 7.65857609108639e+16\n",
      "rmse: 4106396.516268879\n",
      "\n",
      "male_f\n",
      "bic: 7.633186254849541e+16\n",
      "rmse: 4101568.63152971\n",
      "\n",
      "build_count_1971-1995\n",
      "bic: 7.620298167931363e+16\n",
      "rmse: 4096131.4150387836\n",
      "\n",
      "num_room\n",
      "bic: 7.55289604229202e+16\n",
      "rmse: 4080960.3420264814\n",
      "\n",
      "leisure_count_1500\n",
      "bic: 7.5228673102557e+16\n",
      "rmse: 4070437.3257457744\n",
      "\n",
      "state\n",
      "bic: 7.511986503782722e+16\n",
      "rmse: 4062326.489717514\n",
      "\n",
      "fitness_km\n",
      "bic: 7.448653987398805e+16\n",
      "rmse: 4052349.3174064844\n",
      "\n",
      "hospital_beds_raion\n",
      "bic: 7.44006842257685e+16\n",
      "rmse: 4050129.361337675\n",
      "\n",
      "leisure_count_2000\n",
      "bic: 7.40338991329617e+16\n",
      "rmse: 4036121.3174029146\n",
      "\n",
      "cafe_count_1000_price_500\n",
      "bic: 7.36416006952318e+16\n",
      "rmse: 4027147.8220237615\n",
      "\n",
      "cafe_count_2000_na_price\n",
      "bic: 7.340314366723547e+16\n",
      "rmse: 4021216.153837182\n",
      "\n",
      "cafe_count_3000_price_4000\n",
      "bic: 7.339829964297309e+16\n",
      "rmse: 4023368.990371015\n",
      "\n",
      "public_transport_station_min_walk\n",
      "bic: 7.335725917973778e+16\n",
      "rmse: 4017454.945213433\n",
      "\n",
      "ttk_km\n",
      "bic: 7.325136442805432e+16\n",
      "rmse: 4016147.6017246516\n",
      "\n",
      "office_sqm_3000\n",
      "bic: 7.321340653615309e+16\n",
      "rmse: 4016395.618801268\n",
      "\n",
      "big_church_km\n",
      "bic: 7.3188685089022e+16\n",
      "rmse: 4017113.320693792\n",
      "\n",
      "office_sqm_2000\n",
      "bic: 7.294042370722938e+16\n",
      "rmse: 4009411.493887178\n",
      "\n",
      "workplaces_km\n",
      "bic: 7.281308376002829e+16\n",
      "rmse: 4006567.4108336167\n",
      "\n",
      "green_part_1500\n",
      "bic: 7.283128265839187e+16\n",
      "rmse: 4006954.939855951\n",
      "\n",
      "water_km\n",
      "bic: 7.281810275649462e+16\n",
      "rmse: 4004843.3385597537\n",
      "\n",
      "preschool_quota\n",
      "bic: 7.287185139960459e+16\n",
      "rmse: 4007230.1812344743\n",
      "\n",
      "catering_km\n",
      "bic: 7.272089616127267e+16\n",
      "rmse: 4000037.0520485314\n",
      "\n",
      "prom_part_1000\n",
      "bic: 7.271318797496293e+16\n",
      "rmse: 4002231.2540524206\n",
      "\n",
      "market_count_500\n",
      "bic: 7.273189166865533e+16\n",
      "rmse: 4003182.6914754473\n",
      "\n",
      "school_quota\n",
      "bic: 7.278280364131069e+16\n",
      "rmse: 4004422.5878490945\n",
      "\n",
      "office_sqm_1000\n",
      "bic: 7.262241911605189e+16\n",
      "rmse: 4000388.503377341\n",
      "\n",
      "cafe_count_5000_price_2500\n",
      "bic: 7.243400948616074e+16\n",
      "rmse: 3995650.816055692\n",
      "\n",
      "cafe_avg_price_500\n",
      "bic: 7.248550807309618e+16\n",
      "rmse: 3995025.400789014\n",
      "\n",
      "cafe_count_5000_price_1500\n",
      "bic: 7.259119978465805e+16\n",
      "rmse: 3996775.3238605307\n",
      "\n",
      "radiation_km\n",
      "bic: 7.246412109926864e+16\n",
      "rmse: 3993969.4230149486\n",
      "\n",
      "trc_sqm_1000\n",
      "bic: 7.219533704230445e+16\n",
      "rmse: 3989007.373247315\n",
      "\n",
      "church_count_2000\n",
      "bic: 7.235494204335008e+16\n",
      "rmse: 3994145.1400378947\n",
      "\n",
      "young_female\n",
      "bic: 7.205725760616661e+16\n",
      "rmse: 3984362.623183224\n",
      "\n",
      "build_count_foam\n",
      "bic: 7.22759579138958e+16\n",
      "rmse: 3991356.7085112464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# forward cross validated stepwise\n",
    "\n",
    "train_data = X_train.copy()\n",
    "df_with_col = pd.DataFrame()\n",
    "\n",
    "#min_RMSE, min_bic = get_average_RMSE_for_one_model(train_data, y_train_sample)\n",
    "\n",
    "col_added = []\n",
    "rmses = []\n",
    "\n",
    "for i in range(0,50):\n",
    "\n",
    "    min_col = None\n",
    "    min_RMSE = 100000000000000000000\n",
    "    min_bic = 10000000000000000000\n",
    "\n",
    "    for col in train_data.columns:\n",
    "\n",
    "        df_with_col[col] = train_data[col]\n",
    "        \n",
    "        new_RMSE, new_bic = get_average_RMSE_for_one_model(df_with_col, y_train)\n",
    "                \n",
    "        if new_RMSE[0] <= min_RMSE:\n",
    "            min_bic = new_bic[0]\n",
    "            min_RMSE = new_RMSE[0]\n",
    "            min_col = col\n",
    "            \n",
    "        df_with_col.drop(columns=[col], inplace=True)\n",
    "\n",
    "    if min_col is not None:\n",
    "        print(min_col)\n",
    "        df_with_col[min_col] = train_data[min_col]\n",
    "        train_data.drop(columns=[min_col], inplace=True)\n",
    "        rmses.append(min_RMSE)\n",
    "        print(\"bic: \" + str(min_bic))\n",
    "        print(\"rmse: \" + str(min_RMSE))\n",
    "        col_added.append(min_col)\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(\"Failed #2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd76bd9d630>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RMSEs\n",
    "plt.plot(range(0,50), rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['full_sq',\n",
       " 'office_sqm_5000',\n",
       " 'church_count_500',\n",
       " 'prom_part_2000',\n",
       " 'kitch_sq',\n",
       " 'life_sq',\n",
       " 'leisure_count_1000',\n",
       " 'big_church_count_1500',\n",
       " 'mosque_km',\n",
       " 'cafe_count_1500_price_1500',\n",
       " 'office_sqm_1500',\n",
       " 'cafe_count_5000',\n",
       " 'office_count_3000',\n",
       " 'cafe_count_500_price_4000',\n",
       " 'green_part_2000',\n",
       " 'build_count_mix',\n",
       " 'floor',\n",
       " 'male_f',\n",
       " 'build_count_1971-1995',\n",
       " 'num_room',\n",
       " 'leisure_count_1500',\n",
       " 'state',\n",
       " 'fitness_km',\n",
       " 'hospital_beds_raion',\n",
       " 'leisure_count_2000',\n",
       " 'cafe_count_1000_price_500',\n",
       " 'cafe_count_2000_na_price',\n",
       " 'cafe_count_3000_price_4000',\n",
       " 'public_transport_station_min_walk',\n",
       " 'ttk_km',\n",
       " 'office_sqm_3000',\n",
       " 'big_church_km',\n",
       " 'office_sqm_2000',\n",
       " 'workplaces_km',\n",
       " 'green_part_1500',\n",
       " 'water_km',\n",
       " 'preschool_quota',\n",
       " 'catering_km',\n",
       " 'prom_part_1000',\n",
       " 'market_count_500',\n",
       " 'school_quota',\n",
       " 'office_sqm_1000',\n",
       " 'cafe_count_5000_price_2500',\n",
       " 'cafe_avg_price_500',\n",
       " 'cafe_count_5000_price_1500',\n",
       " 'radiation_km',\n",
       " 'trc_sqm_1000',\n",
       " 'church_count_2000',\n",
       " 'young_female',\n",
       " 'build_count_foam']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
